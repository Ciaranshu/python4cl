{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Linguists module 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to module 1.4. In this module, we will start calculating statistics using real corpus. \n",
    "\n",
    "Let's first refresh your memory on ngrams and probabilities by completing the following quiz:\n",
    "\n",
    "## Pre-module quiz\n",
    "\n",
    "Given the sequence `aabbdab`, what is $P(b|a)$?\n",
    "\n",
    "A. 1/2\n",
    "\n",
    "B. 1/3\n",
    "\n",
    "C. 2/3\n",
    "\n",
    "D. 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python organises code into units called modules. Modules are then collected together into packages. Here we're going to be importing the packages from the `spacy` module to create a tokenizer. spaCy is a free open-source Python library for NLP that is widely used in real-world applications because of its efficiency.  so it can be easily referenced from now on. \n",
    "\n",
    "In this tutorial, we have already installed a package named spacy. Let's try importing a class named English from the spacy package, and create a default tokenizer from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English class is in a script named `en.py` in the path `spacy/lang/`. To import the class variable, we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.lang.en #import the path\n",
    "nlp=spacy.lang.en.English() #call the class variable with full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.lang.en as en #this line allows Python to access code from the spacy/lang/en and to rename this code as 'en'\n",
    "nlp=en.English() #call the class variable with the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English # directly import the class variable\n",
    "nlp=English() # call the class directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a default English tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Processing the corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python allows you to read, write and delete files. Here we'll be reading a corpus of Shakespeare's entire works (around 1 million word tokens). The relative path to the corpus file 'Shakespeare' is `../../corpora/shakespeare` (`..` indicates the parent directory). We access this using the `open()` function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../../corpora/Shakespeare','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declared the variable `f` to open the Shakespeare file. `open()` takes 2 arguments, the path to the file that we want to open and a string that represents the kinds of permission or operation we want to do on the file. Here the string is `'r'` which refers to the permission of 'read-only'. We can also create a new file to write by replacing `'r'` with `'w'`. If we want to append data to the file, we can replace `'r'` with `'a'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to close the `f` variable after finishing reading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more recommendable way is to use `with` keyword so that the file will be properly closed after its suite finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "with open('../../corpora/Shakespeare','r') as f:\n",
    "    print ('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use `readlines()` to read in all the lines from the corpus in to a list of strings. We store the list in the variable `lines` .   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../corpora/Shakespeare','r') as f:\n",
    "    lines=f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look of what the variable `lines` contains by printing out the first ten items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1609\\n',\n",
       " '\\n',\n",
       " 'THE SONNETS\\n',\n",
       " '\\n',\n",
       " 'by William Shakespeare\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '                     1\\n',\n",
       " '  From fairest creatures we desire increase,\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have you noticed that each line is followed by the special character `\\n`. This signifies the end of a line of text and the start of a new one. This is called a control character that does not have written symbols in the text but will show up when we process it. \n",
    "\n",
    "Another observation is that some lines such as the last line starts with some space characters. \n",
    "\n",
    "We can get rid of these characters by calling the string method `strip()` that will automatically strip a string with whitespace characters including space, tabs, and control characters both from the start or at the end of the string. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a for loop to process each line of the file and store the processed lines as tokenized word lists into the variable `lines_processed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_processed=[] # a list to store the processed lines\n",
    "with open('../../corpora/Shakespeare','r') as f:\n",
    "    for line in f:\n",
    "        #for each line, we do:\n",
    "        #1. remove whitespace characters like \\t \\r \\n\n",
    "        line=line.strip()\n",
    "        #2. skip the empty lines\n",
    "        if line=='':\n",
    "            continue\n",
    "        else: \n",
    "            #3. tokenize the sentence into word list:\n",
    "            tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
    "            # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
    "            tokens_str=[]\n",
    "            for tok in tokens:\n",
    "                tokens_str.append(str(tok))# str() converts into a string\n",
    "            lines_processed.append(tokens_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap up the above into a function to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "    '''\n",
    "    process file into a list of lines where each line is a list of words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : file path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of list of strings\n",
    "    [\n",
    "        [1609]\n",
    "        [THE, SONNETS]\n",
    "        ...\n",
    "    ]\n",
    "    '''\n",
    "    lines_processed=[] # a list to store the processed lines\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #for each line, we do:\n",
    "            #1. remove whitespace characters like \\t \\r \\n\n",
    "            line=line.strip()\n",
    "            #2. skip the empty lines\n",
    "            if line=='':\n",
    "                continue\n",
    "            else: \n",
    "                #3. tokenize the sentence into word list:\n",
    "                tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
    "                # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
    "                tokens_str=[]\n",
    "                for tok in tokens:\n",
    "                    tokens_str.append(str(tok))# str() converts into a string\n",
    "                lines_processed.append(tokens_str)\n",
    "    return lines_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function `process()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_processed=process('../../corpora/Shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of the lines after tokenisation. Let's print out the first ten lines:\n",
    "Do you think the tokenizer is doing well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1609'],\n",
       " ['THE', 'SONNETS'],\n",
       " ['by', 'William', 'Shakespeare'],\n",
       " ['1'],\n",
       " ['From', 'fairest', 'creatures', 'we', 'desire', 'increase', ','],\n",
       " ['That', 'thereby', 'beauty', \"'s\", 'rose', 'might', 'never', 'die', ','],\n",
       " ['But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ','],\n",
       " ['His', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':'],\n",
       " ['But', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ','],\n",
       " [\"Feed'st\",\n",
       "  'thy',\n",
       "  'light',\n",
       "  \"'s\",\n",
       "  'flame',\n",
       "  'with',\n",
       "  'self',\n",
       "  '-',\n",
       "  'substantial',\n",
       "  'fuel',\n",
       "  ',']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_processed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check how many lines the corpus contains by calling `len()` function over `lines_processed`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 1:\n",
    "In the following cell, please try processing the other corpus 'Marlowe' in the same directory of 'Shakespeare', and store the processed results in variable: `marlowe_processed`. \n",
    "\n",
    "Then, Answer the following question:\n",
    "\n",
    "How many lines does Marlowe corpus contain?\n",
    "\n",
    "A. 19492\n",
    "\n",
    "B. 114422\n",
    "\n",
    "C. 1949\n",
    "\n",
    "D. 45336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "marlowe_processed=process('../../corpora/Marlowe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19492"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marlowe_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary is a collection which is unordered, changeable and indexed. \n",
    "In Python dictionaries are written with curly brackets, and they have keys and values.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict =\t{\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the items of a dictionary by referring to its key name, inside square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ford'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisdict['brand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the value of a specific item by referring to its key name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict['year']=2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an item to the dictionary is done by using a new index key and assigning a value to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict[\"color\"] = \"red\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can loop through a dictionary by using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand\n",
      "model\n",
      "year\n",
      "color\n"
     ]
    }
   ],
   "source": [
    "#Print all key names in the dictionary, one by one:\n",
    "\n",
    "for x in thisdict:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford\n",
      "Mustang\n",
      "2019\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "#Print all values in the dictionary, one by one:\n",
    "\n",
    "for x in thisdict:\n",
    "  print(thisdict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford\n",
      "Mustang\n",
      "2019\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "#You can also use the values() function to return values of a dictionary:\n",
    "\n",
    "for x in thisdict.values():\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand Ford\n",
      "model Mustang\n",
      "year 2019\n",
      "color red\n"
     ]
    }
   ],
   "source": [
    "#Loop through both keys and values, by using the items() function:\n",
    "for x, y in thisdict.items():\n",
    "  print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if a specified key is present in a dictionary use the `in` keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, 'model' is one of the keys in the thisdict dictionary\n"
     ]
    }
   ],
   "source": [
    "if \"model\" in thisdict:\n",
    "  print(\"Yes, 'model' is one of the keys in the thisdict dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how many items (key-value pairs) a dictionary has, use the `len()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#Print the number of items in the dictionary:\n",
    "print(len(thisdict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop over the word lists in `lines_processed` to create a vocabulary dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
    "for line in lines_processed:\n",
    "    for word in line:\n",
    "        if word in vocab:\n",
    "            vocab[word]+=1 # update the count for an existing word\n",
    "        else:\n",
    "            vocab[word]=1 # initilize the count for a new word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can wrap the above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_dict(f_processed_arg):\n",
    "    '''\n",
    "    Collect vocabulary counts from text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_processed_arg: a list of list of words processed from text as the output of process()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with words (str) as keys and counts(int) as values\n",
    "    vocab={\n",
    "    'SONNETS': 1\n",
    "    }\n",
    "    '''\n",
    "    vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
    "    for line in f_processed_arg:\n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1 # update the count for an existing word\n",
    "            else:\n",
    "                vocab[word]=1 # initilize the count for a new word\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=create_vocab_dict(lines_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then retrieve the count of a specific word by the statement `vocab[word]`, eg. the count of 'thee' in the corpus should be 3144."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['thee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 2:\n",
    "Use the following cell to retrieve the counts of the following pronouns in Shakespeare: 'thou','thee','thy','thine','you','your'. Which order of the word counts is correct?\n",
    "\n",
    "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
    "\n",
    "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
    "\n",
    "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
    "\n",
    "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4556"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['thou']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 3:\n",
    "please collect a vocabulary dictionary from 'Marlowe' as well in the following cells, and answer the same question:\n",
    "\n",
    "Which order of the word counts is correct in Marlowe?\n",
    "\n",
    "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
    "\n",
    "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
    "\n",
    "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
    "\n",
    "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_marlowe=create_vocab_dict(marlowe_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_marlowe['thine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caluating Type-token ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type-token ratio (TTR)** is the ratio obtained by dividing the **types** (the total number of different words) occurring in a text or utterance by its **tokens** (the total number of words). A high TTR indicates a high degree of lexical variation while a low TTR indicates the opposite. The range falls between a theoretical 0 (infinite repetition of a single type) and 1 (the complete non-repetition found in a concordance).\n",
    "\n",
    "Let's calculate type count first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32951\n"
     ]
    }
   ],
   "source": [
    "#Recall that vocab stores words as keys. Let's first retrieve the key list of the vocab dictioanry:\n",
    "key_list=vocab.keys()\n",
    "# the number of types is just the length of the key list\n",
    "type_count=len(vocab.keys())\n",
    "print (type_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caculate token count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133110\n"
     ]
    }
   ],
   "source": [
    "# Let's create a loop to aggregate the token counts in the vocabulary:\n",
    "token_count=0\n",
    "for word in vocab:\n",
    "    token_count+=vocab[word] # equals token_count=token_count+vocab[word]\n",
    "print (token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029080142263328362\n"
     ]
    }
   ],
   "source": [
    "#Let's calculate the type-token ratio:\n",
    "ttr=type_count/token_count\n",
    "print (ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could wrap the calculation up into a function that takes in vocab dictionary and outputs the ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr_cal(vocab_arg):\n",
    "    '''\n",
    "    calculate type-token ratio\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_arg: a vocab dictionary with words as keys and counts as values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a float number indicating type-token ratio\n",
    "    '''\n",
    "    type_count=len(vocab_arg.keys())\n",
    "    token_count=0\n",
    "    for word in vocab_arg:\n",
    "        token_count+=vocab_arg[word]\n",
    "    ttr=type_count/token_count\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.029080142263328362\n"
     ]
    }
   ],
   "source": [
    "print (ttr_cal(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 4:\n",
    "\n",
    "Use the above functions to calculate the type-token ratio of Marlowe corpus and compare the results with the Shakespeare corpus. Who has more lexical variation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04869252015708204"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_marlowe=create_vocab_dict(marlowe_processed)\n",
    "ttr_cal(vocab_marlowe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import `matplotlib` for plotting. Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. In the following line, we import the `pyplot` module from `matplot` and rename as `plt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the following function `produce_words_counts` to select a list of words and their counts as input for making the graph. We first sort the vocabulary from high frequency to low frequency. We can define the range of the vocabulary by changing `rank_start` and `rank_end`. `rank_start` specifies the rank of the top frequency word and `rank_end` specifies the rank of the low frequency word of the range. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_words_counts(vocab,rank_start,rank_end):\n",
    "    '''\n",
    "    produce words and counts for plotting graphs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab: a vocab dictionary with words as keys and counts as values\n",
    "    rank_start: the start rank of the high frequency words\n",
    "    rank_end: the end rank of the low frequency words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    words: a list of words from rank_start to rank_end\n",
    "    counts: a list of teh respective counts of words\n",
    "    \n",
    "    '''\n",
    "    # 1. Sort the vocabulary according to frequency. It returns sorted pairs of (word,count) for the range defined between rank_start and rank_end. \n",
    "    # Please don't worry if you can't understand this part\n",
    "    vocab_sorted=sorted(vocab.items(),key=lambda x: x[1],reverse=True)[rank_start:rank_end] \n",
    "    # loop through the sorted vocabulary to get words and their respective counts for plotting the graphs later on\n",
    "    words=[]\n",
    "    counts=[]\n",
    "    for w_c in vocab_sorted:\n",
    "        w=w_c[0]\n",
    "        words.append(w)\n",
    "        count=w_c[1]\n",
    "        counts.append(count)\n",
    "    return words, counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Try changing `rank_start` and `rank_end` to produce different `words` and `counts` lists. For example, to retrieve the top 100 words, simply run `words,counts=produce(vocab,0,100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words,counts=produce_words_counts(vocab,0,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now plot a histogram using the words and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(15,8)) #figure size\n",
    "plt.bar(words,counts) #plot the historgram\n",
    "plt.ylim(0,90000) # specifies the minimum and maximum of the y axis. \n",
    "plt.xticks(rotation=90) # rotate the x label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 5:\n",
    "Try changing `rank_start` and `rank_end` and observe what kind of words the top-frequent words are? What about the low-frequent words?  How many top-frequent words do we have? How many low-frequent words do we have?\n",
    "\n",
    "In addition, you can try changing the arguments of `plot.ylim()` to adjust the y axis. For example, you may want to lower the maximum when examining the lower rank words. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigram is a sequence of adjacent two words. Let's create a dictionary to store the bigrams. The key will be each bigram and the value will be counts. We can extract bigrams by looping through the items in `lines_processed`. Recall that `lines_processed` is a list that contains the tokenized corpus. \n",
    "\n",
    "We also need to insert `<start>` and `<end>` tokens in each line so that we can model probabilities of the words in the start and at the end of a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looping through the list, we are looping over the items of the list. What if we want to loop over the index of each item as well? A useful function to use is `enumerate()` that allows us to loop over both the index and the item. We will see how it is used in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict={}\n",
    "for line in lines_processed:\n",
    "    #insert start <start> and end <end> token\n",
    "    line=['<start>']+line+['<end>']\n",
    "    for i,w in enumerate(line): \n",
    "        #'enumerate() loops over the variables i (the index of the current word) and w (the string of\n",
    "        #the current word)\n",
    "        w_first=w\n",
    "        if i+1<len(line): #not the end of the line\n",
    "            w_second=line[i+1]\n",
    "            bigram=(w_first,w_second) #a tuple to represent bigram\n",
    "            if bigram not in bigram_dict:\n",
    "                bigram_dict[bigram]=1\n",
    "            else:\n",
    "                bigram_dict[bigram]+=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 6\n",
    "What is the size of the bigram dictionary ie. how many bigrams do we have in the corpus? If you compare with the size of the `vocab` which is basically a unigram dictionary, are you surprised by the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315400"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 7:\n",
    "Is gender associated with beauty? Try retrieving the count of 'his beauty' and 'her beauty'? Which has more counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling\n",
    "\n",
    "Language modelling is a fundamental task in computational lingustics where the aim is to predict the probablity of a sequence of tokens occuring. We will consider tokens to be words in this exercise, but they can be many other things depending on the application (perhaps characters, utterances, phonemes, _etc_). \n",
    "\n",
    "Formally, we want to estimate the probablity $P(w_{1},\\ldots ,w_{m})$ of an $m$-length sequence, using some given corpus.\n",
    "\n",
    "There are many techniques for language modelling; we will consider here a very simple approach called $n$-gram models. These models make the assumption that the probability of a word only depends on the previous $n-1$ context words in the sequence. If $n=1$, we call this a *unigram* model, if $n=2$, it's a *bigram* model etc.\n",
    "\n",
    "So for example, given a sequence of words: *'to be or not to'* what is the probablity the next word will be *'be'*?  In $n$-gram models, this is calculated using *conditional probablity*: $P(`be'\\mid \\text{`to be or not to'})$ where we estimate the probablity of the entire sequence using the probablity chain rule (_i.e_. multiplying the conditional probabilities of the sequence):\n",
    "\n",
    "\n",
    "$$P(w_{1},\\ldots ,w_{m})=\\prod _{{i=1}}^{m}P(w_{i}\\mid w_{1},\\ldots ,w_{{i-1}})\\approx \\prod _{{i=1}}^{m}P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})$$\n",
    "\n",
    "We can estimate the conditional probility from our training corpus by simply counting:\n",
    "\n",
    "$$P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})={\\frac  {{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}},w_{i})}{{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}})}}$$\n",
    "\n",
    "### Unigram language model\n",
    "Let's start by building the most basic $n$-gram model, a unigram model where the conditional probablity for each word is simply the probablity of the word occuring in the corpus data: it does not dependent on any of the previous words in the sequence.\n",
    "\n",
    "We can start by defining a function called *unigram_prob* which calculates the probability of a unigram given as an input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(word):\n",
    "    return float(vocab[word]/token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by calculating the probability of the word \"horse\" occuring in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021175916277485677"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 8:\n",
    "Calculate the probability and the effect of capitalisation on the definite article. Try \"The\" vs \"THE\" vs \"the\", and note how much the probability differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035452013167890607"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010323259185274268"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"THE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020503580935675506"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the probability of a sequence (a sentence, paragraph, _etc_.) occurring, we use the chain rule of probability by multiplying the unigram probability of individual words. So lets calculate the probability of the sequence: *'To be, or not to be, that is the question:'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.408562399710954e-30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"To\") * unigram_prob(\"be\") * unigram_prob(\"or\")* unigram_prob(\"not\") * unigram_prob(\"to\") * unigram_prob(\"be\") * unigram_prob(\",\") * unigram_prob(\"that\") * unigram_prob(\"is\") * unigram_prob(\"the\") * unigram_prob(\"the\") * unigram_prob(\"question\") * unigram_prob(\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probablity value of our example sequence is a very small number! In fact, it is so small that it's about the same probability of picking the same ant thrice at random from all the ants on the planet!\n",
    "\n",
    "What happens if we continue to add further words to this sequence?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-probability\n",
    "\n",
    "Eventually the probability values will get so tiny that computers will not be able to represent them correctly in memory; this is known as an *underflow error*.  It will not require much more text to get an underflow error: a simple paragraph using our unigram model will result in one! \n",
    "\n",
    "Fortunately, there is a very simple way to avoid this: we can use log-probability instead of regular probability. To do so, we simply use the log function in our unigram_prob function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return math.log(float(vocab[word]/token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try calculacting the log-probability for the word \"horse\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.460060953704279"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the log-probability of an entire sequence, we simply add the individual log-probablity values of individual unigrams instead of multiplying them. \n",
    "\n",
    "Let's write a function that calculates the log-probability of a given text sequence (as a single string). The function must tokenize the input string correctly before performing the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob_for_sequence(tokens):\n",
    "    return sum([unigram_prob(tok) for tok in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the log-probablity of our example sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.11605999786988"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"To be, or not to be, that is the question:\"\n",
    "example_sequence = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 9:\n",
    "Calculate the log-probability of the sentence: *'A horse, a horse! My kingdom for a horse!'*. \n",
    "\n",
    "Is this sequence more or less likely to occur than *\"To be, or not to be, that is the question:\"* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-74.21539318177221"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A horse, a horse! My kingdom for a horse!\"\n",
    "tokens = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model\n",
    "\n",
    "Unlike the unigram language model above, a bigram language model uses the context history. It uses conditional probability to estimate the probability of a word occuring relative to the occurance of its predecessor in the sequence:\n",
    "\n",
    "$$P(w_{i}\\mid w_{i-1})={\\frac  {{\\mathrm  {count}}(w_{i-1},w_{i})}{{\\mathrm  {count}}(w_{i-1})}}$$\n",
    "\n",
    "Let's implement a bigram_prob function that calculates the bigram conditional probability for a single token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(w1, w2):\n",
    "    return math.log(float(bigram_dict[(w1,w2)]/vocab[w1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by calculating the conditional log-probability for the bigram \"to be\", i.e., the log-probability of the word \"be\" given the word \"to\" appearing before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0051406123266844"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob(\"to\", \"be\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can write a function to calculate the log-probability for a sequence of text using our bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob_for_sequence(tokens):\n",
    "    return sum([bigram_prob(w1,w2) for w1,w2 in zip(tokens, tokens[1:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to calculate the log probability of our example sequence, and compare it to the unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.11605999786988"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob_for_sequence(example_sequence) # log-probability using unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-49.81572109265195"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob_for_sequence(example_sequence) # log-probability using bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the log-probability value using the bigram model is substantially larger (the sequence is much likelier to occur). Is this perhaps because the bigram language model takes the context history into account whereas a unigram language model does not?\n",
    "\n",
    "We discuss how to correctly evaluate language models in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 10:\n",
    "Calculate and compare the unigram and bigram log-probabilities of the sentence: *'Double, double, toil and trouble; fire burns, and cauldron bubble.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-107.52272746936798"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Double, double, toil and trouble; fire burns, and cauldron bubble.\"\n",
    "tokens = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(tokens) # log-probability using unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-65.18214590870909"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob_for_sequence(tokens) # log-probability using bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating language models\n",
    "\n",
    "Language models are evaluated using *perplexity* ($P$), a measure of how well a probability model predicts a given sample (e.g. text sequences).\n",
    "\n",
    "Perplexity is defined as $P = 2^{{h(s)}}$ where $h$ is the information entropy per word for a given input $s$. $h(s)$ can be calculated as follows:\n",
    "\n",
    "$$h(s)=\\frac{-1}{|s|}\\sum _{w \\in s}\\log _{2}P(w)$$\n",
    "\n",
    "Here, $s$ is some sample text and $|s|$ is the number of tokens in $s$.\n",
    "\n",
    "The value of perplexity can be thought of as the number of choices the model needs to make on average per token of the input sequence. The lower the perplexity score, the better the language model is since the model has fewer choices on average for the input sequence. \n",
    "\n",
    "Let's implement a simple function to calculate the perplexity, taking as input the log-probability (calculated by a language model) and the length of a sequence (in number of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(log_prob, length):\n",
    "    H =  - (log_prob/ math.log(2)) / length  #change of log-base from base 10 to base 2, then normalize by length\n",
    "    return math.pow(2,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the perplexity of both language models by simply calling the respective functions, and feed the results to the perplexity function above along with the length of the example sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.1537652847171"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(bigram_prob_for_sequence(example_sequence),len(example_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.7174649447508"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(unigram_prob_for_sequence(example_sequence),len(example_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 11:\n",
    "Which of the two models has a lower perplexity?  measure the perplixity of both models using the example text sentences from previous quizes. \n",
    "\n",
    "Can you find a text string where the unigram model has a lower perplexity than the bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
