{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Linguists module 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to module 1.4. In this module, we will start calculating statistics using real corpus. \n",
    "\n",
    "Let's first refresh your memory on ngrams and probabilities by completing the following quiz:\n",
    "\n",
    "## Pre-module quiz\n",
    "\n",
    "Given the sequence 'aabbdab', what is *P*(b|a)?\n",
    "\n",
    "A. 1/2\n",
    "\n",
    "B. 1/3\n",
    "\n",
    "C. 2/3\n",
    "\n",
    "D. 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, there are many pre-built packages that contain modules (python scripts) that can be installed and imported easily. Once imported, we can use the functions defined in the imported modules, so that we do not need to write them yourselves. \n",
    "\n",
    "In this tutorial, we have already installed a package named spacy. Let's try importing a class named English from the spacy package, and create a default tokenizer from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English class is in a script named 'en.py' in the path spacy/lang/. To import the class variable, we could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.lang.en #import the path\n",
    "nlp=spacy.lang.en.English() #call the class variable with full path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy.lang.en as en #import the path and give it a name\n",
    "nlp=en.English() #call the class variable with the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English # directly import the class variable\n",
    "nlp=English() # call the class directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a default English tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.Defaults.create_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Processing the corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the the full works of Shakespeare corpus. The relative path to the corpus file is ../../corpora/shakespeare (.. indicates the parent directory). \n",
    "Let's first load the corpus by using the open function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../../corpora/Shakespeare','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declared the variable f to open the Shakespeare file. open() takes 2 arguments, the path to the file that we want to open and a string that represents the kinds of permission or operation we want to do on the file. Here 'r' refers to the permission of 'read-only'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to close the f variable after finishing reading:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more recommendable way is to use with keyword so that the file will be properly closed after its suite finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "with open('../../corpora/Shakespeare','r') as f:\n",
    "    print ('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a for loop to process each line of the file and store the processed lines as tokenized word lists into another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_processed=[] # a list to store the processed lines\n",
    "with open('../../corpora/Shakespeare','r') as f:\n",
    "    for line in f:\n",
    "        #for each line, we do:\n",
    "        #1. remove control characters like \\t \\r \\n\n",
    "        line=line.strip()\n",
    "        #2. skip the empty lines\n",
    "        if line=='':\n",
    "            continue\n",
    "        else: \n",
    "            #3. tokenize the sentence into word list:\n",
    "            tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
    "            # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
    "            tokens_str=[]\n",
    "            for tok in tokens:\n",
    "                tokens_str.append(str(tok))# str() converts into a string\n",
    "            f_processed.append(tokens_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wrap up the above into a function to process files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(filename):\n",
    "    '''\n",
    "    process file into a list of lines where each line is a list of words\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : file path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a list of list of strings\n",
    "    [\n",
    "        [1609]\n",
    "        [THE, SONNETS]\n",
    "        ...\n",
    "    ]\n",
    "    '''\n",
    "    f_processed=[] # a list to store the processed lines\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #for each line, we do:\n",
    "            #1. remove control characters like \\t \\r \\n\n",
    "            line=line.strip()\n",
    "            #2. skip the empty lines\n",
    "            if line=='':\n",
    "                continue\n",
    "            else: \n",
    "                #3. tokenize the sentence into word list:\n",
    "                tokens=tokenizer(line) #the tokenizer() function that we imported return a series of token items which are now Spacy classes\n",
    "                # To convert each item in tokens to strings, we need to loop over the line again and convert each token to strings by calling str()\n",
    "                tokens_str=[]\n",
    "                for tok in tokens:\n",
    "                    tokens_str.append(str(tok))# str() converts into a string\n",
    "                f_processed.append(tokens_str)\n",
    "    return f_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_processed=process('../../corpora/Shakespeare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check how many lines the corpus contains by calling len() function over f_processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114422"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 1:\n",
    "In the following cell, please try processing the other corpus 'Marlowe' in the same directory of 'Shakespeare', and store the processed results in variable: marlowe_processed. \n",
    "\n",
    "Then, Answer the following question:\n",
    "\n",
    "How many lines does Marlowe corpus contain?\n",
    "\n",
    "A. 19492\n",
    "\n",
    "B. 114422\n",
    "\n",
    "C. 1949\n",
    "\n",
    "D. 45336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "marlowe_processed=process('../../corpora/Marlowe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19492"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(marlowe_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary is a collection which is unordered, changeable and indexed. \n",
    "In Python dictionaries are written with curly brackets, and they have keys and values.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict =\t{\n",
    "  \"brand\": \"Ford\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the items of a dictionary by referring to its key name, inside square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ford'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisdict['brand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the value of a specific item by referring to its key name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict['year']=2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding an item to the dictionary is done by using a new index key and assigning a value to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdict[\"color\"] = \"red\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can loop through a dictionary by using a for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand\n",
      "model\n",
      "year\n",
      "color\n"
     ]
    }
   ],
   "source": [
    "#Print all key names in the dictionary, one by one:\n",
    "\n",
    "for x in thisdict:\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford\n",
      "Mustang\n",
      "2019\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "#Print all values in the dictionary, one by one:\n",
    "\n",
    "for x in thisdict:\n",
    "  print(thisdict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ford\n",
      "Mustang\n",
      "2019\n",
      "red\n"
     ]
    }
   ],
   "source": [
    "#You can also use the values() function to return values of a dictionary:\n",
    "\n",
    "for x in thisdict.values():\n",
    "  print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand Ford\n",
      "model Mustang\n",
      "year 2019\n",
      "color red\n"
     ]
    }
   ],
   "source": [
    "#Loop through both keys and values, by using the items() function:\n",
    "for x, y in thisdict.items():\n",
    "  print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine if a specified key is present in a dictionary use the 'in' keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, 'model' is one of the keys in the thisdict dictionary\n"
     ]
    }
   ],
   "source": [
    "if \"model\" in thisdict:\n",
    "  print(\"Yes, 'model' is one of the keys in the thisdict dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine how many items (key-value pairs) a dictionary has, use the len() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "#Print the number of items in the dictionary:\n",
    "print(len(thisdict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop over the word lists in f_processed to create a vocabulary dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
    "for line in f_processed:\n",
    "    for word in line:\n",
    "        if word in vocab:\n",
    "            vocab[word]+=1 # update the count for an existing word\n",
    "        else:\n",
    "            vocab[word]=1 # initilize the count for a new word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can wrap the above into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_dict(f_processed_arg):\n",
    "    '''\n",
    "    Collect vocabulary counts from text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_processed_arg: a list of list of words processed from text as the output of process()\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with words (str) as keys and counts(int) as values\n",
    "    vocab={\n",
    "    'SONNETS': 1\n",
    "    }\n",
    "    '''\n",
    "    vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
    "    for line in f_processed_arg:\n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1 # update the count for an existing word\n",
    "            else:\n",
    "                vocab[word]=1 # initilize the count for a new word\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=create_vocab_dict(f_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then retrieve the count of a specific word by vocab[word], eg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['thee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 2:\n",
    "Use the following cell to retrieve the counts of the following pronouns in Shakespeare: 'thou','thee','thy','thine','you','your'. Which order of the word counts is correct?\n",
    "\n",
    "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
    "\n",
    "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
    "\n",
    "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
    "\n",
    "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4557"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['thou']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 3:\n",
    "please collect a vocabulary dictionary from 'Marlowe' as well in the following cells, and answer the same question:\n",
    "\n",
    "Which order of the word counts is correct in Marlowe?\n",
    "\n",
    "A. 'you'>'thou'>'your'>'thy'>'thee'>'thine'\n",
    "\n",
    "B. 'you'>'your'>'thou'>'thy'>'thee'>'thine'\n",
    "\n",
    "C. 'thou'>'you'>'thy'>'thine'>'your'>'thee'\n",
    "\n",
    "D. 'you'>'thou'>'your'>'thy'>'thee'>'thine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can insert your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_marlowe=create_vocab_dict(marlowe_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_marlowe['thine']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caluating Type-token ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate type count first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32864\n"
     ]
    }
   ],
   "source": [
    "#Recall that vocab stores words as keys. Let's first retrieve the key list of the vocab dictioanry:\n",
    "key_list=vocab.keys()\n",
    "# the number of types is just the length of the key list\n",
    "type_count=len(vocab.keys())\n",
    "print (type_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's caculate token count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1133363\n"
     ]
    }
   ],
   "source": [
    "# Let's create a loop to aggregate the token counts in the vocabulary:\n",
    "token_count=0\n",
    "for word in vocab:\n",
    "    token_count+=vocab[word]\n",
    "print (token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028996888022637054\n"
     ]
    }
   ],
   "source": [
    "#Let's calculate the type-token ratio:\n",
    "ttr=type_count/token_count\n",
    "print (ttr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could wrap the calculation up into a function that takes in vocab dictionary and outputs the ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr_cal(vocab_arg):\n",
    "    '''\n",
    "    calculate type-token ratio\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_arg: a vocab dictionary with words as keys and counts as values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a float number indicating type-token ratio\n",
    "    '''\n",
    "    type_count=len(vocab_arg.keys())\n",
    "    token_count=0\n",
    "    for word in vocab_arg:\n",
    "        token_count+=vocab_arg[word]\n",
    "    ttr=type_count/token_count\n",
    "    return ttr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028996888022637054\n"
     ]
    }
   ],
   "source": [
    "print (ttr_cal(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 4:\n",
    "What does type-token ratio tell us about a corpus. Eg. lexical variation? \n",
    "\n",
    "Use the above functions to calculate the type-token ratio of Marlowe corpus and compare the results with the Shakespeare corpus. Who has more lexical variation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04869252015708204"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_marlowe=create_vocab_dict(marlowe_processed)\n",
    "ttr_cal(vocab_marlowe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting a frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import matplotlib for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can sort the words to get the top 100 frequent words. You can change rank_start and rank_end to retrieve ranks that you want to show. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_start=0\n",
    "rank_end=100\n",
    "vocab_sorted=sorted(vocab.items(),key=lambda x: x[1],reverse=True)[rank_start:rank_end] #sorting. returns sorted pairs of (word,count). Please don't worry if you can't understand this part\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through the sorted vocabulary to get words and counts for plotting the graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "counts=[]\n",
    "for w_c in vocab_sorted:\n",
    "    w=w_c[0]\n",
    "    words.append(w)\n",
    "    count=w_c[1]\n",
    "    counts.append(count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now plot a histogram using the words and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(15,8)) #figure size\n",
    "plt.bar(words,counts) #plot the historgram\n",
    "plt.ylim(0,90000)\n",
    "plt.xticks(rotation=90) # rotate the x label\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 5:\n",
    "Try changing rank_start and rank_end and observe what kind of words the top-frequent words are? What about the low-frequent words?  How many top-frequent words do we have? How many low-frequent words do we have?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bigram is a sequence of adjacent two words. Let's create a nested dictionary to store the bigrams. The key will be each bigram and the value will be counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first loop through the lines in f_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict={}\n",
    "for line in f_processed:\n",
    "    #insert start <start> and end <end> token\n",
    "    line=['<start>']+line+['<end>']\n",
    "    for i,w in enumerate(line):\n",
    "        w_first=w\n",
    "        if i+1<len(line): #not the end of the line\n",
    "            w_second=line[i+1]\n",
    "            bigram=(w_first,w_second) #a tuple to represent bigram\n",
    "            if bigram not in bigram_dict:\n",
    "                bigram_dict[bigram]=1\n",
    "            else:\n",
    "                bigram_dict[bigram]+=1\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 6:\n",
    "Is gender associated with beauty? Try retrieving the count of 'his beauty' and 'her beauty'? Which has more counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language modelling\n",
    "\n",
    "Language modelling is a fundamental task in computational lingustics where the aim is to predict the probablity of a sequence of tokens occuring. We will consider tokens to be words in this exercise, but they can be many other things depending on the application (perhaps characters, utterances, phonemes, _etc_). \n",
    "\n",
    "Formally, we want to estimate the probablity $P(w_{1},\\ldots ,w_{m})$ of an $m$-length sequence, using some given corpus.\n",
    "\n",
    "There are many techniques for language modelling; we will consider here a very simple approach called $n$-gram models. These models make the assumption that the probability of a word only depends on the previous $n-1$ context words in the sequence. If $n=1$, we call this a *unigram* model, if $n=2$, it's a *bigram* model etc.\n",
    "\n",
    "So for example, given a sequence of words: *'to be or not to'* what is the probablity the next word will be *'be'*?  In $n$-gram models, this is calculated using *conditional probablity*: $P(`be'\\mid \\text{`to be or not to'})$ where we estimate the probablity of the entire sequence using the probablity chain rule (_i.e_. multiplying the conditional probabilities of the sequence):\n",
    "\n",
    "\n",
    "$$P(w_{1},\\ldots ,w_{m})=\\prod _{{i=1}}^{m}P(w_{i}\\mid w_{1},\\ldots ,w_{{i-1}})\\approx \\prod _{{i=1}}^{m}P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})$$\n",
    "\n",
    "We can estimate the conditional probility from our training corpus by simply counting:\n",
    "\n",
    "$$P(w_{i}\\mid w_{{i-(n-1)}},\\ldots ,w_{{i-1}})={\\frac  {{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}},w_{i})}{{\\mathrm  {count}}(w_{{i-(n-1)}},\\ldots ,w_{{i-1}})}}$$\n",
    "\n",
    "### Unigram language model\n",
    "Let's start by building the most basic $n$-gram model, a unigram model where the conditional probablity for each word is simply the probablity of the word occuring in the corpus data: it does not dependent on any of the previous words in the sequence.\n",
    "\n",
    "We can start by defining a function called *unigram_prob* which calculates the probability of a unigram given as an input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(word):\n",
    "    return float(vocab[word]/token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by calculating the probability of the word \"horse\" occuring in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00021175916277485677"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 7:\n",
    "Calculate the probability and the effect of capitalisation on the definite article. Try \"The\" vs \"THE\" vs \"the\", and note how much the probability differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0035452013167890607"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"The\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00010323259185274268"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"THE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020503580935675506"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the probability of a sequence (a sentence, paragraph, _etc_.) occurring, we use the chain rule of probability by multiplying the unigram probability of individual words. So lets calculate the probability of the sequence: *'To be, or not to be, that is the question:'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.408562399710954e-30"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"To\") * unigram_prob(\"be\") * unigram_prob(\"or\")* unigram_prob(\"not\") * unigram_prob(\"to\") * unigram_prob(\"be\") * unigram_prob(\",\") * unigram_prob(\"that\") * unigram_prob(\"is\") * unigram_prob(\"the\") * unigram_prob(\"the\") * unigram_prob(\"question\") * unigram_prob(\":\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probablity value of our example sequence is a very small number! In fact, it is so small that it's about the same probability of picking the same ant thrice at random from all the ants on the planet!\n",
    "\n",
    "What happens if we continue to add further words to this sequence?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-probability\n",
    "\n",
    "Eventually the probability values will get so tiny that computers will not be able to represent them correctly in memory; this is known as an *underflow error*.  It will not require much more text to get an underflow error: a simple paragraph using our unigram model will result in one! \n",
    "\n",
    "Fortunately, there is a very simple way to avoid this: we can use log-probability instead of regular probability. To do so, we simply use the log function in our unigram_prob function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def unigram_prob(word):\n",
    "    return math.log(float(vocab[word]/token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try calculacting the log-probability for the word \"horse\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.460060953704279"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob(\"horse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the log-probability of an entire sequence, we simply add the individual log-probablity values of individual unigrams instead of multiplying them. \n",
    "\n",
    "Let's write a function that calculates the log-probability of a given text sequence (as a single string). The function must tokenize the input string correctly before performing the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob_for_sequence(tokens):\n",
    "    return sum([unigram_prob(tok) for tok in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the log-probablity of our example sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.11605999786988"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"To be, or not to be, that is the question:\"\n",
    "example_sequence = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 8:\n",
    "Calculate the log-probability of the sentence: *'A horse, a horse! My kingdom for a horse!'*. \n",
    "\n",
    "Is this sequence more or less likely to occur than *\"To be, or not to be, that is the question:\"* ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-74.21539318177221"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A horse, a horse! My kingdom for a horse!\"\n",
    "tokens = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram language model\n",
    "\n",
    "Unlike the unigram language model above, a bigram language model uses the context history. It uses conditional probability to estimate the probability of a word occuring relative to the occurance of its predecessor in the sequence:\n",
    "\n",
    "$$P(w_{i}\\mid w_{i-1})={\\frac  {{\\mathrm  {count}}(w_{i-1},w_{i})}{{\\mathrm  {count}}(w_{i-1})}}$$\n",
    "\n",
    "Let's implement a bigram_prob function that calculates the bigram conditional probability for a single token:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(w1, w2):\n",
    "    return math.log(float(bigram_dict[(w1,w2)]/vocab[w1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this by calculating the conditional log-probability for the bigram \"to be\", i.e., the log-probability of the word \"be\" given the word \"to\" appearing before it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0051406123266844"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob(\"to\", \"be\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can write a function to calculate the log-probability for a sequence of text using our bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob_for_sequence(tokens):\n",
    "    return sum([bigram_prob(w1,w2) for w1,w2 in zip(tokens, tokens[1:])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use this function to calculate the log probability of our example sequence, and compare it to the unigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.11605999786988"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_prob_for_sequence(example_sequence) # log-probability using unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-49.81572109265195"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob_for_sequence(example_sequence) # log-probability using bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the log-probability value using the bigram model is substantially larger (the sequence is much likelier to occur). Is this perhaps because the bigram language model takes the context history into account whereas a unigram language model does not?\n",
    "\n",
    "We discuss how to correctly evaluate language models in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 9:\n",
    "Calculate and compare the unigram and bigram log-probabilities of the sentence: *'Double, double, toil and trouble; fire burns, and cauldron bubble.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-107.52272746936798"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Double, double, toil and trouble; fire burns, and cauldron bubble.\"\n",
    "tokens = [str(tok) for tok in tokenizer(text)]\n",
    "unigram_prob_for_sequence(tokens) # log-probability using unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-65.18214590870909"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_prob_for_sequence(tokens) # log-probability using bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating language models\n",
    "\n",
    "Language models are evaluated using *perplexity* ($P$), a measure of how well a probability model predicts a given sample (e.g. text sequences).\n",
    "\n",
    "Perplexity is defined as $P = 2^{{h(s)}}$ where $h$ is the information entropy per word for a given input $s$. $h(s)$ can be calculated as follows:\n",
    "\n",
    "$$h(s)=\\frac{-1}{|s|}\\sum _{w \\in s}\\log _{2}P(w)$$\n",
    "\n",
    "Here, $s$ is some sample text and $|s|$ is the number of tokens in $s$.\n",
    "\n",
    "The value of perplexity can be thought of as the number of choices the model needs to make on average per token of the input sequence. The lower the perplexity score, the better the language model is since the model has fewer choices on average for the input sequence. \n",
    "\n",
    "Let's implement a simple function to calculate the perplexity, taking as input the log-probability (calculated by a language model) and the length of a sequence (in number of tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(log_prob, length):\n",
    "    H =  - (log_prob/ math.log(2)) / length  #change of log-base from base 10 to base 2, then normalize by length\n",
    "    return math.pow(2,H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the perplexity of both language models by simply calling the respective functions, and feed the results to the perplexity function above along with the length of the example sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.1537652847171"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(bigram_prob_for_sequence(example_sequence),len(example_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.7174649447508"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(unigram_prob_for_sequence(example_sequence),len(example_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 10:\n",
    "Which of the two models has a lower perplexity?  measure the perplixity of both models using the example text sentences from previous quizes. \n",
    "\n",
    "Can you find a text string where the unigram model has a lower perplexity than the bigram model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
