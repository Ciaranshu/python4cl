{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xv4NPtgAReWr"
   },
   "source": [
    "Welcome to module 2.1. In this module, we will use a popular Python module `scikit-learn` to build a sentiment classifier with Naive Bayes! We will introduce the concept of feature as numerical representation of the input data. We will experiment with different types of features to investiage their impact on training. \n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "## ❓ Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ra8qD88ReWu"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii18vTN8ReWx"
   },
   "source": [
    "# Sentiment Analysis Task Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AplzSdOIReWz"
   },
   "source": [
    "Our task of focus today is a popular NLP classification task: sentiment analysis. What exactly is sentiment? Sentiment relates to the meaning of a word or sequence of words and is usually associated with an opinion or emotion. And analysis? Well, this is the process of looking at data and making inferences; in this case, using machine learning to learn and predict whether a movie review is positive or negative.\n",
    "\n",
    "In this section, we will replicate the experiments from the paper: Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques (https://www.aclweb.org/anthology/W02-1011.pdf). We will extract a number of features including unigrams, bigrams, pos tags etc., and train Naive Bayes models on these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bav1A-D6ReW1"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2Mgv4bfReW3"
   },
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBsHahXeScH7"
   },
   "source": [
    "As usual, we download the files for the notebook from Github. If you're running this notebook locally or on Binder, you may skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kFuAQbpSdRf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-24 13:04:10--  https://github.com/cambridgeltl/python4cl/raw/module_2.1/module_2/module_2.1/data.zip\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/cambridgeltl/python4cl/module_2.1/module_2/module_2.1/data.zip [following]\n",
      "--2020-11-24 13:04:11--  https://raw.githubusercontent.com/cambridgeltl/python4cl/module_2.1/module_2/module_2.1/data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4848090 (4.6M) [application/zip]\n",
      "Saving to: ‘data.zip.5’\n",
      "\n",
      "data.zip.5          100%[===================>]   4.62M  1.52MB/s    in 3.0s    \n",
      "\n",
      "2020-11-24 13:04:15 (1.52 MB/s) - ‘data.zip.5’ saved [4848090/4848090]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/cambridgeltl/python4cl/raw/module_2.1/module_2/module_2.1/data.zip\n",
    "!unzip -n -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFew9K1WReW5"
   },
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file “cv000_29416.txt” as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QngDdJeGReW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mmv7wS0cReXH"
   },
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOSa4383ReXI"
   },
   "source": [
    "We can turn this into a function called load_doc() that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GZg4B9FReXL"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: the filename to extract text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    text in strings\n",
    "    \"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_13EAmfReXS"
   },
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jL_pY341ReXT"
   },
   "source": [
    "Let's first read in these positive and negative files and store them as two list of texts. To navigate the files, we can use Python's `os` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60kbNDGiReXV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv676_22202.txt\n",
      "Loaded cv839_22807.txt\n",
      "Loaded cv155_7845.txt\n",
      "Loaded cv465_23401.txt\n",
      "Loaded cv398_17047.txt\n",
      "Loaded cv206_15893.txt\n",
      "Loaded cv037_19798.txt\n",
      "Loaded cv279_19452.txt\n",
      "Loaded cv646_16817.txt\n",
      "Loaded cv756_23676.txt\n",
      "Loaded cv823_17055.txt\n",
      "Loaded cv747_18189.txt\n",
      "Loaded cv258_5627.txt\n",
      "Loaded cv948_25870.txt\n",
      "Loaded cv744_10091.txt\n",
      "Loaded cv754_7709.txt\n",
      "Loaded cv838_25886.txt\n",
      "Loaded cv131_11568.txt\n",
      "Loaded cv401_13758.txt\n",
      "Loaded cv523_18285.txt\n",
      "Loaded cv073_23039.txt\n",
      "Loaded cv688_7884.txt\n",
      "Loaded cv664_4264.txt\n",
      "Loaded cv461_21124.txt\n",
      "Loaded cv909_9973.txt\n",
      "Loaded cv939_11247.txt\n",
      "Loaded cv368_11090.txt\n",
      "Loaded cv185_28372.txt\n",
      "Loaded cv749_18960.txt\n",
      "Loaded cv836_14311.txt\n",
      "Loaded cv322_21820.txt\n",
      "Loaded cv789_12991.txt\n",
      "Loaded cv617_9561.txt\n",
      "Loaded cv288_20212.txt\n",
      "Loaded cv464_17076.txt\n",
      "Loaded cv904_25663.txt\n",
      "Loaded cv866_29447.txt\n",
      "Loaded cv429_7937.txt\n",
      "Loaded cv212_10054.txt\n",
      "Loaded cv007_4992.txt\n",
      "Loaded cv522_5418.txt\n",
      "Loaded cv109_22599.txt\n",
      "Loaded cv753_11812.txt\n",
      "Loaded cv312_29308.txt\n",
      "Loaded cv294_12695.txt\n",
      "Loaded cv886_19210.txt\n",
      "Loaded cv479_5450.txt\n",
      "Loaded cv867_18362.txt\n",
      "Loaded cv260_15652.txt\n",
      "Loaded cv313_19337.txt\n",
      "Loaded cv317_25111.txt\n",
      "Loaded cv506_17521.txt\n",
      "Loaded cv602_8830.txt\n",
      "Loaded cv710_23745.txt\n",
      "Loaded cv971_11790.txt\n",
      "Loaded cv098_17021.txt\n",
      "Loaded cv960_28877.txt\n",
      "Loaded cv423_12089.txt\n",
      "Loaded cv887_5306.txt\n",
      "Loaded cv291_26844.txt\n",
      "Loaded cv049_21917.txt\n",
      "Loaded cv382_8393.txt\n",
      "Loaded cv915_9342.txt\n",
      "Loaded cv217_28707.txt\n",
      "Loaded cv729_10475.txt\n",
      "Loaded cv862_15924.txt\n",
      "Loaded cv034_29446.txt\n",
      "Loaded cv560_18608.txt\n",
      "Loaded cv684_12727.txt\n",
      "Loaded cv627_12603.txt\n",
      "Loaded cv498_9288.txt\n",
      "Loaded cv044_18429.txt\n",
      "Loaded cv359_6751.txt\n",
      "Loaded cv537_13516.txt\n",
      "Loaded cv089_12222.txt\n",
      "Loaded cv784_16077.txt\n",
      "Loaded cv801_26335.txt\n",
      "Loaded cv692_17026.txt\n",
      "Loaded cv531_26838.txt\n",
      "Loaded cv913_29127.txt\n",
      "Loaded cv319_16459.txt\n",
      "Loaded cv377_8440.txt\n",
      "Loaded cv943_23547.txt\n",
      "Loaded cv691_5090.txt\n",
      "Loaded cv604_23339.txt\n",
      "Loaded cv327_21743.txt\n",
      "Loaded cv718_12227.txt\n",
      "Loaded cv686_15553.txt\n",
      "Loaded cv195_16146.txt\n",
      "Loaded cv170_29808.txt\n",
      "Loaded cv254_5870.txt\n",
      "Loaded cv435_24355.txt\n",
      "Loaded cv184_26935.txt\n",
      "Loaded cv388_12810.txt\n",
      "Loaded cv835_20531.txt\n",
      "Loaded cv541_28683.txt\n",
      "Loaded cv609_25038.txt\n",
      "Loaded cv708_28539.txt\n",
      "Loaded cv386_10229.txt\n",
      "Loaded cv392_12238.txt\n",
      "Loaded cv822_21545.txt\n",
      "Loaded cv758_9740.txt\n",
      "Loaded cv819_9567.txt\n",
      "Loaded cv704_17622.txt\n",
      "Loaded cv458_9000.txt\n",
      "Loaded cv679_28221.txt\n",
      "Loaded cv097_26081.txt\n",
      "Loaded cv740_13643.txt\n",
      "Loaded cv773_20264.txt\n",
      "Loaded cv600_25043.txt\n",
      "Loaded cv416_12048.txt\n",
      "Loaded cv702_12371.txt\n",
      "Loaded cv527_10338.txt\n",
      "Loaded cv055_8926.txt\n",
      "Loaded cv798_24779.txt\n",
      "Loaded cv371_8197.txt\n",
      "Loaded cv549_22771.txt\n",
      "Loaded cv283_11963.txt\n",
      "Loaded cv430_18662.txt\n",
      "Loaded cv921_13988.txt\n",
      "Loaded cv730_10729.txt\n",
      "Loaded cv107_25639.txt\n",
      "Loaded cv705_11973.txt\n",
      "Loaded cv421_9752.txt\n",
      "Loaded cv447_27334.txt\n",
      "Loaded cv739_12179.txt\n",
      "Loaded cv240_15948.txt\n",
      "Loaded cv122_7891.txt\n",
      "Loaded cv460_11723.txt\n",
      "Loaded cv893_26731.txt\n",
      "Loaded cv234_22123.txt\n",
      "Loaded cv803_8584.txt\n",
      "Loaded cv379_23167.txt\n",
      "Loaded cv198_19313.txt\n",
      "Loaded cv556_16563.txt\n",
      "Loaded cv632_9704.txt\n",
      "Loaded cv853_29119.txt\n",
      "Loaded cv023_13847.txt\n",
      "Loaded cv410_25624.txt\n",
      "Loaded cv490_18986.txt\n",
      "Loaded cv755_24881.txt\n",
      "Loaded cv930_14949.txt\n",
      "Loaded cv645_17078.txt\n",
      "Loaded cv497_27086.txt\n",
      "Loaded cv876_9633.txt\n",
      "Loaded cv841_3367.txt\n",
      "Loaded cv318_11146.txt\n",
      "Loaded cv189_24248.txt\n",
      "Loaded cv888_25678.txt\n",
      "Loaded cv210_9557.txt\n",
      "Loaded cv815_23466.txt\n",
      "Loaded cv227_25406.txt\n",
      "Loaded cv748_14044.txt\n",
      "Loaded cv667_19672.txt\n",
      "Loaded cv437_24070.txt\n",
      "Loaded cv988_20168.txt\n",
      "Loaded cv027_26270.txt\n",
      "Loaded cv918_27080.txt\n",
      "Loaded cv768_12709.txt\n",
      "Loaded cv036_18385.txt\n",
      "Loaded cv938_10706.txt\n",
      "Loaded cv362_16985.txt\n",
      "Loaded cv849_17215.txt\n",
      "Loaded cv144_5010.txt\n",
      "Loaded cv320_9693.txt\n",
      "Loaded cv492_19370.txt\n",
      "Loaded cv112_12178.txt\n",
      "Loaded cv558_29376.txt\n",
      "Loaded cv141_17179.txt\n",
      "Loaded cv168_7435.txt\n",
      "Loaded cv944_15042.txt\n",
      "Loaded cv308_5079.txt\n",
      "Loaded cv752_25330.txt\n",
      "Loaded cv356_26170.txt\n",
      "Loaded cv738_10287.txt\n",
      "Loaded cv731_3968.txt\n",
      "Loaded cv519_16239.txt\n",
      "Loaded cv847_20855.txt\n",
      "Loaded cv687_22207.txt\n",
      "Loaded cv854_18955.txt\n",
      "Loaded cv050_12128.txt\n",
      "Loaded cv179_9533.txt\n",
      "Loaded cv489_19046.txt\n",
      "Loaded cv127_16451.txt\n",
      "Loaded cv496_11185.txt\n",
      "Loaded cv614_11320.txt\n",
      "Loaded cv140_7963.txt\n",
      "Loaded cv987_7394.txt\n",
      "Loaded cv851_21895.txt\n",
      "Loaded cv228_5644.txt\n",
      "Loaded cv542_20359.txt\n",
      "Loaded cv225_29083.txt\n",
      "Loaded cv063_28852.txt\n",
      "Loaded cv341_25667.txt\n",
      "Loaded cv563_18610.txt\n",
      "Loaded cv022_14227.txt\n",
      "Loaded cv520_13297.txt\n",
      "Loaded cv102_8306.txt\n",
      "Loaded cv150_14279.txt\n",
      "Loaded cv858_20266.txt\n",
      "Loaded cv936_17473.txt\n",
      "Loaded cv762_15604.txt\n",
      "Loaded cv891_6035.txt\n",
      "Loaded cv445_26683.txt\n",
      "Loaded cv946_20084.txt\n",
      "Loaded cv507_9509.txt\n",
      "Loaded cv330_29675.txt\n",
      "Loaded cv350_22139.txt\n",
      "Loaded cv837_27232.txt\n",
      "Loaded cv347_14722.txt\n",
      "Loaded cv483_18103.txt\n",
      "Loaded cv287_17410.txt\n",
      "Loaded cv777_10247.txt\n",
      "Loaded cv630_10152.txt\n",
      "Loaded cv975_11920.txt\n",
      "Loaded cv633_29730.txt\n",
      "Loaded cv028_26964.txt\n",
      "Loaded cv101_10537.txt\n",
      "Loaded cv171_15164.txt\n",
      "Loaded cv554_14678.txt\n",
      "Loaded cv963_7208.txt\n",
      "Loaded cv775_17966.txt\n",
      "Loaded cv564_12011.txt\n",
      "Loaded cv448_16409.txt\n",
      "Loaded cv681_9744.txt\n",
      "Loaded cv865_28796.txt\n",
      "Loaded cv770_11061.txt\n",
      "Loaded cv264_14108.txt\n",
      "Loaded cv650_15974.txt\n",
      "Loaded cv714_19704.txt\n",
      "Loaded cv607_8235.txt\n",
      "Loaded cv699_7773.txt\n",
      "Loaded cv205_9676.txt\n",
      "Loaded cv297_10104.txt\n",
      "Loaded cv491_12992.txt\n",
      "Loaded cv310_14568.txt\n",
      "Loaded cv475_22978.txt\n",
      "Loaded cv824_9335.txt\n",
      "Loaded cv941_10718.txt\n",
      "Loaded cv965_26688.txt\n",
      "Loaded cv176_14196.txt\n",
      "Loaded cv349_15032.txt\n",
      "Loaded cv612_5396.txt\n",
      "Loaded cv053_23117.txt\n",
      "Loaded cv040_8829.txt\n",
      "Loaded cv370_5338.txt\n",
      "Loaded cv203_19052.txt\n",
      "Loaded cv623_16988.txt\n",
      "Loaded cv937_9816.txt\n",
      "Loaded cv394_5311.txt\n",
      "Loaded cv054_4101.txt\n",
      "Loaded cv139_14236.txt\n",
      "Loaded cv134_23300.txt\n",
      "Loaded cv580_15681.txt\n",
      "Loaded cv986_15092.txt\n",
      "Loaded cv611_2253.txt\n",
      "Loaded cv816_15257.txt\n",
      "Loaded cv393_29234.txt\n",
      "Loaded cv326_14777.txt\n",
      "Loaded cv302_26481.txt\n",
      "Loaded cv010_29063.txt\n",
      "Loaded cv415_23674.txt\n",
      "Loaded cv932_14854.txt\n",
      "Loaded cv003_12683.txt\n",
      "Loaded cv962_9813.txt\n",
      "Loaded cv115_26443.txt\n",
      "Loaded cv409_29625.txt\n",
      "Loaded cv180_17823.txt\n",
      "Loaded cv502_10970.txt\n",
      "Loaded cv615_15734.txt\n",
      "Loaded cv735_20218.txt\n",
      "Loaded cv482_11233.txt\n",
      "Loaded cv324_7502.txt\n",
      "Loaded cv926_18471.txt\n",
      "Loaded cv844_13890.txt\n",
      "Loaded cv845_15886.txt\n",
      "Loaded cv257_11856.txt\n",
      "Loaded cv335_16299.txt\n",
      "Loaded cv634_11989.txt\n",
      "Loaded cv200_29006.txt\n",
      "Loaded cv192_16079.txt\n",
      "Loaded cv759_15091.txt\n",
      "Loaded cv009_29417.txt\n",
      "Loaded cv123_12165.txt\n",
      "Loaded cv024_7033.txt\n",
      "Loaded cv655_12055.txt\n",
      "Loaded cv366_10709.txt\n",
      "Loaded cv208_9475.txt\n",
      "Loaded cv931_18783.txt\n",
      "Loaded cv656_25395.txt\n",
      "Loaded cv828_21392.txt\n",
      "Loaded cv440_16891.txt\n",
      "Loaded cv146_19587.txt\n",
      "Loaded cv671_5164.txt\n",
      "Loaded cv113_24354.txt\n",
      "Loaded cv501_12675.txt\n",
      "Loaded cv493_14135.txt\n",
      "Loaded cv622_8583.txt\n",
      "Loaded cv643_29282.txt\n",
      "Loaded cv703_17948.txt\n",
      "Loaded cv121_18621.txt\n",
      "Loaded cv378_21982.txt\n",
      "Loaded cv902_13217.txt\n",
      "Loaded cv162_10977.txt\n",
      "Loaded cv922_10185.txt\n",
      "Loaded cv105_19135.txt\n",
      "Loaded cv006_17022.txt\n",
      "Loaded cv526_12868.txt\n",
      "Loaded cv390_12187.txt\n",
      "Loaded cv885_13390.txt\n",
      "Loaded cv778_18629.txt\n",
      "Loaded cv535_21183.txt\n",
      "Loaded cv088_25274.txt\n",
      "Loaded cv128_29444.txt\n",
      "Loaded cv525_17930.txt\n",
      "Loaded cv651_11120.txt\n",
      "Loaded cv905_28965.txt\n",
      "Loaded cv592_23391.txt\n",
      "Loaded cv665_29386.txt\n",
      "Loaded cv137_17020.txt\n",
      "Loaded cv534_15683.txt\n",
      "Loaded cv295_17060.txt\n",
      "Loaded cv411_16799.txt\n",
      "Loaded cv499_11407.txt\n",
      "Loaded cv782_21078.txt\n",
      "Loaded cv201_7421.txt\n",
      "Loaded cv883_27621.txt\n",
      "Loaded cv292_7804.txt\n",
      "Loaded cv282_6833.txt\n",
      "Loaded cv065_16909.txt\n",
      "Loaded cv776_21934.txt\n",
      "Loaded cv487_11058.txt\n",
      "Loaded cv969_14760.txt\n",
      "Loaded cv964_5794.txt\n",
      "Loaded cv925_9459.txt\n",
      "Loaded cv979_2029.txt\n",
      "Loaded cv553_26965.txt\n",
      "Loaded cv638_29394.txt\n",
      "Loaded cv795_10291.txt\n",
      "Loaded cv610_24153.txt\n",
      "Loaded cv725_10266.txt\n",
      "Loaded cv035_3343.txt\n",
      "Loaded cv923_11951.txt\n",
      "Loaded cv884_15230.txt\n",
      "Loaded cv352_5414.txt\n",
      "Loaded cv025_29825.txt\n",
      "Loaded cv426_10976.txt\n",
      "Loaded cv223_28923.txt\n",
      "Loaded cv074_7188.txt\n",
      "Loaded cv467_26610.txt\n",
      "Loaded cv114_19501.txt\n",
      "Loaded cv038_9781.txt\n",
      "Loaded cv296_13146.txt\n",
      "Loaded cv514_12173.txt\n",
      "Loaded cv790_16202.txt\n",
      "Loaded cv015_29356.txt\n",
      "Loaded cv547_18043.txt\n",
      "Loaded cv270_5873.txt\n",
      "Loaded cv529_10972.txt\n",
      "Loaded cv485_26879.txt\n",
      "Loaded cv736_24947.txt\n",
      "Loaded cv271_15364.txt\n",
      "Loaded cv289_6239.txt\n",
      "Loaded cv344_5376.txt\n",
      "Loaded cv745_14009.txt\n",
      "Loaded cv165_2389.txt\n",
      "Loaded cv585_23576.txt\n",
      "Loaded cv384_18536.txt\n",
      "Loaded cv763_16486.txt\n",
      "Loaded cv442_15499.txt\n",
      "Loaded cv033_25680.txt\n",
      "Loaded cv933_24953.txt\n",
      "Loaded cv641_13412.txt\n",
      "Loaded cv216_20165.txt\n",
      "Loaded cv601_24759.txt\n",
      "Loaded cv572_20053.txt\n",
      "Loaded cv894_22140.txt\n",
      "Loaded cv618_9469.txt\n",
      "Loaded cv376_20883.txt\n",
      "Loaded cv860_15520.txt\n",
      "Loaded cv914_2856.txt\n",
      "Loaded cv983_24219.txt\n",
      "Loaded cv583_29465.txt\n",
      "Loaded cv584_29549.txt\n",
      "Loaded cv166_11959.txt\n",
      "Loaded cv959_16218.txt\n",
      "Loaded cv004_12641.txt\n",
      "Loaded cv706_25883.txt\n",
      "Loaded cv579_12542.txt\n",
      "Loaded cv947_11316.txt\n",
      "Loaded cv929_1841.txt\n",
      "Loaded cv713_29002.txt\n",
      "Loaded cv226_26692.txt\n",
      "Loaded cv561_9484.txt\n",
      "Loaded cv951_11816.txt\n",
      "Loaded cv495_16121.txt\n",
      "Loaded cv420_28631.txt\n",
      "Loaded cv761_13769.txt\n",
      "Loaded cv346_19198.txt\n",
      "Loaded cv106_18379.txt\n",
      "Loaded cv389_9611.txt\n",
      "Loaded cv488_21453.txt\n",
      "Loaded cv850_18185.txt\n",
      "Loaded cv129_18373.txt\n",
      "Loaded cv436_20564.txt\n",
      "Loaded cv032_23718.txt\n",
      "Loaded cv087_2145.txt\n",
      "Loaded cv075_6250.txt\n",
      "Loaded cv303_27366.txt\n",
      "Loaded cv970_19532.txt\n",
      "Loaded cv174_9735.txt\n",
      "Loaded cv700_23163.txt\n",
      "Loaded cv690_5425.txt\n",
      "Loaded cv781_5358.txt\n",
      "Loaded cv910_21930.txt\n",
      "Loaded cv571_29292.txt\n",
      "Loaded cv890_3515.txt\n",
      "Loaded cv047_18725.txt\n",
      "Loaded cv605_12730.txt\n",
      "Loaded cv454_21961.txt\n",
      "Loaded cv280_8651.txt\n",
      "Loaded cv869_24782.txt\n",
      "Loaded cv920_29423.txt\n",
      "Loaded cv414_11161.txt\n",
      "Loaded cv181_16083.txt\n",
      "Loaded cv433_10443.txt\n",
      "Loaded cv693_19147.txt\n",
      "Loaded cv472_29140.txt\n",
      "Loaded cv990_12443.txt\n",
      "Loaded cv675_22871.txt\n",
      "Loaded cv809_5012.txt\n",
      "Loaded cv232_16768.txt\n",
      "Loaded cv298_24487.txt\n",
      "Loaded cv787_15277.txt\n",
      "Loaded cv056_14663.txt\n",
      "Loaded cv404_21805.txt\n",
      "Loaded cv059_28723.txt\n",
      "Loaded cv451_11502.txt\n",
      "Loaded cv029_19943.txt\n",
      "Loaded cv016_4348.txt\n",
      "Loaded cv546_12723.txt\n",
      "Loaded cv286_26156.txt\n",
      "Loaded cv120_3793.txt\n",
      "Loaded cv061_9321.txt\n",
      "Loaded cv238_14285.txt\n",
      "Loaded cv143_21158.txt\n",
      "Loaded cv157_29302.txt\n",
      "Loaded cv624_11601.txt\n",
      "Loaded cv694_4526.txt\n",
      "Loaded cv757_10668.txt\n",
      "Loaded cv950_13478.txt\n",
      "Loaded cv562_10847.txt\n",
      "Loaded cv608_24647.txt\n",
      "Loaded cv241_24602.txt\n",
      "Loaded cv746_10471.txt\n",
      "Loaded cv889_22670.txt\n",
      "Loaded cv315_12638.txt\n",
      "Loaded cv829_21725.txt\n",
      "Loaded cv202_11382.txt\n",
      "Loaded cv796_17243.txt\n",
      "Loaded cv071_12969.txt\n",
      "Loaded cv082_11979.txt\n",
      "Loaded cv870_18090.txt\n",
      "Loaded cv043_16808.txt\n",
      "Loaded cv391_11615.txt\n",
      "Loaded cv380_8164.txt\n",
      "Loaded cv997_5152.txt\n",
      "Loaded cv998_15691.txt\n",
      "Loaded cv512_17618.txt\n",
      "Loaded cv640_5380.txt\n",
      "Loaded cv550_23226.txt\n",
      "Loaded cv548_18944.txt\n",
      "Loaded cv701_15880.txt\n",
      "Loaded cv954_19932.txt\n",
      "Loaded cv019_16117.txt\n",
      "Loaded cv039_5963.txt\n",
      "Loaded cv626_7907.txt\n",
      "Loaded cv689_13701.txt\n",
      "Loaded cv117_25625.txt\n",
      "Loaded cv518_14798.txt\n",
      "Loaded cv154_9562.txt\n",
      "Loaded cv148_18084.txt\n",
      "Loaded cv428_12202.txt\n",
      "Loaded cv177_10904.txt\n",
      "Loaded cv337_29061.txt\n",
      "Loaded cv374_26455.txt\n",
      "Loaded cv357_14710.txt\n",
      "Loaded cv169_24973.txt\n",
      "Loaded cv727_5006.txt\n",
      "Loaded cv861_12809.txt\n",
      "Loaded cv955_26154.txt\n",
      "Loaded cv982_22209.txt\n",
      "Loaded cv880_29629.txt\n",
      "Loaded cv792_3257.txt\n",
      "Loaded cv325_18330.txt\n",
      "Loaded cv741_12765.txt\n",
      "Loaded cv263_20693.txt\n",
      "Loaded cv539_21865.txt\n",
      "Loaded cv299_17950.txt\n",
      "Loaded cv400_20631.txt\n",
      "Loaded cv825_5168.txt\n",
      "Loaded cv597_26744.txt\n",
      "Loaded cv716_11153.txt\n",
      "Loaded cv996_12447.txt\n",
      "Loaded cv220_28906.txt\n",
      "Loaded cv820_24157.txt\n",
      "Loaded cv272_20313.txt\n",
      "Loaded cv682_17947.txt\n",
      "Loaded cv685_5710.txt\n",
      "Loaded cv978_22192.txt\n",
      "Loaded cv248_15672.txt\n",
      "Loaded cv233_17614.txt\n",
      "Loaded cv647_15275.txt\n",
      "Loaded cv698_16930.txt\n",
      "Loaded cv194_12855.txt\n",
      "Loaded cv806_9405.txt\n",
      "Loaded cv193_5393.txt\n",
      "Loaded cv405_21868.txt\n",
      "Loaded cv707_11421.txt\n",
      "Loaded cv072_5928.txt\n",
      "Loaded cv808_13773.txt\n",
      "Loaded cv147_22625.txt\n",
      "Loaded cv892_18788.txt\n",
      "Loaded cv267_16618.txt\n",
      "Loaded cv441_15276.txt\n",
      "Loaded cv259_11827.txt\n",
      "Loaded cv161_12224.txt\n",
      "Loaded cv896_17819.txt\n",
      "Loaded cv424_9268.txt\n",
      "Loaded cv908_17779.txt\n",
      "Loaded cv639_10797.txt\n",
      "Loaded cv338_9183.txt\n",
      "Loaded cv907_3193.txt\n",
      "Loaded cv574_23191.txt\n",
      "Loaded cv596_4367.txt\n",
      "Loaded cv477_23530.txt\n",
      "Loaded cv080_14899.txt\n",
      "Loaded cv385_29621.txt\n",
      "Loaded cv239_29828.txt\n",
      "Loaded cv246_28668.txt\n",
      "Loaded cv069_11613.txt\n",
      "Loaded cv079_12766.txt\n",
      "Loaded cv152_9052.txt\n",
      "Loaded cv672_27988.txt\n",
      "Loaded cv606_17672.txt\n",
      "Loaded cv419_14799.txt\n",
      "Loaded cv321_14191.txt\n",
      "Loaded cv917_29484.txt\n",
      "Loaded cv555_25047.txt\n",
      "Loaded cv008_29326.txt\n",
      "Loaded cv927_11471.txt\n",
      "Loaded cv231_11028.txt\n",
      "Loaded cv160_10848.txt\n",
      "Loaded cv953_7078.txt\n",
      "Loaded cv457_19546.txt\n",
      "Loaded cv237_20635.txt\n",
      "Loaded cv042_11927.txt\n",
      "Loaded cv742_8279.txt\n",
      "Loaded cv629_16604.txt\n",
      "Loaded cv783_14724.txt\n",
      "Loaded cv993_29565.txt\n",
      "Loaded cv935_24977.txt\n",
      "Loaded cv387_12391.txt\n",
      "Loaded cv972_26837.txt\n",
      "Loaded cv103_11943.txt\n",
      "Loaded cv142_23657.txt\n",
      "Loaded cv306_10859.txt\n",
      "Loaded cv995_23113.txt\n",
      "Loaded cv274_26379.txt\n",
      "Loaded cv949_21565.txt\n",
      "Loaded cv576_15688.txt\n",
      "Loaded cv721_28993.txt\n",
      "Loaded cv508_17742.txt\n",
      "Loaded cv104_19176.txt\n",
      "Loaded cv709_11173.txt\n",
      "Loaded cv723_9002.txt\n",
      "Loaded cv794_17353.txt\n",
      "Loaded cv985_5964.txt\n",
      "Loaded cv879_16585.txt\n",
      "Loaded cv831_16325.txt\n",
      "Loaded cv020_9234.txt\n",
      "Loaded cv785_23748.txt\n",
      "Loaded cv275_28725.txt\n",
      "Loaded cv842_5702.txt\n",
      "Loaded cv543_5107.txt\n",
      "Loaded cv532_6495.txt\n",
      "Loaded cv187_14112.txt\n",
      "Loaded cv011_13044.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv450_8319.txt\n",
      "Loaded cv247_14668.txt\n",
      "Loaded cv236_12427.txt\n",
      "Loaded cv827_19479.txt\n",
      "Loaded cv658_11186.txt\n",
      "Loaded cv712_24217.txt\n",
      "Loaded cv582_6678.txt\n",
      "Loaded cv030_22893.txt\n",
      "Loaded cv094_27868.txt\n",
      "Loaded cv786_23608.txt\n",
      "Loaded cv151_17231.txt\n",
      "Loaded cv364_14254.txt\n",
      "Loaded cv276_17126.txt\n",
      "Loaded cv945_13012.txt\n",
      "Loaded cv767_15673.txt\n",
      "Loaded cv167_18094.txt\n",
      "Loaded cv149_17084.txt\n",
      "Loaded cv158_10914.txt\n",
      "Loaded cv334_0074.txt\n",
      "Loaded cv956_12547.txt\n",
      "Loaded cv589_12853.txt\n",
      "Loaded cv680_10533.txt\n",
      "Loaded cv474_10682.txt\n",
      "Loaded cv354_8573.txt\n",
      "Loaded cv834_23192.txt\n",
      "Loaded cv649_13947.txt\n",
      "Loaded cv108_17064.txt\n",
      "Loaded cv268_20288.txt\n",
      "Loaded cv126_28821.txt\n",
      "Loaded cv912_5562.txt\n",
      "Loaded cv791_17995.txt\n",
      "Loaded cv311_17708.txt\n",
      "Loaded cv521_1730.txt\n",
      "Loaded cv697_12106.txt\n",
      "Loaded cv256_16529.txt\n",
      "Loaded cv463_10846.txt\n",
      "Loaded cv780_8467.txt\n",
      "Loaded cv406_22199.txt\n",
      "Loaded cv715_19246.txt\n",
      "Loaded cv456_20370.txt\n",
      "Loaded cv545_12848.txt\n",
      "Loaded cv728_17931.txt\n",
      "Loaded cv224_18875.txt\n",
      "Loaded cv586_8048.txt\n",
      "Loaded cv397_28890.txt\n",
      "Loaded cv984_14006.txt\n",
      "Loaded cv934_20426.txt\n",
      "Loaded cv578_16825.txt\n",
      "Loaded cv575_22598.txt\n",
      "Loaded cv737_28733.txt\n",
      "Loaded cv057_7962.txt\n",
      "Loaded cv631_4782.txt\n",
      "Loaded cv221_27081.txt\n",
      "Loaded cv262_13812.txt\n",
      "Loaded cv446_12209.txt\n",
      "Loaded cv013_10494.txt\n",
      "Loaded cv095_28730.txt\n",
      "Loaded cv833_11961.txt\n",
      "Loaded cv856_28882.txt\n",
      "Loaded cv215_23246.txt\n",
      "Loaded cv812_19051.txt\n",
      "Loaded cv875_5622.txt\n",
      "Loaded cv573_29384.txt\n",
      "Loaded cv281_24711.txt\n",
      "Loaded cv348_19207.txt\n",
      "Loaded cv734_22821.txt\n",
      "Loaded cv253_10190.txt\n",
      "Loaded cv636_16954.txt\n",
      "Loaded cv957_9059.txt\n",
      "Loaded cv145_12239.txt\n",
      "Loaded cv332_17997.txt\n",
      "Loaded cv802_28381.txt\n",
      "Loaded cv026_29229.txt\n",
      "Loaded cv810_13660.txt\n",
      "Loaded cv229_15200.txt\n",
      "Loaded cv014_15600.txt\n",
      "Loaded cv439_17633.txt\n",
      "Loaded cv422_9632.txt\n",
      "Loaded cv900_10800.txt\n",
      "Loaded cv530_17949.txt\n",
      "Loaded cv455_28866.txt\n",
      "Loaded cv204_8930.txt\n",
      "Loaded cv453_10911.txt\n",
      "Loaded cv625_13518.txt\n",
      "Loaded cv805_21128.txt\n",
      "Loaded cv471_18405.txt\n",
      "Loaded cv961_5578.txt\n",
      "Loaded cv425_8603.txt\n",
      "Loaded cv637_13682.txt\n",
      "Loaded cv826_12761.txt\n",
      "Loaded cv251_23901.txt\n",
      "Loaded cv788_26409.txt\n",
      "Loaded cv300_23302.txt\n",
      "Loaded cv567_29420.txt\n",
      "Loaded cv799_19812.txt\n",
      "Loaded cv222_18720.txt\n",
      "Loaded cv769_8565.txt\n",
      "Loaded cv552_0150.txt\n",
      "Loaded cv644_18551.txt\n",
      "Loaded cv540_3092.txt\n",
      "Loaded cv066_11668.txt\n",
      "Loaded cv153_11607.txt\n",
      "Loaded cv991_19973.txt\n",
      "Loaded cv852_27512.txt\n",
      "Loaded cv173_4295.txt\n",
      "Loaded cv365_12442.txt\n",
      "Loaded cv214_13285.txt\n",
      "Loaded cv077_23172.txt\n",
      "Loaded cv459_21834.txt\n",
      "Loaded cv620_2556.txt\n",
      "Loaded cv673_25874.txt\n",
      "Loaded cv722_7571.txt\n",
      "Loaded cv992_12806.txt\n",
      "Loaded cv733_9891.txt\n",
      "Loaded cv807_23024.txt\n",
      "Loaded cv511_10360.txt\n",
      "Loaded cv118_28837.txt\n",
      "Loaded cv942_18509.txt\n",
      "Loaded cv577_28220.txt\n",
      "Loaded cv898_1576.txt\n",
      "Loaded cv598_18184.txt\n",
      "Loaded cv083_25491.txt\n",
      "Loaded cv290_11981.txt\n",
      "Loaded cv678_14887.txt\n",
      "Loaded cv135_12506.txt\n",
      "Loaded cv588_14467.txt\n",
      "Loaded cv654_19345.txt\n",
      "Loaded cv018_21672.txt\n",
      "Loaded cv048_18380.txt\n",
      "Loaded cv207_29141.txt\n",
      "Loaded cv096_12262.txt\n",
      "Loaded cv516_12117.txt\n",
      "Loaded cv369_14245.txt\n",
      "Loaded cv345_9966.txt\n",
      "Loaded cv695_22268.txt\n",
      "Loaded cv484_26169.txt\n",
      "Loaded cv339_22452.txt\n",
      "Loaded cv001_19502.txt\n",
      "Loaded cv163_10110.txt\n",
      "Loaded cv581_20790.txt\n",
      "Loaded cv277_20467.txt\n",
      "Loaded cv358_11557.txt\n",
      "Loaded cv469_21998.txt\n",
      "Loaded cv642_29788.txt\n",
      "Loaded cv976_10724.txt\n",
      "Loaded cv244_22935.txt\n",
      "Loaded cv058_8469.txt\n",
      "Loaded cv307_26382.txt\n",
      "Loaded cv305_9937.txt\n",
      "Loaded cv355_18174.txt\n",
      "Loaded cv772_12971.txt\n",
      "Loaded cv750_10606.txt\n",
      "Loaded cv243_22164.txt\n",
      "Loaded cv764_12701.txt\n",
      "Loaded cv674_11593.txt\n",
      "Loaded cv980_11851.txt\n",
      "Loaded cv351_17029.txt\n",
      "Loaded cv111_12253.txt\n",
      "Loaded cv551_11214.txt\n",
      "Loaded cv402_16097.txt\n",
      "Loaded cv871_25971.txt\n",
      "Loaded cv878_17204.txt\n",
      "Loaded cv989_17297.txt\n",
      "Loaded cv719_5581.txt\n",
      "Loaded cv084_15183.txt\n",
      "Loaded cv188_20687.txt\n",
      "Loaded cv766_7983.txt\n",
      "Loaded cv857_17527.txt\n",
      "Loaded cv340_14776.txt\n",
      "Loaded cv872_13710.txt\n",
      "Loaded cv771_28466.txt\n",
      "Loaded cv230_7913.txt\n",
      "Loaded cv559_0057.txt\n",
      "Loaded cv266_26644.txt\n",
      "Loaded cv125_9636.txt\n",
      "Loaded cv566_8967.txt\n",
      "Loaded cv628_20758.txt\n",
      "Loaded cv218_25651.txt\n",
      "Loaded cv353_19197.txt\n",
      "Loaded cv417_14653.txt\n",
      "Loaded cv999_14636.txt\n",
      "Loaded cv265_11625.txt\n",
      "Loaded cv462_20788.txt\n",
      "Loaded cv093_15606.txt\n",
      "Loaded cv897_11703.txt\n",
      "Loaded cv899_17812.txt\n",
      "Loaded cv068_14810.txt\n",
      "Loaded cv536_27221.txt\n",
      "Loaded cv568_17065.txt\n",
      "Loaded cv940_18935.txt\n",
      "Loaded cv399_28593.txt\n",
      "Loaded cv438_8500.txt\n",
      "Loaded cv683_13047.txt\n",
      "Loaded cv211_9955.txt\n",
      "Loaded cv663_14484.txt\n",
      "Loaded cv662_14791.txt\n",
      "Loaded cv245_8938.txt\n",
      "Loaded cv116_28734.txt\n",
      "Loaded cv594_11945.txt\n",
      "Loaded cv099_11189.txt\n",
      "Loaded cv427_11693.txt\n",
      "Loaded cv510_24758.txt\n",
      "Loaded cv373_21872.txt\n",
      "Loaded cv590_20712.txt\n",
      "Loaded cv657_25835.txt\n",
      "Loaded cv952_26375.txt\n",
      "Loaded cv570_28960.txt\n",
      "Loaded cv219_19874.txt\n",
      "Loaded cv199_9721.txt\n",
      "Loaded cv895_22200.txt\n",
      "Loaded cv967_5626.txt\n",
      "Loaded cv480_21195.txt\n",
      "Loaded cv533_9843.txt\n",
      "Loaded cv958_13020.txt\n",
      "Loaded cv840_18033.txt\n",
      "Loaded cv046_10613.txt\n",
      "Loaded cv743_17023.txt\n",
      "Loaded cv190_27176.txt\n",
      "Loaded cv544_5301.txt\n",
      "Loaded cv342_20917.txt\n",
      "Loaded cv213_20300.txt\n",
      "Loaded cv613_23104.txt\n",
      "Loaded cv868_12799.txt\n",
      "Loaded cv813_6649.txt\n",
      "Loaded cv648_17277.txt\n",
      "Loaded cv811_22646.txt\n",
      "Loaded cv800_13494.txt\n",
      "Loaded cv717_17472.txt\n",
      "Loaded cv336_10363.txt\n",
      "Loaded cv408_5367.txt\n",
      "Loaded cv136_12384.txt\n",
      "Loaded cv859_15689.txt\n",
      "Loaded cv051_10751.txt\n",
      "Loaded cv164_23451.txt\n",
      "Loaded cv587_20532.txt\n",
      "Loaded cv595_26420.txt\n",
      "Loaded cv974_24303.txt\n",
      "Loaded cv814_20316.txt\n",
      "Loaded cv765_20429.txt\n",
      "Loaded cv363_29273.txt\n",
      "Loaded cv881_14767.txt\n",
      "Loaded cv278_14533.txt\n",
      "Loaded cv060_11754.txt\n",
      "Loaded cv504_29120.txt\n",
      "Loaded cv470_17444.txt\n",
      "Loaded cv017_23487.txt\n",
      "Loaded cv666_20301.txt\n",
      "Loaded cv031_19540.txt\n",
      "Loaded cv301_13010.txt\n",
      "Loaded cv599_22197.txt\n",
      "Loaded cv494_18689.txt\n",
      "Loaded cv843_17054.txt\n",
      "Loaded cv906_12332.txt\n",
      "Loaded cv331_8656.txt\n",
      "Loaded cv591_24887.txt\n",
      "Loaded cv021_17313.txt\n",
      "Loaded cv832_24713.txt\n",
      "Loaded cv919_18155.txt\n",
      "Loaded cv090_0049.txt\n",
      "Loaded cv182_7791.txt\n",
      "Loaded cv760_8977.txt\n",
      "Loaded cv395_11761.txt\n",
      "Loaded cv092_27987.txt\n",
      "Loaded cv172_12037.txt\n",
      "Loaded cv503_11196.txt\n",
      "Loaded cv873_19937.txt\n",
      "Loaded cv981_16679.txt\n",
      "Loaded cv209_28973.txt\n",
      "Loaded cv269_23018.txt\n",
      "Loaded cv432_15873.txt\n",
      "Loaded cv418_16562.txt\n",
      "Loaded cv659_21483.txt\n",
      "Loaded cv528_11669.txt\n",
      "Loaded cv293_29731.txt\n",
      "Loaded cv249_12674.txt\n",
      "Loaded cv877_29132.txt\n",
      "Loaded cv903_18981.txt\n",
      "Loaded cv818_10698.txt\n",
      "Loaded cv119_9909.txt\n",
      "Loaded cv635_0984.txt\n",
      "Loaded cv774_15488.txt\n",
      "Loaded cv478_15921.txt\n",
      "Loaded cv660_23140.txt\n",
      "Loaded cv110_27832.txt\n",
      "Loaded cv052_29318.txt\n",
      "Loaded cv314_16095.txt\n",
      "Loaded cv669_24318.txt\n",
      "Loaded cv804_11763.txt\n",
      "Loaded cv711_12687.txt\n",
      "Loaded cv524_24885.txt\n",
      "Loaded cv468_16844.txt\n",
      "Loaded cv846_29359.txt\n",
      "Loaded cv329_29293.txt\n",
      "Loaded cv677_18938.txt\n",
      "Loaded cv064_25842.txt\n",
      "Loaded cv078_16506.txt\n",
      "Loaded cv473_7869.txt\n",
      "Loaded cv328_10908.txt\n",
      "Loaded cv977_4776.txt\n",
      "Loaded cv966_28671.txt\n",
      "Loaded cv309_23737.txt\n",
      "Loaded cv076_26009.txt\n",
      "Loaded cv797_7245.txt\n",
      "Loaded cv444_9975.txt\n",
      "Loaded cv375_9932.txt\n",
      "Loaded cv911_21695.txt\n",
      "Loaded cv557_12237.txt\n",
      "Loaded cv505_12926.txt\n",
      "Loaded cv178_14380.txt\n",
      "Loaded cv186_2396.txt\n",
      "Loaded cv360_8927.txt\n",
      "Loaded cv668_18848.txt\n",
      "Loaded cv316_5972.txt\n",
      "Loaded cv323_29633.txt\n",
      "Loaded cv696_29619.txt\n",
      "Loaded cv449_9126.txt\n",
      "Loaded cv124_3903.txt\n",
      "Loaded cv396_19127.txt\n",
      "Loaded cv100_12406.txt\n",
      "Loaded cv156_11119.txt\n",
      "Loaded cv652_15653.txt\n",
      "Loaded cv500_10722.txt\n",
      "Loaded cv045_25077.txt\n",
      "Loaded cv383_14662.txt\n",
      "Loaded cv304_28489.txt\n",
      "Loaded cv619_13677.txt\n",
      "Loaded cv732_13092.txt\n",
      "Loaded cv261_11855.txt\n",
      "Loaded cv538_28485.txt\n",
      "Loaded cv726_4365.txt\n",
      "Loaded cv476_18402.txt\n",
      "Loaded cv616_29187.txt\n",
      "Loaded cv372_6654.txt\n",
      "Loaded cv973_10171.txt\n",
      "Loaded cv603_18885.txt\n",
      "Loaded cv817_3675.txt\n",
      "Loaded cv515_18484.txt\n",
      "Loaded cv864_3087.txt\n",
      "Loaded cv901_11934.txt\n",
      "Loaded cv070_13249.txt\n",
      "Loaded cv916_17034.txt\n",
      "Loaded cv751_17208.txt\n",
      "Loaded cv720_5383.txt\n",
      "Loaded cv361_28738.txt\n",
      "Loaded cv196_28898.txt\n",
      "Loaded cv670_2666.txt\n",
      "Loaded cv517_20616.txt\n",
      "Loaded cv343_10906.txt\n",
      "Loaded cv621_15984.txt\n",
      "Loaded cv333_9443.txt\n",
      "Loaded cv793_15235.txt\n",
      "Loaded cv848_10061.txt\n",
      "Loaded cv041_22364.txt\n",
      "Loaded cv367_24065.txt\n",
      "Loaded cv509_17354.txt\n",
      "Loaded cv133_18065.txt\n",
      "Loaded cv130_18521.txt\n",
      "Loaded cv242_11354.txt\n",
      "Loaded cv724_15265.txt\n",
      "Loaded cv285_18186.txt\n",
      "Loaded cv132_5423.txt\n",
      "Loaded cv381_21673.txt\n",
      "Loaded cv403_6721.txt\n",
      "Loaded cv452_5179.txt\n",
      "Loaded cv284_20530.txt\n",
      "Loaded cv830_5778.txt\n",
      "Loaded cv138_13903.txt\n",
      "Loaded cv183_19826.txt\n",
      "Loaded cv863_7912.txt\n",
      "Loaded cv431_7538.txt\n",
      "Loaded cv924_29397.txt\n",
      "Loaded cv779_18989.txt\n",
      "Loaded cv250_26462.txt\n",
      "Loaded cv434_5641.txt\n",
      "Loaded cv593_11931.txt\n",
      "Loaded cv994_13229.txt\n",
      "Loaded cv197_29271.txt\n",
      "Loaded cv661_25780.txt\n",
      "Loaded cv513_7236.txt\n",
      "Loaded cv175_7375.txt\n",
      "Loaded cv407_23928.txt\n",
      "Loaded cv968_25413.txt\n",
      "Loaded cv413_7893.txt\n",
      "Loaded cv466_20092.txt\n",
      "Loaded cv928_9478.txt\n",
      "Loaded cv062_24556.txt\n",
      "Loaded cv874_12182.txt\n",
      "Loaded cv005_29357.txt\n",
      "Loaded cv653_2107.txt\n",
      "Loaded cv273_28961.txt\n",
      "Loaded cv565_29403.txt\n",
      "Loaded cv443_22367.txt\n",
      "Loaded cv412_25254.txt\n",
      "Loaded cv855_22134.txt\n",
      "Loaded cv882_10042.txt\n",
      "Loaded cv002_17424.txt\n",
      "Loaded cv481_7930.txt\n",
      "Loaded cv191_29539.txt\n",
      "Loaded cv235_10704.txt\n",
      "Loaded cv000_29416.txt\n",
      "Loaded cv086_19488.txt\n",
      "Loaded cv067_21192.txt\n",
      "Loaded cv569_26750.txt\n",
      "Loaded cv821_29283.txt\n",
      "Loaded cv091_7899.txt\n",
      "Loaded cv486_9788.txt\n",
      "Loaded cv255_15267.txt\n",
      "Loaded cv159_29374.txt\n",
      "Loaded cv085_15286.txt\n",
      "Loaded cv081_18241.txt\n",
      "Loaded cv012_29411.txt\n",
      "Loaded cv252_24974.txt\n"
     ]
    }
   ],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of of documents, where each document is string text\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6SlpyplReXd"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ4Ol0plReXe"
   },
   "source": [
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngOZpkTQReXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf82-7OFReXm"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU3WDGsMReXn"
   },
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LbJ8xKIReXo"
   },
   "source": [
    "So far, each document is represented as text. However, to input the data into a machine learning model, we often need to convert text into numerical representations of features. \n",
    "\n",
    "We will start by introducing the concept of features and feature vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlQMfCX6ReYQ"
   },
   "source": [
    "## What is a feature and what is a feature vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb62eNVSReYR"
   },
   "source": [
    "A feature is an individual measurable property or characteristic of a phenomenon being observed. For sentiment analysis, we can extract many features from a document to predict the sentiment polarity. For example, whether the word 'good' occurs in the document can be a feature. We can assign a number for this feature to indicate occurrence (eg. 1) or absence (eg. 0). We can then combine the values for all the features in the document into a numerical list that we call as a feature ector.  \n",
    "\n",
    "The feature vector has a fixed length corresponding to the number of features. Each dimension represents a feature. For example, to represent unigrams in a document, we can create a vector with the same length of the vocabulary, and the value in each dimension (ie. each position of the list) stores the frequency or presence of a specific word. \n",
    "\n",
    "\n",
    "As an example, suppose we have seven words (apple,banana,red,dog,is,the,and) in the vocabulary which are represented as seven dimensions (features). We also have two documents as in below:\n",
    "\n",
    "document 1: \"the apple is red and the banana is yellow\"\n",
    "document 2: \"the red dog\"\n",
    "\n",
    "To produce a feature vector to represent unigram presence in a document, we can write '1' in a dimension to indicate the word the dimension corresponds to is present in the document, and we will write '0' to indicate the word is not present. Below is a table view of the vectors. \n",
    "\n",
    "\n",
    "|document no.||apple|banana|red|dog|is|the|and\n",
    "|------||------|------|------|------|------|\n",
    "|document 1 ||1|0|1|0|1|1|1|\n",
    "|document 2 ||0|0|1|1|0|1|0|\n",
    "\n",
    "\n",
    "The feature vector for document 1 becomes `[1,0,1,0,1,1,1]`\n",
    "\n",
    "To produce a feature vector that represent frequency of each unigram in a document, we can count the number of occurence of each unigram word and write down the number in the corresponding dimension. \n",
    "\n",
    "|document no.||apple|banana|red|dog|is|the|and\n",
    "|------||------|------|------|------|------|\n",
    "|document 1 ||1|0|1|0|1|2|1|\n",
    "|document 2 ||0|0|1|1|0|1|0|\n",
    "\n",
    "\n",
    "Now the feature vector for document 1 becomes `[1,0,1,0,1,2,1]`\n",
    "\n",
    "When the data consists of more than one document as in our example, we will have multiple feature vectors as representations of our data. We can stack the vectors into a list of vectors. This is often referred to as matrix. \n",
    "In our example, we have a 2*7 matrix where 2 is the number of documents, and 7 is the number of features\n",
    "\n",
    "Presence feature matrix for our toy data: `[[1,0,1,0,1,1,1],[0,0,1,1,0,1,0]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "Following the examples, please write below both the unigram presence and unigram frequenc feature vector for the document text 'the red dog and the red apple'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Unigram presence feature vector: [1,0,1,1,0,1,1]</p>\n",
    "    <p>Unigram frequency feature vector: [1,0,2,1,0,2,1]</p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "What is the matrix consisting of the unigram presence vectors of the following documents:\n",
    "\n",
    "document1: 'the apple is red and the banana is yellow'\n",
    "\n",
    "document2: 'the red dog and the red apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>The matrix is: [[1,0,1,0,1,1,1],[1,0,1,1,0,1,1]] </p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vectors are usually represented as `numpys` arrays in Python. Let's spend some time to understand what `numpy` is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd8XWczOReY1"
   },
   "source": [
    "## Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQhMs1Q1ReY1"
   },
   "source": [
    "A `numpy` array is just like a `list` but with smaller memory and faster access. \n",
    "\n",
    "Below, we introduce several ways to create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcbIondQReY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1 [0. 0.]\n",
      "vector2 [1 2 3]\n",
      "vector3 [3. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create a numpy array of zeros with dimension 2\n",
    "vector1=np.zeros(2)\n",
    "# create a numpy array from a list [1,2,3]\n",
    "vector2=np.array([1,2,3])\n",
    "# create an empty array of dimension 3 with arbitary data\n",
    "vector3=np.empty(3)\n",
    "print ('vector1',vector1)\n",
    "print ('vector2',vector2)\n",
    "print ('vector3',vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTzXgRTuReY6"
   },
   "source": [
    "We can also concatenate two numpy arrays of the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSNpS0TJReY7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 3., 2., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((vector2,vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq3Do1UxReZE"
   },
   "source": [
    "So far, we have created numpy arrays of one dimension. Let's try creating a 2-D array (also called a matrix). We can pass dimension size (also called axes) as (a,b) where a is the number of rows in the matrix, and b specifies the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzQII4NNReZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1 [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "matrix2 [[1 2 3]\n",
      " [2 3 4]]\n",
      "matrix3 []\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "matrix1=np.zeros((3,3)) \n",
    "# this is a matrix of zeros that has 3 vectors, and within each vector there are 4 items. \n",
    "print ('matrix1',matrix1)\n",
    "# a matrix from nested list\n",
    "matrix2=np.array([[1,2,3],[2,3,4]])\n",
    "print ('matrix2',matrix2)\n",
    "# an empty matrix usually used as initialisation. It will print as an empty list\n",
    "matrix3=np.empty((0,4))\n",
    "print ('matrix3',matrix3)\n",
    "#To check the axes of an array, you can retrieve the shape attribute like this:\n",
    "print (matrix2.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJI7IUkMReZJ"
   },
   "source": [
    "Numpy arrays are mutable. Therefore, we can change values in the vector. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ky4YPO0ReZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3]\n",
      "[[1 2 0]\n",
      " [2 3 4]]\n",
      "[[0 2 3]\n",
      " [2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "vector2[0]=0 # change the first item in vector3 to 0\n",
    "print (vector2)\n",
    "matrix2[0][2]=0 # change the third item of the first vector to 0\n",
    "print (matrix2)\n",
    "matrix2[0]=vector2 # change the first vector in matrix2 to vector3\n",
    "print (matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjSFnyhwReZP"
   },
   "source": [
    "We can also slice a numpy array with an index array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r9u1f_TReZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's select the values at index 1,2 of vector2\n",
    "vector2[[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0rSLnjtReXp"
   },
   "source": [
    "## Extracting unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qJxTridReXq"
   },
   "source": [
    "Now let's extract unigram features from our data. First, let’s load one document and look at the raw tokens split by white space. We will use the `load_doc()` function developed in the previous section. We can use the `split()` function to split the loaded document into unigram tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3eSu9nvReXr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeYaaXFDReX5"
   },
   "source": [
    "To keep track of the frequency and presence for each token in the document, we will use the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVTG7u5FReX9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({',': 44, 'the': 38, '.': 34, 'it': 21, 'and': 20, 'to': 16, 'of': 16, 'a': 14, 'that': 13, 'are': 13, 'is': 12, 'but': 10, '\"': 10, 'this': 10, 'there': 10, '(': 9, ')': 9, 'in': 8, 'i': 7, '-': 7, '?': 6, 'movie': 6, 'all': 6, 'they': 5, 'into': 5, 'with': 5, 'pretty': 5, 'film': 5, 'make': 5, 'teen': 4, 'her': 4, 'for': 4, 'on': 4, 'which': 4, 'just': 4, 'its': 4, \"it's\": 4, 'from': 4, 'most': 4, 'over': 4, 'we': 4, ':': 3, 'then': 3, 'get': 3, 'an': 3, 'one': 3, 'has': 3, 'out': 3, 'even': 3, 'so': 3, 'you': 3, 'who': 3, 'like': 3, 'not': 3, '!': 3, 'two': 2, 'go': 2, 'see': 2, \"what's\": 2, 'mind-fuck': 2, 'very': 2, 'cool': 2, 'idea': 2, 'bad': 2, 'what': 2, 'since': 2, 'films': 2, 'your': 2, 'lost': 2, 'highway': 2, 'memento': 2, 'good': 2, \"didn't\": 2, 'have': 2, 'problem': 2, 'simply': 2, 'world': 2, 'audience': 2, 'going': 2, 'coming': 2, 'dead': 2, 'others': 2, 'look': 2, 'scenes': 2, 'things': 2, 'now': 2, \"don't\": 2, 'same': 2, 'again': 2, 'up': 2, 'after': 2, 'biggest': 2, 'secret': 2, 'hide': 2, 'minutes': 2, 'do': 2, 'entertaining': 2, 'or': 2, 'really': 2, 'part': 2, 'actually': 2, 'by': 2, 'strangeness': 2, 'did': 2, 'little': 2, 'sense': 2, 'still': 2, 'more': 2, 'guess': 2, 'before': 2, 'sagemiller': 2, 'away': 2, 'about': 2, 'throughout': 2, 'apparently': 2, 'been': 2, \"doesn't\": 2, 'because': 2, 'way': 2, '7/10': 2, 'crow': 2, '9/10': 2, '10/10': 2, 'plot': 1, 'couples': 1, 'church': 1, 'party': 1, 'drink': 1, 'drive': 1, 'accident': 1, 'guys': 1, 'dies': 1, 'his': 1, 'girlfriend': 1, 'continues': 1, 'him': 1, 'life': 1, 'nightmares': 1, 'deal': 1, 'watch': 1, 'sorta': 1, 'find': 1, 'critique': 1, 'generation': 1, 'touches': 1, 'presents': 1, 'package': 1, 'makes': 1, 'review': 1, 'harder': 1, 'write': 1, 'generally': 1, 'applaud': 1, 'attempt': 1, 'break': 1, 'mold': 1, 'mess': 1, 'head': 1, 'such': 1, '&': 1, 'ways': 1, 'making': 1, 'types': 1, 'these': 1, 'folks': 1, 'snag': 1, 'correctly': 1, 'seem': 1, 'taken': 1, 'neat': 1, 'concept': 1, 'executed': 1, 'terribly': 1, 'problems': 1, 'well': 1, 'main': 1, 'too': 1, 'jumbled': 1, 'starts': 1, 'off': 1, 'normal': 1, 'downshifts': 1, 'fantasy': 1, 'as': 1, 'member': 1, 'no': 1, 'dreams': 1, 'characters': 1, 'back': 1, 'strange': 1, 'apparitions': 1, 'disappearances': 1, 'looooot': 1, 'chase': 1, 'tons': 1, 'weird': 1, 'happen': 1, 'explained': 1, 'personally': 1, 'mind': 1, 'trying': 1, 'unravel': 1, 'every': 1, 'when': 1, 'does': 1, 'give': 1, 'me': 1, 'clue': 1, 'kind': 1, 'fed': 1, 'while': 1, \"film's\": 1, 'obviously': 1, 'got': 1, 'big': 1, 'seems': 1, 'want': 1, 'completely': 1, 'until': 1, 'final': 1, 'five': 1, 'thrilling': 1, 'engaging': 1, 'meantime': 1, 'sad': 1, 'arrow': 1, 'both': 1, 'dig': 1, 'flicks': 1, 'figured': 1, 'half-way': 1, 'point': 1, 'start': 1, 'bit': 1, 'bottom': 1, 'line': 1, 'movies': 1, 'should': 1, 'always': 1, 'sure': 1, 'given': 1, 'password': 1, 'enter': 1, 'understanding': 1, 'mean': 1, 'showing': 1, 'melissa': 1, 'running': 1, 'visions': 1, '20': 1, 'plain': 1, 'lazy': 1, 'okay': 1, 'people': 1, 'chasing': 1, 'know': 1, 'need': 1, 'how': 1, 'giving': 1, 'us': 1, 'different': 1, 'offering': 1, 'further': 1, 'insight': 1, 'down': 1, 'studio': 1, 'took': 1, 'director': 1, 'chopped': 1, 'themselves': 1, 'shows': 1, \"might've\": 1, 'decent': 1, 'here': 1, 'somewhere': 1, 'suits': 1, 'decided': 1, 'turning': 1, 'music': 1, 'video': 1, 'edge': 1, 'would': 1, 'actors': 1, 'although': 1, 'wes': 1, 'bentley': 1, 'seemed': 1, 'be': 1, 'playing': 1, 'exact': 1, 'character': 1, 'he': 1, 'american': 1, 'beauty': 1, 'only': 1, 'new': 1, 'neighborhood': 1, 'my': 1, 'kudos': 1, 'holds': 1, 'own': 1, 'entire': 1, 'feeling': 1, \"character's\": 1, 'unraveling': 1, 'overall': 1, 'stick': 1, 'entertain': 1, 'confusing': 1, 'rarely': 1, 'excites': 1, 'feels': 1, 'redundant': 1, 'runtime': 1, 'despite': 1, 'ending': 1, 'explanation': 1, 'craziness': 1, 'came': 1, 'oh': 1, 'horror': 1, 'slasher': 1, 'flick': 1, 'packaged': 1, 'someone': 1, 'assuming': 1, 'genre': 1, 'hot': 1, 'kids': 1, 'also': 1, 'wrapped': 1, 'production': 1, 'years': 1, 'ago': 1, 'sitting': 1, 'shelves': 1, 'ever': 1, 'whatever': 1, 'skip': 1, \"where's\": 1, 'joblo': 1, 'nightmare': 1, 'elm': 1, 'street': 1, '3': 1, 'blair': 1, 'witch': 1, '2': 1, 'salvation': 1, '4/10': 1, 'stir': 1, 'echoes': 1, '8/10': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: a list of tokens in a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary that records the number of occurrence for each token in a document\n",
    "    \"\"\"\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuCta1E-ReYC"
   },
   "source": [
    "We can put all above preprocessing steps into a function `clean_doc_unigrams()`. This function will preprocess the data and extract unigram tokens from the document. We then test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6UgUxOZReYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 46, ',': 43, '.': 23, 'and': 20, '(': 18, ')': 18, 'in': 18, 'a': 15, 'to': 15, 'of': 14, 'from': 8, 'but': 7, 'is': 7, 'it': 6, '\"': 6, 'comic': 5, \"don't\": 5, 'film': 5, 'about': 4, 'like': 4, 'who': 4, 'with': 4, 'say': 4, 'you': 4, 'this': 4, \"it's\": 4, 'i': 4, 'had': 3, 'or': 3, 'been': 3, 'book': 3, 'for': 3, 'moore': 3, 'campbell': 3, 'ripper': 3, 'be': 3, 'that': 3, \"hell's\": 3, 'me': 3, ':': 3, 'than': 3, '?': 3, 'he': 3, 'an': 3, 'so': 3, 'even': 3, 'at': 3, 'all': 3, 'have': 2, 'world': 2, 'never': 2, 'really': 2, 'hell': 2, 'whole': 2, 'called': 2, 'jack': 2, 'starting': 2, 'little': 2, 'if': 2, 'more': 2, 'other': 2, 'because': 2, 'get': 2, 'hughes': 2, 'direct': 2, 'seems': 2, 'as': 2, 'ghetto': 2, 'whitechapel': 2, 'end': 2, 'place': 2, 'has': 2, 'when': 2, 'first': 2, 'turns': 2, 'peter': 2, 'not': 2, 'enough': 2, 'abberline': 2, 'depp': 2, 'graham': 2, 'into': 2, 'here': 2, 'both': 2, 'identity': 2, 'good': 2, '-': 2, 'make': 2, 'see': 2, '2': 2, \"wasn't\": 2, 'strong': 2, 'accent': 2, 'her': 2, 'films': 1, 'adapted': 1, 'books': 1, 'plenty': 1, 'success': 1, 'whether': 1, \"they're\": 1, 'superheroes': 1, 'batman': 1, 'superman': 1, 'spawn': 1, 'geared': 1, 'toward': 1, 'kids': 1, 'casper': 1, 'arthouse': 1, 'crowd': 1, 'ghost': 1, \"there's\": 1, 'before': 1, 'starters': 1, 'was': 1, 'created': 1, 'by': 1, 'alan': 1, 'eddie': 1, 'brought': 1, 'medium': 1, 'new': 1, 'level': 1, 'mid': 1, \"'80s\": 1, '12-part': 1, 'series': 1, 'watchmen': 1, 'thoroughly': 1, 'researched': 1, 'subject': 1, 'would': 1, 'saying': 1, 'michael': 1, 'jackson': 1, 'look': 1, 'odd': 1, 'graphic': 1, 'novel': 1, 'will': 1, 'over': 1, '500': 1, 'pages': 1, 'long': 1, 'includes': 1, 'nearly': 1, '30': 1, 'consist': 1, 'nothing': 1, 'footnotes': 1, 'words': 1, 'dismiss': 1, 'its': 1, 'source': 1, 'can': 1, 'past': 1, 'thing': 1, 'might': 1, 'find': 1, 'another': 1, 'stumbling': 1, 'block': 1, 'directors': 1, 'albert': 1, 'allen': 1, 'getting': 1, 'brothers': 1, 'almost': 1, 'ludicrous': 1, 'casting': 1, 'carrot': 1, 'top': 1, 'well': 1, 'anything': 1, 'riddle': 1, 'better': 1, \"that's\": 1, 'set': 1, 'features': 1, 'violent': 1, 'street': 1, 'crime': 1, 'mad': 1, 'geniuses': 1, 'behind': 1, 'menace': 1, 'ii': 1, 'society': 1, 'question': 1, 'course': 1, '1888': 1, \"london's\": 1, 'east': 1, 'filthy': 1, 'sooty': 1, 'where': 1, 'whores': 1, 'unfortunates': 1, 'are': 1, 'nervous': 1, 'mysterious': 1, 'psychopath': 1, 'carving': 1, 'through': 1, 'their': 1, 'profession': 1, 'surgical': 1, 'precision': 1, 'stiff': 1, 'up': 1, 'copper': 1, 'godley': 1, 'robbie': 1, 'coltrane': 1, 'calls': 1, 'inspector': 1, 'frederick': 1, 'johnny': 1, 'blow': 1, 'crack': 1, 'case': 1, 'widower': 1, 'prophetic': 1, 'dreams': 1, 'unsuccessfully': 1, 'tries': 1, 'quell': 1, 'copious': 1, 'amounts': 1, 'absinthe': 1, 'opium': 1, 'upon': 1, 'arriving': 1, 'befriends': 1, 'unfortunate': 1, 'named': 1, 'mary': 1, 'kelly': 1, 'heather': 1, \"isn't\": 1, 'proceeds': 1, 'investigate': 1, 'horribly': 1, 'gruesome': 1, 'crimes': 1, 'police': 1, 'surgeon': 1, \"can't\": 1, 'stomach': 1, 'think': 1, 'anyone': 1, 'needs': 1, 'briefed': 1, 'on': 1, \"won't\": 1, 'go': 1, 'particulars': 1, 'unique': 1, 'interesting': 1, 'theory': 1, 'killer': 1, 'reasons': 1, 'chooses': 1, 'slay': 1, 'they': 1, 'bother': 1, 'cloaking': 1, 'screenwriters': 1, 'terry': 1, 'hayes': 1, 'vertical': 1, 'limit': 1, 'rafael': 1, 'yglesias': 1, 'les': 1, 'mis': 1, 'rables': 1, 'do': 1, 'job': 1, 'keeping': 1, 'him': 1, 'hidden': 1, 'viewers': 1, 'until': 1, 'very': 1, 'funny': 1, 'watch': 1, 'locals': 1, 'blindly': 1, 'point': 1, 'finger': 1, 'blame': 1, 'jews': 1, 'indians': 1, 'after': 1, 'englishman': 1, 'could': 1, 'capable': 1, 'committing': 1, 'such': 1, 'ghastly': 1, 'acts': 1, 'ending': 1, 'whistling': 1, 'stonecutters': 1, 'song': 1, 'simpsons': 1, 'days': 1, 'holds': 1, 'back': 1, 'electric': 1, 'car/who': 1, 'made': 1, 'steve': 1, 'guttenberg': 1, 'star': 1, 'worry': 1, \"it'll\": 1, 'sense': 1, 'now': 1, 'onto': 1, 'appearance': 1, 'certainly': 1, 'dark': 1, 'bleak': 1, 'surprising': 1, 'how': 1, 'much': 1, 'looks': 1, 'tim': 1, 'burton': 1, 'planet': 1, 'apes': 1, 'did': 1, 'times': 1, 'sleepy': 1, 'hollow': 1, 'print': 1, 'saw': 1, 'completely': 1, 'finished': 1, 'color': 1, 'music': 1, 'finalized': 1, 'no': 1, 'comments': 1, 'marilyn': 1, 'manson': 1, 'cinematographer': 1, 'deming': 1, 'word': 1, 'ably': 1, 'captures': 1, 'dreariness': 1, 'victorian-era': 1, 'london': 1, 'helped': 1, 'flashy': 1, 'killing': 1, 'scenes': 1, 'remind': 1, 'crazy': 1, 'flashbacks': 1, 'twin': 1, 'peaks': 1, 'though': 1, 'violence': 1, 'pales': 1, 'comparison': 1, 'black-and-white': 1, 'oscar': 1, 'winner': 1, 'martin': 1, \"childs'\": 1, 'shakespeare': 1, 'love': 1, 'production': 1, 'design': 1, 'original': 1, 'prague': 1, 'surroundings': 1, 'one': 1, 'creepy': 1, 'acting': 1, 'solid': 1, 'dreamy': 1, 'turning': 1, 'typically': 1, 'performance': 1, 'deftly': 1, 'handling': 1, 'british': 1, 'ians': 1, 'holm': 1, 'joe': 1, \"gould's\": 1, 'secret': 1, 'richardson': 1, '102': 1, 'dalmatians': 1, 'log': 1, 'great': 1, 'supporting': 1, 'roles': 1, 'big': 1, 'surprise': 1, 'cringed': 1, 'time': 1, 'she': 1, 'opened': 1, 'mouth': 1, 'imagining': 1, 'attempt': 1, 'irish': 1, 'actually': 1, 'half': 1, 'bad': 1, 'however': 1, '00': 1, 'r': 1, 'violence/gore': 1, 'sexuality': 1, 'language': 1, 'drug': 1, 'content': 1})\n"
     ]
    }
   ],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: text from a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary of tokens\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9__t3RWKReYI"
   },
   "source": [
    "Finally, we can integrate the above preprocessing `clean_doc_unigrams()` into the data processing pipeline for all the files in a directory. We do so by the function `process_docs_unigram()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA4RU1KRReYK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n"
     ]
    }
   ],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of unigram token counter dictionary where each token dictionary records the frequency of each token that occur in a document. \n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    tokens_all=[]\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_unigrams(text)\n",
    "        tokens_all.append(tokens_dict)\n",
    "    return tokens_all\n",
    "\n",
    "\n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn text to feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPWBwd2jReYS"
   },
   "source": [
    "Now that we have the count of each present unigrams for each document, we can convert these unigram counts into feature vectors. But before that, we first need to collect all the features to establish the dimensions of the feature vector. Here, the features are unigram words in the vocabulary. To do this, we define a function `collect_vocab()` to collect all the unique words in the vocabulary from `unigrams_all`, the list of token dictionary from all the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rw30VUGkReYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50920\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def collect_vocab(tokens_all):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document. \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a set of unique tokens in the vocabulary of tokens_all\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab=set() #Here, we create a `set()` to store all the unique words.\n",
    "    for token_lst in tokens_all: # iterate through token dictionary for each document\n",
    "        for token in token_lst:\n",
    "            # add a word in the vocab set. If a word already exists in vocab, it will not be added twice.\n",
    "            vocab.add(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# Let's check how many words we have in the vocab\n",
    "print (len(unigram_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kXVnFnpReYp"
   },
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor dimension index from `unigram_vocab`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IX4zO6tGReYq"
   },
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(vocab):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab: a set of unique features in the vocabulary\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    vocab2index: a mapping dictionary with feature as key and dimension index as value\n",
    "    index2vocab: a mapping dicitoanry with dimension index as key and feature as value\n",
    "    \"\"\"\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(vocab): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRuZAgH3ReYv"
   },
   "source": [
    "Now, we can use `unigram2index` mapping dictionary to turn each document's token dictionary representation into a unigram feature vector. Remember, we can either represent in each dimension cell the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "Let's design two functions `create_feature_presence()` and `create_feature_frequency()`  to turn the token dictionary for each document into these two types of features. \n",
    "\n",
    "To create the `create_feature_presence()`, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVKBHpEqReYw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_feature_presence(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1gqUZ4XReZa"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWuzZ5KpReZb"
   },
   "source": [
    "Can you try implementing the function`create_feature_frequency()` to extract unigram frequency? (You can modify on the basis of `create_feature_presence()`)\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yK92gQ75ReZc"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=token_dict[w]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3W3TtcJReZh"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>simple change the line vector[index]=1 from create_feature_presence() to vector[index]=token_dict_current[w]</p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDhe6PPjReZi"
   },
   "source": [
    "Now let's join the dots to create the function `create_feature_presence_all()` to loop over the unigrams from all the documents in `unigrams_all` and convert the token dictionary in each document to feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSB84BTTReZj"
   },
   "outputs": [],
   "source": [
    "def create_feature_presence_all(tokens_all,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a presence feature vector for the document\n",
    "    \"\"\"\n",
    "   \n",
    "    # since we know the number of documents, and the vocabulary size, we can initialize our result matrix as an empty matrix (2-D array) with the shape of (number of document, vocabulary size)\n",
    "    \n",
    "    presence_features=np.empty((len(tokens_all),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    for doc_i,token_dict in enumerate(tokens_all):\n",
    "        # convert bag of word dictionary in each document into unigram features\n",
    "        presence_feature=create_feature_presence(token_dict,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        presence_features[doc_i]=presence_feature\n",
    "    return presence_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z4WeZ1bReZm"
   },
   "source": [
    "Now we are ready to extract features from unigrams collected from all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC6L5sQMReZm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUwcH3xkReZv"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_hJeToEReZw"
   },
   "source": [
    "Can you follow the code above to extract all the documents' unigram frequency features (using the `create_feature_frequency()` functions you defined in quiz 3)? You can name your function as `create_feature_frequency_all()`. Please store the features into `features_unigram_frequency`. \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g6uwOwSReZx"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency(tokens_all,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a frequency feature vector for the document\n",
    "    \"\"\"\n",
    "   \n",
    "    # since we know the number of documents, and the vocabulary size, we can initialize our result matrix as an empty matrix (2-D array) with the shape of (number of document, vocabulary size)\n",
    "    \n",
    "    frequency_features=np.empty((len(tokens_all),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    for doc_i,token_dict in enumerate(tokens_all):\n",
    "        # convert bag of word dictionary in each document into unigram features\n",
    "        frequency_feature=create_feature_frequency(token_dict,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        frequency_features[doc_i]=unigram_frequency_feature\n",
    "    return frequency_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kvb5s1w-ReZ2"
   },
   "source": [
    "### ❓ Quiz  \n",
    "\n",
    "Let's check the first review's unigram presence feature, can you use the mapping in `index2unigram` to reveal what unigrams are present in this review? \n",
    "Please answer: Which words in the following list are present?\n",
    "A. gayness\n",
    "B. fabulous\n",
    "C. snappiness\n",
    "D. happiness\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khPgk5R1ReZ3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[index2unigram[i] for i,item in enumerate(features_unigram_presence[0]) if item==1]\n",
    "'happiness' in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU52Lmt7ReZ6"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p> Code:</p>\n",
    "    <p>wordlist=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]</p>\n",
    "    <p> A,C are present in the review </p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEDH7-WERea3"
   },
   "source": [
    "At the same time, we should also find a way to represent the labels (positive or negative) in numeric ways for the model to train on. Let's create numpy arrays of labels of 1s and 0s. Let's say 1 corresponds to positive reviews and 0 corresponds to negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we concatenate positive and negative data when producing `unigrams_all` and `features_unigram_presence`, we will follow the same data order to create the numpy array of labels that tell us the sentiment for each feature vector in `features_unigram_presence`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K12juH1zRea4"
   },
   "outputs": [],
   "source": [
    " \n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGNbpwvAReZ7"
   },
   "source": [
    "## Save the features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntPhUTCrReZ7"
   },
   "source": [
    "We can also save the prepared features for the reviews ready for modeling.\n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas.\n",
    "\n",
    "To store a numpy array, we can use the `numpy`'s `save()` function. `save()` takes two arguments: the first is an the filename to be written, and the second argument is the numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE3m4vnpReZ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('features_unigram_presence.npy',features_unigram_presence)\n",
    "np.save('labels_unigram_presence.npy',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gL_Qwr_1ReaC"
   },
   "source": [
    "We can load the postiive data from the files by:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBEc1d1DReaC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('features_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgQWG19jReak"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_SGP7aReak"
   },
   "source": [
    "## Prepare train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuHpK9tfReak"
   },
   "source": [
    "Now it's time to prepare this dataset for model evlauation. We want to train a model that is generalisable to unseen data. Therefore, we can split the dataset into train and test where the model is trained on the train set and tested on the unseen test set. Usually the train-test split is 70% and 30%. \n",
    "\n",
    "We will follow the paper to adopt a 3-fold cross validation. We will use `scikit-learn`'s `KFold` to do that. In addition, to ensure that we have an equal number of positive and negative examples, we make the split in both positive and negative datasets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpmPH4YkReal"
   },
   "source": [
    "Let's first load the unigram presence features and their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('features_unigram_presence.npy')\n",
    "labels=np.load('labels_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce the indices for the positive and the negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "neg_id=[i for i,label in enumerate(labels) if label==0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `KFold` from `scikit-learn` and generate cross validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1...\n",
      "[   0    1    4 ... 1996 1997 1999]\n",
      "Running fold 2...\n",
      "[   1    2    3 ... 1995 1996 1998]\n",
      "Running fold 3...\n",
      "[   0    2    3 ... 1997 1998 1999]\n"
     ]
    }
   ],
   "source": [
    "# import KFold and random\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "\n",
    "# set the number of folds\n",
    "num_folds=3\n",
    "\n",
    "# set random seed for cross validation\n",
    "cross_val_seed = 42\n",
    "\n",
    "# create the KFold object and create the splits for positive data\n",
    "kf_posi = KFold(n_splits=num_folds)\n",
    "kf_posi.get_n_splits(posi_id)\n",
    "kf_posi = KFold(n_splits=num_folds, random_state=cross_val_seed, shuffle=True)\n",
    "posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "# create the KFold object and create the splits for negative data\n",
    "kf_neg = KFold(n_splits=num_folds)\n",
    "kf_neg.get_n_splits(posi_id)\n",
    "kf_neg = KFold(n_splits=num_folds, random_state=cross_val_seed, shuffle=True)\n",
    "neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "# loop over the folds to create train and test data/label. \n",
    "\n",
    "for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "    print(f'Running fold {fold_idx+1}...')\n",
    "    \n",
    "    # Get the current fold for positive data\n",
    "    fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "    fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "    \n",
    "    # Get the current fold for negative data\n",
    "    train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "    fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "    fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "    \n",
    "    # ensure that we have balanced classes in both train and test\n",
    "    assert len(fold_train_posi)==len(fold_train_neg)\n",
    "    assert len(fold_test_posi)==len(fold_test_neg)\n",
    "    # combine all train and test for the current fold\n",
    "    fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "    fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "    print (fold_train)\n",
    "    \n",
    "    # shuffle the train and test indexes in the current fold\n",
    "    random.shuffle(fold_train)\n",
    "    random.shuffle(fold_test)\n",
    "    \n",
    "    # use the indexes in fold_train and fold_test to slice data and labels\n",
    "    fold_train_data=data[fold_train]\n",
    "    fold_test_data=data[fold_test]\n",
    "    fold_train_label=labels[fold_train]\n",
    "    fold_test_label=labels[fold_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can wrap the above into a function `create_kfold_validadtion()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_validation(data,labels,num_folds):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of features\n",
    "    labels: a numpy array of labels (1=positive, 0=negative)\n",
    "    num_folds: the number of folds in cross validation\n",
    "\n",
    "    Yields (works as a generator)\n",
    "    ------\n",
    "    fold_train_data: current fold of training data as numpy arrays\n",
    "    fold_test_data: current fold of test data as numpy arrays\n",
    "    fold_train_label: current fold of training labels as numpy arrays\n",
    "    fold_test_label: current fold of test labels as numpy arrays\n",
    "    \"\"\"\n",
    "    #produce the indices for the positive and the negative data\n",
    "    posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "    neg_id=[i for i,label in enumerate(labels) if label==0]\n",
    "\n",
    "    # set random seed for cross validation\n",
    "    cross_val_seed = 42\n",
    "\n",
    "    # create the KFold object and create the splits for positive data\n",
    "    kf_posi = KFold(n_splits=num_folds)\n",
    "    kf_posi.get_n_splits(posi_id)\n",
    "    kf_posi = KFold(n_splits=num_folds, random_state=cross_val_seed, shuffle=True)\n",
    "    posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "    # create the KFold object and create the splits for negative data\n",
    "    kf_neg = KFold(n_splits=num_folds)\n",
    "    kf_neg.get_n_splits(posi_id)\n",
    "    kf_neg = KFold(n_splits=num_folds, random_state=cross_val_seed, shuffle=True)\n",
    "    neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "    # loop over the folds to create train and test data/label. \n",
    "\n",
    "    for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "        print(f'Running fold {fold_idx+1}...')\n",
    "\n",
    "        # Get the current fold for positive data\n",
    "        fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "        fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "\n",
    "        # Get the current fold for negative data\n",
    "        train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "        fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "        fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "\n",
    "        # ensure that we have balanced classes in both train and test\n",
    "        assert len(fold_train_posi)==len(fold_train_neg)\n",
    "        assert len(fold_test_posi)==len(fold_test_neg)\n",
    "        # combine all train and test for the current fold\n",
    "        print ('combining positive and negative for train and test')\n",
    "        fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "        fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "        \n",
    "        \n",
    "        # use the indexes in fold_train and fold_test to slice data and labels\n",
    "        print ('slice training and test data')\n",
    "        fold_train_data=data[fold_train]\n",
    "        fold_test_data=data[fold_test]\n",
    "        fold_train_label=labels[fold_train]\n",
    "        fold_test_label=labels[fold_test]\n",
    "        print ('yield')\n",
    "        \n",
    "        assert len(fold_train_data)==len(fold_train_label)\n",
    "        assert len(fold_test_data)==len(fold_test_label)\n",
    "        yield fold_train_data,fold_test_data,fold_train_label,fold_test_label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `create_kfold_validation()`, we are initializing a generator. We can loop over the generator to produce `fold_train_data,fold_test_data,fold_train_label,fold_test_label`  for every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1...\n",
      "combining positive and negative for train and test\n",
      "slice training and test data\n",
      "yield\n",
      "length of train data 1332\n",
      "length of test data 668\n",
      "Running fold 2...\n",
      "combining positive and negative for train and test\n",
      "slice training and test data\n",
      "yield\n",
      "length of train data 1334\n",
      "length of test data 666\n",
      "Running fold 3...\n",
      "combining positive and negative for train and test\n",
      "slice training and test data\n",
      "yield\n",
      "length of train data 1334\n",
      "length of test data 666\n"
     ]
    }
   ],
   "source": [
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    print ('length of train data',len(fold_train_data))\n",
    "    print ('length of test data',len(fold_test_data))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2ScVYh9RebR"
   },
   "source": [
    "## Naive Bayes Model and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnKOr56PRebS"
   },
   "source": [
    "Let's try fitting the features and labels from each validation fold into a naive bayes model using the `MultinomialNB` package in `sklearn`. `MultinomialNB` is a Naive Bayes classifier for multinomial models. It implements additive smoothing by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04CNQg8eRebT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from sklearn)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Running fold 1...\n",
      "[   0    1    4 ... 1996 1997 1999]\n",
      "Running fold 2...\n",
      "[   1    2    3 ... 1995 1996 1998]\n",
      "Running fold 3...\n",
      "[   0    2    3 ... 1997 1998 1999]\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(fold_train_data,fold_train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(fold_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DHgKdlJRebW"
   },
   "source": [
    "In each fold, `predicted` is an array of predictions in 1s and 0s. Let's compare it with the gold labels and calculate accuracy following:\n",
    "\n",
    "accruracy= number of correct examples/number of total examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfCvRGRhRebW"
   },
   "source": [
    "Let's create a function that calls the naive bayes model and performs evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4SstOdiRebc"
   },
   "outputs": [],
   "source": [
    "def train_evaluate(train_data,train_label,test_data,test_label):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of train data (number of examples * number of features)\n",
    "    train_label: numpy array of train labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "    test_data: numpy array of test data (number of examples * number of features)\n",
    "    test_label: numpy array of test labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    accuracy score on the test data\n",
    "    \"\"\"\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(train_data,train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(test_data)\n",
    "    #evaluation:\n",
    "    correct=0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]==test_label[i]: #if the predicted result is the same with the gold label\n",
    "            correct+=1\n",
    "    acc=correct/len(predicted)\n",
    "\n",
    "\n",
    "    print ('accuracy',acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CFSUKl5Rebf"
   },
   "source": [
    "Now integrating `train_evaluate()` into cross validation, we will get accuracy per each fold. We then take the average and standard deviation of the accuracy scores across folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxNffzRuRebg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1...\n",
      "[   0    1    4 ... 1996 1997 1999]\n",
      "accuracy 0.8233532934131736\n",
      "Running fold 2...\n",
      "[   1    2    3 ... 1995 1996 1998]\n",
      "accuracy 0.8108108108108109\n",
      "Running fold 3...\n",
      "[   0    2    3 ... 1997 1998 1999]\n",
      "accuracy 0.8258258258258259\n",
      "Mean accuracy      : 0.82000\n",
      "Standard deviation : 0.00657\n"
     ]
    }
   ],
   "source": [
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: The whole pipeline to train on unigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n",
      "Running fold 1...\n",
      "[   0    1    4 ... 1996 1997 1999]\n",
      "accuracy 0.8233532934131736\n",
      "Running fold 2...\n",
      "[   1    2    3 ... 1995 1996 1998]\n",
      "accuracy 0.8108108108108109\n",
      "Running fold 3...\n",
      "[   0    2    3 ... 1997 1998 1999]\n",
      "accuracy 0.8258258258258259\n",
      "Mean accuracy      : 0.82000\n",
      "Standard deviation : 0.00657\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract unigrams from all documents\n",
    "from os import listdir \n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n",
    "\n",
    "# 2. Turn text to feature vectors\n",
    "# collect all unique unigram as all features\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# map unigram features to dimension indexes\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)\n",
    "# Convert unigrams to feature vectors for all documents\n",
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n",
    "\n",
    "# 3. Encode labels\n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)\n",
    "\n",
    "# 4. Evaluation with 3-fold cross validation\n",
    "data=features_unigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhzv2aDmReaG"
   },
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fOlUBWYReaH"
   },
   "source": [
    "Based on `clean_doc_unigrams()` that turns a document into a unigram dictionary, we can create a function `clean_doc_bigrams()` to extract bigram dictionary. (You can refresh yourself of how to create a bigram counter dictionary in module 1.4. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hehFhHqgReaI"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: string of the document text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a counter dictionary with bigram as key and count as value\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # extract bigrams\n",
    "    bigram_dict=Counter() #initialize a bigram dictionary to be updated\n",
    "    tokens=['<start>']+tokens+['<end>'] # add <start> and <end> token\n",
    "    for i in range(len(tokens)): #loop over all the indices of the token list\n",
    "        if i<len(tokens)-1: #if it's not the end of the token list\n",
    "            bigram_current=(tokens[i],tokens[i+1])\n",
    "            bigram_dict[bigram_current]+=1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzesiKPtReaK"
   },
   "source": [
    "Now we can replace the `clean_doc_unigrams()` line with `clean_doc_bigrams()` in `process_docs_unigrams()`, we will rename the function as `process_docs_bigrams()` that counts all the bigrams in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpo6ay2bReaL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n"
     ]
    }
   ],
   "source": [
    "def process_docs_bigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "    overall_bigrams_counter: \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a list of bigram counters each representing a document\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    tokens_all=[]\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_bigrams(text)\n",
    "        tokens_all.append(tokens_dict_current)\n",
    "    return tokens_all\n",
    "bigrams_neg=process_docs_bigrams('./data/neg')\n",
    "bigrams_posi=process_docs_bigrams('./data/pos')\n",
    "bigrams_all=bigrams_posi+bigrams_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1KBMkOFReaN"
   },
   "source": [
    "We can follow the same procedure to turn bigram counters per each document into bigram feature presence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vocab=collect_vocab(bigrams_all)\n",
    "bigram2index,index2bigram=create_vocab_feature_mappings(bigram_vocab)\n",
    "features_bigram_presence=create_feature_presence_all(bigrams_all,bigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the same for `features_unigram_presence`. We can then directly train Naive Bayes model with these bigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fold 1...\n",
      "[   0    1    4 ... 1996 1997 1999]\n",
      "accuracy 0.844311377245509\n",
      "Running fold 2...\n",
      "[   1    2    3 ... 1995 1996 1998]\n",
      "accuracy 0.8423423423423423\n",
      "Running fold 3...\n",
      "[   0    2    3 ... 1997 1998 1999]\n",
      "accuracy 0.8708708708708709\n",
      "Mean accuracy      : 0.85251\n",
      "Standard deviation : 0.01301\n"
     ]
    }
   ],
   "source": [
    "data=features_bigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W8h1ByjReaY"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8ADZKR1ReaZ"
   },
   "source": [
    "Now we have features in the form of bigrams. Now what does each dimension represent for a vector now? How many dimensions do we have for each vector? You can inspect `bigram_presence_positive` to answer these questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHhMYkawReaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VgTk8uTReac"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Each dimension corresponds to a bigram. There are 463119 bigrams and therfore the dimension size of each vector is 463119. </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8bO5CiDRead"
   },
   "source": [
    "other features afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qH_mYfLrReae"
   },
   "source": [
    "#### Unigrams + POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glZAvmyZReag"
   },
   "source": [
    "#### Adjective Unigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkkftelGReag"
   },
   "source": [
    "#### Unigrams above certain frequency threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXq0GHvvReah"
   },
   "source": [
    "#### Unigrams + Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUQ_-39-Reah"
   },
   "source": [
    "#### Unigrams + Bigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4AsqDygReaj"
   },
   "source": [
    "Negation (extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCaLlXqgRebo"
   },
   "source": [
    "### ❓Final Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbpA7tuVRebo"
   },
   "source": [
    "1. So far we have trained a model and evaluate on the bigram and unigram presence features, can you try to build another model on bigram frequency features with three fold cross validation? (Hint, you can simply change the arguments to the `produce_data_label_splits()`)\n",
    "You can write your code below and report the accuracy\n",
    "\n",
    "2. Adding Negation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-mHntH4Rebp"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>train_data,test_data,train_label,test_label=produce_data_label_splits(bigram_presence_positive,bigram_presence_negative)</p>\n",
    " <p>acc=evaluate(train_data,train_label,test_data,test_label)</p>\n",
    "    <p>One run of the model gives 0.84. Remember that your result can be a bit different due to the random split.  </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_2.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
