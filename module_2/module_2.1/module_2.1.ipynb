{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to module 2.1. In this module, we will start building and testing a sentiment classifier with Naive Bayes!\n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "## Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>Sentiment Anlaysis</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file “cv000_29416.txt” as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this into a function called load_doc() that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read in these positive and negative files and store them as two list of texts. To navigate the files, we can use Python's `os` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 1:\n",
    "\n",
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at cleaning and extracting features from the movie review data. We will start from splitting the text to extract unigrams. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s load one document and look at the raw tokens split by white space. We will use the load_doc() function developed in the previous section. We can use the split() function to split the loaded document into tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
    "\n",
    "Removing tokens that are just punctuation (e.g. ‘-‘).\n",
    "Removing tokens that contain numbers (e.g. ’10/10′).\n",
    "Remove tokens that don’t have much meaning (e.g. ‘and’)\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "We can remove tokens that are just punctuation or contain numbers by using an `isalpha()` function to check on each token.\n",
    "We can remove English stop words using the list loaded using `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the above preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install nltk using pip in the jupyter notebook\n",
    "!python -m pip install nltk \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "# remove tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are mainly interested in the frequency and presence of each unigram, we can store the unigram features using the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary of unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this into a function called `clean_doc_unigrams()` and test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add the above preprocessing steps into a function `process_docs_unigram()` to process all the files in a directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_unigrams(text)\n",
    "process_docs_unigrams('./data/neg')\n",
    "process_docs_unigrams('./data/pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the features as input to the model, we need to turn the now dictionary features into a vector of numbers. For unigram features, the feature vector will have a dimension size of the vocabulary. Each dimension stores the frequency or presence of the corresponding word. \n",
    "\n",
    "For example, suppose we have five words (apple,banana,red,dog,is) in the vocabulary which are represented as five dimensions in the feature vecotors. We also have a document (document 1) containing the following words: \n",
    "\n",
    "document 1: \"apple is red\"\n",
    "\n",
    "We add '1' for the words as in: \n",
    "\n",
    "|document no.|apple|banana|red|dog|is\n",
    "|------|------|------|------|------|------|\n",
    "|document 1 |1|0|1|0|1|\n",
    "\n",
    "We thus can represent the document as a feature vector [1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step now is to create a mapping between dimension index of the vector and the word in the vocabulary. Let's define an overall `Counter` dictionary that updates the words and their counts while we process all the documents. To do this, we can define a function `update_counter()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_counter(overall_vocab_counter,current_vocab_counter):\n",
    "    for w in current_vocab_counter:\n",
    "        overall_vocab_counter[w]+=current_vocab_counter[w]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an `overall_vocab_counter` and then integrate the `update_counter()` function into `process_docs_unigrams()` to update `overall_vocab_counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_vocab_counter=Counter()\n",
    "def process_docs_unigrams(directory):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_unigrams(text)\n",
    "        update_counter(overall_vocab_counter,tokens_dict)\n",
    "process_docs_unigrams('./data/neg')\n",
    "process_docs_unigrams('./data/pos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 1:\n",
    "\n",
    "Let's check the compiled `overall_vocab_counter`, how many words are there in total after preprocessing? And what are the top 100 most frequent words?\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(overall_vocab_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>There are 37607 words in total</p>\n",
    "    <p>The 5 most frequent words are: \n",
    "        ('film', 8849),\n",
    " ('one', 5514),\n",
    " ('movie', 5429),\n",
    " ('like', 3543),\n",
    " ('even', 2554),</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor dimension index from this `overall_vocab_counter`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(overall_vocab_counter):\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(overall_vocab_counter.keys()): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "vocab2index,index2vocab=create_vocab_feature_mappings(overall_vocab_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's turn each document into a unigram feature vector. We can either represent in each dimension cell the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "\n",
    "To represent these feature vectors, we use the numpy arrays. These can be computed from python's `numpy` module. These numpy arrays require less memory and are much faster to access. \n",
    "\n",
    "Let's design two functions `create_feature_frequency()` and `create_feature_presence()` to turn the token dictionary for each document to form these two types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short introduction of numpy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def create_feature_frequency(tokens_dict,vocab2index):\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the prepared features for the reviews ready for modeling.\n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's change on the basis of `clean_doc_unigrams()` to extract bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjective Unigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams above certain frequency threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + Bigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
