{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xv4NPtgAReWr"
   },
   "source": [
    "Welcome to module 2.1. In this module, we will use a popular Python module `scikit-learn` to build a sentiment classifier with Naive Bayes! We will introduce the concept of feature as numerical representation of the input data. We will experiment with different types of features to investiage their impact on training. \n",
    "At the end of the module, you should be able to:\n",
    "\n",
    "* understand features and how to extract them from input data\n",
    "* experiment with an off-the-shelf Naive Bayes model\n",
    "* be familar with the pipeline for bulding a machine learning model incluidng data cleaning, feature extraction, and model evaluation.\n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "## ❓ Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ra8qD88ReWu"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii18vTN8ReWx"
   },
   "source": [
    "# Sentiment Analysis Task Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AplzSdOIReWz"
   },
   "source": [
    "Our task of focus today is a popular NLP classification task: sentiment analysis. What exactly is sentiment? Sentiment relates to the meaning of a word or sequence of words and is usually associated with an opinion or emotion. And analysis? Well, this is the process of looking at data and making inferences; in this case, using machine learning to learn and predict whether a movie review is positive or negative.\n",
    "\n",
    "We will replicate some of the experiments from the paper: [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011.pdf). We will extract a number of features including unigrams, bigrams etc., and train Naive Bayes models on these features. (Notice that we use the second release of the data which is preprocessed and cleaned rather than the original data in the paper. Therefore, the results are not directly comparable. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bav1A-D6ReW1"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2Mgv4bfReW3"
   },
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBsHahXeScH7"
   },
   "source": [
    "As usual, we download the files for the notebook from Github. If you're running this notebook locally or on Binder, you may skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kFuAQbpSdRf"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/cambridgeltl/python4cl/raw/module_2.1/module_2/module_2.1/data.zip\n",
    "!unzip -n -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFew9K1WReW5"
   },
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file “cv000_29416.txt” as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QngDdJeGReW_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mmv7wS0cReXH"
   },
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOSa4383ReXI"
   },
   "source": [
    "We can turn this into a function called load_doc() that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GZg4B9FReXL"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: the filename to extract text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    text in strings\n",
    "    \"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_13EAmfReXS"
   },
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jL_pY341ReXT"
   },
   "source": [
    "Let's first read in these positive and negative files and store them as two list of texts. To navigate the files, we can use Python's `os` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60kbNDGiReXV"
   },
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of of documents, where each document is string text\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6SlpyplReXd"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ4Ol0plReXe"
   },
   "source": [
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngOZpkTQReXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf82-7OFReXm"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU3WDGsMReXn"
   },
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LbJ8xKIReXo"
   },
   "source": [
    "So far we have obtained the text of each document. If we follow the previous module, we could then take bag of word represention for each document, and compute the likelihood $P(d \\mid c)$ as:\n",
    "\n",
    "$P(d \\mid c) = P(w_1, \\dots , w_n \\mid c) $\n",
    "\n",
    "This is however only one way to represent the document. We could for example select some of the words (eg. more informative words) or use bigrams instead of words, or combine unigrams and bigrams to represent the document. We could even add information such as document lenghth, date of creation into the representation. All these measurable properties that we used to represent the input data are referred to as $features$. Therefore, $w_1, w_2, \\dots, w_n$ can be written as $f_1,f_2,\\dots,f_n$ indicating individual features. The following is a more general formulation of the likelihood. \n",
    "\n",
    "$P(d \\mid c) = P(f_1, \\dots , f_n \\mid c) $\n",
    "\n",
    "Practically, we can group a document's feature values into a list: [f1,f2..fn]. This will be a numerical representation of the document that can be fed directly into a model. We call this list of numbers as a feature vector.  However, to input the data into a machine learning model, we often need to convert text into numerical representations of features. \n",
    "\n",
    "We will start by introducing the concept of features and feature vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlQMfCX6ReYQ"
   },
   "source": [
    "## What is a feature and what is a feature vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb62eNVSReYR"
   },
   "source": [
    "So far we have obtained the text of each document. If we follow the previous module, we could then take bag of word represention for each document, and compute the likelihood $P(d \\mid c)$ as:\n",
    "\n",
    "$$P(d \\mid c) = P(w_1, \\dots , w_n \\mid c) $$\n",
    "\n",
    "This is however only one way to represent the document. We could for example select some of the words (eg. more informative words) or use bigrams instead of words, or combine unigrams and bigrams to represent the document. We could even add information such as document lenghth, date of creation into the representation. All these measurable properties that we used to represent the input data are referred to as $features$. Therefore, $w_1, w_2, \\dots, w_n$ can be written as $f_1,f_2,\\dots,f_n$ indicating individual features. The following is a more general formulation of the likelihood. \n",
    "\n",
    "$$P(d \\mid c) = P(f_1, \\dots , f_n \\mid c) $$\n",
    "\n",
    "Practically, we can group a document's feature values into a list: [f1,f2..fn]. This will be a numerical representation of the document that can be fed directly into a model. We call this list of numbers as a feature vector.  However, to input the data into a machine learning model, we often need to convert text into numerical representations of features. \n",
    "\n",
    "\n",
    "The feature vector has a fixed length corresponding to the number of features. Each position of the list (or referred as dimension) represents a feature. For example, to represent unigrams in a document, we can create a vector with the same length of the vocabulary, and the value in each dimension (ie. each position of the list) stores the frequency or presence of a specific word. \n",
    "\n",
    "\n",
    "As an example, suppose we have seven words (apple,banana,red,dog,is,the,and) in the vocabulary which are represented as seven dimensions (features). We also have two documents as in below:\n",
    "\n",
    "document 1: \"the apple is red and the banana is yellow\"\n",
    "document 2: \"the red dog\"\n",
    "\n",
    "To produce a feature vector to represent unigram presence in a document, we can write '1' in a dimension to indicate the word the dimension corresponds to is present in the document, and we will write '0' to indicate the word is not present. Below is a table view of the vectors. \n",
    "\n",
    "\n",
    "|document no.||apple|banana|red|dog|is|the|and|\n",
    "|------||------|------|------|------|------|\n",
    "|document 1 ||1|0|1|0|1|1|1|\n",
    "|document 2 ||0|0|1|1|0|1|0|\n",
    "\n",
    "\n",
    "The feature vector for document 1 becomes `[1,0,1,0,1,1,1]`\n",
    "\n",
    "To produce a feature vector that represent frequency of each unigram in a document, we can count the number of occurence of each unigram word and write down the number in the corresponding dimension. \n",
    "\n",
    "|document no.||apple|banana|red|dog|is|the|and|\n",
    "|------||------|------|------|------|------|\n",
    "|document 1 ||1|0|1|0|1|2|1|\n",
    "|document 2 ||0|0|1|1|0|1|0|\n",
    "\n",
    "\n",
    "Now the feature vector for document 1 becomes `[1,0,1,0,1,2,1]`\n",
    "\n",
    "When the data consists of more than one document as in our example, we will have multiple feature vectors as representations of our data. We can stack the vectors into a list of vectors. This is often referred to as matrix. \n",
    "In our example, we have a 2*7 matrix where 2 is the number of documents, and 7 is the number of features\n",
    "\n",
    "Presence feature matrix for our toy data: `[[1,0,1,0,1,1,1],[0,0,1,1,0,1,0]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "Following the examples, please write below both the unigram presence and unigram frequenc feature vector for the document text 'the red dog and the red apple'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Unigram presence feature vector: [1,0,1,1,0,1,1]</p>\n",
    "    <p>Unigram frequency feature vector: [1,0,2,1,0,2,1]</p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "What is the matrix consisting of the unigram presence vectors of the following documents:\n",
    "\n",
    "document1: 'the apple is red and the banana is yellow'\n",
    "\n",
    "document2: 'the red dog and the red apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>The matrix is: [[1,0,1,0,1,1,1],[1,0,1,1,0,1,1]] </p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vectors are usually represented as `numpys` arrays in Python. Let's spend some time to understand what `numpy` is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd8XWczOReY1"
   },
   "source": [
    "## Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQhMs1Q1ReY1"
   },
   "source": [
    "A `numpy` array is just like a `list` but with smaller memory and faster access. \n",
    "\n",
    "Below, we introduce several ways to create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcbIondQReY2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a numpy array of zeros with dimension 2\n",
    "vector1=np.zeros(2)\n",
    "# create a numpy array from a list [1,2,3]\n",
    "vector2=np.array([1,2,3])\n",
    "# create an empty array of dimension 3 with arbitary data\n",
    "vector3=np.empty(3)\n",
    "print ('vector1',vector1)\n",
    "print ('vector2',vector2)\n",
    "print ('vector3',vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTzXgRTuReY6"
   },
   "source": [
    "We can also concatenate two numpy arrays of the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSNpS0TJReY7"
   },
   "outputs": [],
   "source": [
    "np.concatenate((vector2,vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq3Do1UxReZE"
   },
   "source": [
    "So far, we have created numpy arrays of one dimension. Let's try creating a 2-D array (also called a matrix). We can pass dimension size (also called axes) as (a,b) where a is the number of rows in the matrix, and b specifies the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzQII4NNReZF"
   },
   "outputs": [],
   "source": [
    "matrix1=np.zeros((3,3)) \n",
    "# this is a matrix of zeros that has 3 vectors, and within each vector there are 4 items. \n",
    "print ('matrix1',matrix1)\n",
    "# a matrix from nested list\n",
    "matrix2=np.array([[1,2,3],[2,3,4]])\n",
    "print ('matrix2',matrix2)\n",
    "# an empty matrix usually used as initialisation. It will print as an empty list\n",
    "matrix3=np.empty((0,4))\n",
    "print ('matrix3',matrix3)\n",
    "#To check the axes of an array, you can retrieve the shape attribute like this:\n",
    "print (matrix2.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJI7IUkMReZJ"
   },
   "source": [
    "Numpy arrays are mutable. Therefore, we can change values in the vector. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ky4YPO0ReZL"
   },
   "outputs": [],
   "source": [
    "vector2[0]=0 # change the first item in vector3 to 0\n",
    "print (vector2)\n",
    "matrix2[0][2]=0 # change the third item of the first vector to 0\n",
    "print (matrix2)\n",
    "matrix2[0]=vector2 # change the first vector in matrix2 to vector3\n",
    "print (matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjSFnyhwReZP"
   },
   "source": [
    "We can also slice a numpy array with an index array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r9u1f_TReZS"
   },
   "outputs": [],
   "source": [
    "#Let's select the values at index 1,2 of vector2\n",
    "vector2[[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0rSLnjtReXp"
   },
   "source": [
    "## Extracting unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qJxTridReXq"
   },
   "source": [
    "Now let's extract unigram features from our data. First, let’s load one document and look at the raw tokens split by white space. We will use the `load_doc()` function developed in the previous section. We can use the `split()` function to split the loaded document into unigram tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3eSu9nvReXr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeYaaXFDReX5"
   },
   "source": [
    "To keep track of the frequency and presence for each token in the document, we will use the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVTG7u5FReX9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: a list of tokens in a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary that records the number of occurrence for each token in a document\n",
    "    \"\"\"\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuCta1E-ReYC"
   },
   "source": [
    "We can put all above preprocessing steps into a function `clean_doc_unigrams()`. This function will preprocess the data and extract unigram tokens from the document. We then test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6UgUxOZReYD"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: text from a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary of tokens\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9__t3RWKReYI"
   },
   "source": [
    "Finally, we can integrate the above preprocessing `clean_doc_unigrams()` into the data processing pipeline for all the files in a directory. We do so by the function `process_docs_unigram()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA4RU1KRReYK"
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of unigram token counter dictionary where each token dictionary records the frequency of each token that occur in a document. \n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    tokens_all=[]\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_unigrams(text)\n",
    "        tokens_all.append(tokens_dict)\n",
    "    return tokens_all\n",
    "\n",
    "\n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn text to feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPWBwd2jReYS"
   },
   "source": [
    "Now that we have the count of each present unigrams for each document, we can convert these unigram counts into feature vectors. But before that, we first need to collect all the features to establish the dimensions of the feature vector. Here, the features are unigram words in the vocabulary. To do this, we define a function `collect_vocab()` to collect all the unique words in the vocabulary from `unigrams_all`, the list of token dictionary from all the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rw30VUGkReYZ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def collect_vocab(tokens_all):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document. \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a set of unique tokens in the vocabulary of tokens_all\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab=set() #Here, we create a `set()` to store all the unique words.\n",
    "    for token_lst in tokens_all: # iterate through token dictionary for each document\n",
    "        for token in token_lst:\n",
    "            # add a word in the vocab set. If a word already exists in vocab, it will not be added twice.\n",
    "            vocab.add(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# Let's check how many words we have in the vocab\n",
    "print (len(unigram_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kXVnFnpReYp"
   },
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor dimension index from `unigram_vocab`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IX4zO6tGReYq"
   },
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(vocab):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab: a set of unique features in the vocabulary\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    vocab2index: a mapping dictionary with feature as key and dimension index as value\n",
    "    index2vocab: a mapping dicitoanry with dimension index as key and feature as value\n",
    "    \"\"\"\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(vocab): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRuZAgH3ReYv"
   },
   "source": [
    "Now, we can use `unigram2index` mapping dictionary to turn each document's token dictionary representation into a unigram feature vector. Remember, we can either represent in each dimension cell the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "Let's design two functions `create_feature_presence()` and `create_feature_frequency()`  to turn the token dictionary for each document into these two types of features. \n",
    "\n",
    "To create the `create_feature_presence()`, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVKBHpEqReYw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_feature_presence(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1gqUZ4XReZa"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWuzZ5KpReZb"
   },
   "source": [
    "Can you try implementing the function`create_feature_frequency()` to extract unigram frequency? (You can modify on the basis of `create_feature_presence()`)\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yK92gQ75ReZc"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=token_dict[w]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3W3TtcJReZh"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>simple change the line vector[index]=1 from create_feature_presence() to vector[index]=token_dict_current[w]</p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDhe6PPjReZi"
   },
   "source": [
    "Now let's join the dots to create the function `create_feature_presence_all()` to loop over the unigrams from all the documents in `unigrams_all` and convert the token dictionary in each document to feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSB84BTTReZj"
   },
   "outputs": [],
   "source": [
    "def create_feature_presence_all(tokens_all,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a presence feature vector for the document\n",
    "    \"\"\"\n",
    "   \n",
    "    # since we know the number of documents, and the vocabulary size, we can initialize our result matrix as an empty matrix (2-D array) with the shape of (number of document, vocabulary size)\n",
    "    \n",
    "    presence_features=np.empty((len(tokens_all),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    for doc_i,token_dict in enumerate(tokens_all):\n",
    "        # convert bag of word dictionary in each document into unigram features\n",
    "        presence_feature=create_feature_presence(token_dict,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        presence_features[doc_i]=presence_feature\n",
    "    return presence_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z4WeZ1bReZm"
   },
   "source": [
    "Now we are ready to extract features from unigrams collected from all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC6L5sQMReZm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUwcH3xkReZv"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_hJeToEReZw"
   },
   "source": [
    "Can you follow the code above to extract all the documents' unigram frequency features (using the `create_feature_frequency()` functions you defined in quiz 3)? You can name your function as `create_feature_frequency_all()`. Please store the features into `features_unigram_frequency`. \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g6uwOwSReZx"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency_all(tokens_all,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a frequency feature vector for the document\n",
    "    \"\"\"\n",
    "   \n",
    "    # since we know the number of documents, and the vocabulary size, we can initialize our result matrix as an empty matrix (2-D array) with the shape of (number of document, vocabulary size)\n",
    "    \n",
    "    frequency_features=np.empty((len(tokens_all),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    for doc_i,token_dict in enumerate(tokens_all):\n",
    "        # convert bag of word dictionary in each document into unigram features\n",
    "        frequency_feature=create_feature_frequency(token_dict,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        frequency_features[doc_i]=frequency_feature\n",
    "    return frequency_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kvb5s1w-ReZ2"
   },
   "source": [
    "### ❓ Quiz  \n",
    "\n",
    "Let's check the first review's unigram presence feature, can you use the mapping in `index2unigram` to reveal what unigrams are present in this review? \n",
    "Please answer: Which words in the following list are present?\n",
    "A. gayness\n",
    "B. fabulous\n",
    "C. snappiness\n",
    "D. happiness\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khPgk5R1ReZ3"
   },
   "outputs": [],
   "source": [
    "a=[index2unigram[i] for i,item in enumerate(features_unigram_presence[0]) if item==1]\n",
    "'happiness' in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU52Lmt7ReZ6"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p> Code:</p>\n",
    "    <p>wordlist=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]</p>\n",
    "    <p> A,C are present in the review </p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEDH7-WERea3"
   },
   "source": [
    "At the same time, we should also find a way to represent the labels (positive or negative) in numeric ways for the model to train on. Let's create numpy arrays of labels of 1s and 0s. Let's say 1 corresponds to positive reviews and 0 corresponds to negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we concatenate positive and negative data when producing `unigrams_all` and `features_unigram_presence`, we will follow the same data order to create the numpy array of labels that tell us the sentiment for each feature vector in `features_unigram_presence`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K12juH1zRea4"
   },
   "outputs": [],
   "source": [
    " \n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGNbpwvAReZ7"
   },
   "source": [
    "## Save the features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntPhUTCrReZ7"
   },
   "source": [
    "We can also save the prepared features for the reviews ready for modeling.\n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas.\n",
    "\n",
    "To store a numpy array, we can use the `numpy`'s `save()` function. `save()` takes two arguments: the first is an the filename to be written, and the second argument is the numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE3m4vnpReZ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('features_unigram_presence.npy',features_unigram_presence)\n",
    "np.save('labels_unigram_presence.npy',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gL_Qwr_1ReaC"
   },
   "source": [
    "We can load the postiive data from the files by:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBEc1d1DReaC"
   },
   "outputs": [],
   "source": [
    "np.load('features_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgQWG19jReak"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_SGP7aReak"
   },
   "source": [
    "## Prepare train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuHpK9tfReak"
   },
   "source": [
    "Now it's time to prepare this dataset for model evlauation. We want to train a model that is generalisable to unseen data. Therefore, we can split the dataset into train and test where the model is trained on the train set and tested on the unseen test set. Usually the train-test split is 70% and 30%. \n",
    "\n",
    "We will follow the paper to adopt a 3-fold cross validation. We will use `scikit-learn`'s `KFold` to do that. In addition, to ensure that we have an equal number of positive and negative examples, we make the split in both positive and negative datasets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpmPH4YkReal"
   },
   "source": [
    "Let's first load the unigram presence features and their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.load('features_unigram_presence.npy')\n",
    "labels=np.load('labels_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce the indices for the positive and the negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "neg_id=[i for i,label in enumerate(labels) if label==0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `KFold` from `scikit-learn` and generate cross validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import KFold and random\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "\n",
    "# set the number of folds\n",
    "num_folds=3\n",
    "\n",
    "# set random seed for cross validation\n",
    "CROSS_VAL_SEED = 42\n",
    "\n",
    "# create the KFold object and create the splits for positive data\n",
    "kf_posi = KFold(n_splits=num_folds)\n",
    "kf_posi.get_n_splits(posi_id)\n",
    "kf_posi = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "# create the KFold object and create the splits for negative data\n",
    "kf_neg = KFold(n_splits=num_folds)\n",
    "kf_neg.get_n_splits(posi_id)\n",
    "kf_neg = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "# loop over the folds to create train and test data/label. \n",
    "\n",
    "for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "    print(f'Running fold {fold_idx+1}...')\n",
    "    \n",
    "    # Get the current fold for positive data\n",
    "    fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "    fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "    \n",
    "    # Get the current fold for negative data\n",
    "    train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "    fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "    fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "    \n",
    "    # ensure that we have balanced classes in both train and test\n",
    "    assert len(fold_train_posi)==len(fold_train_neg)\n",
    "    assert len(fold_test_posi)==len(fold_test_neg)\n",
    "    # combine all train and test for the current fold\n",
    "    fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "    fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "    print (fold_train)\n",
    "    \n",
    "    # shuffle the train and test indexes in the current fold\n",
    "    random.shuffle(fold_train)\n",
    "    random.shuffle(fold_test)\n",
    "    \n",
    "    # use the indexes in fold_train and fold_test to slice data and labels\n",
    "    fold_train_data=data[fold_train]\n",
    "    fold_test_data=data[fold_test]\n",
    "    fold_train_label=labels[fold_train]\n",
    "    fold_test_label=labels[fold_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can wrap the above into a function `create_kfold_validadtion()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kfold_validation(data,labels,num_folds):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of features\n",
    "    labels: a numpy array of labels (1=positive, 0=negative)\n",
    "    num_folds: the number of folds in cross validation\n",
    "\n",
    "    Yields (works as a generator)\n",
    "    ------\n",
    "    fold_train_data: current fold of training data as numpy arrays\n",
    "    fold_test_data: current fold of test data as numpy arrays\n",
    "    fold_train_label: current fold of training labels as numpy arrays\n",
    "    fold_test_label: current fold of test labels as numpy arrays\n",
    "    \"\"\"\n",
    "    #produce the indices for the positive and the negative data\n",
    "    posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "    neg_id=[i for i,label in enumerate(labels) if label==0]\n",
    "\n",
    "    # set random seed for cross validation\n",
    "    cross_val_seed = 42\n",
    "\n",
    "    # create the KFold object and create the splits for positive data\n",
    "    kf_posi = KFold(n_splits=num_folds)\n",
    "    kf_posi.get_n_splits(posi_id)\n",
    "    kf_posi = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "    posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "    # create the KFold object and create the splits for negative data\n",
    "    kf_neg = KFold(n_splits=num_folds)\n",
    "    kf_neg.get_n_splits(posi_id)\n",
    "    kf_neg = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "    neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "    # loop over the folds to create train and test data/label. \n",
    "\n",
    "    for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "        print(f'Running fold {fold_idx+1}...')\n",
    "\n",
    "        # Get the current fold for positive data\n",
    "        fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "        fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "\n",
    "        # Get the current fold for negative data\n",
    "        train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "        fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "        fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "\n",
    "        # ensure that we have balanced classes in both train and test\n",
    "        assert len(fold_train_posi)==len(fold_train_neg)\n",
    "        assert len(fold_test_posi)==len(fold_test_neg)\n",
    "        # combine all train and test for the current fold\n",
    "        print ('combining positive and negative for train and test')\n",
    "        fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "        fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "        \n",
    "        \n",
    "        # use the indexes in fold_train and fold_test to slice data and labels\n",
    "        print ('slice training and test data')\n",
    "        fold_train_data=data[fold_train]\n",
    "        fold_test_data=data[fold_test]\n",
    "        fold_train_label=labels[fold_train]\n",
    "        fold_test_label=labels[fold_test]\n",
    "        print ('yield')\n",
    "        \n",
    "        assert len(fold_train_data)==len(fold_train_label)\n",
    "        assert len(fold_test_data)==len(fold_test_label)\n",
    "        yield fold_train_data,fold_test_data,fold_train_label,fold_test_label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call `create_kfold_validation()`, we are initializing a generator. We can loop over the generator to produce `fold_train_data,fold_test_data,fold_train_label,fold_test_label`  for every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(np.array(data),labels,num_folds=3):\n",
    "    print ('length of train data',len(fold_train_data))\n",
    "    print ('length of test data',len(fold_test_data))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2ScVYh9RebR"
   },
   "source": [
    "## Naive Bayes Model and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnKOr56PRebS"
   },
   "source": [
    "Let's try fitting the features and labels from each validation fold into a naive bayes model using the `MultinomialNB` package in `sklearn`. `MultinomialNB` is a Naive Bayes classifier for multinomial models. It implements additive smoothing by default. For more details, please see [scikit-learn's multinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04CNQg8eRebT"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(fold_train_data,fold_train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(fold_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DHgKdlJRebW"
   },
   "source": [
    "In each fold, `predicted` is an array of predictions in 1s and 0s. Let's compare it with the gold labels and calculate accuracy following:\n",
    "\n",
    "accruracy= number of correct examples/number of total examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W8h1ByjReaY"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8ADZKR1ReaZ"
   },
   "source": [
    "Please complete the function `get_accuracy()` below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHhMYkawReaa"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(predicted,gold):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted: a list or numpy array of predicted labels in the form of 1=positive, 0=negative\n",
    "    gold: a list or numpy array of gold labels in the form of 1=positive, 0=negative\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    accuracy score\n",
    "    \"\"\"\n",
    "    correct=0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]==gold[i]: #if the predicted result is the same with the gold label\n",
    "            correct+=1\n",
    "    acc=correct/len(predicted)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VgTk8uTReac"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <code>\n",
    "    def get_accuracy(predicted,gold):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted: a list or numpy array of predicted labels in the form of 1=positive, 0=negative\n",
    "        gold: a list or numpy array of gold labels in the form of 1=positive, 0=negative<br>\n",
    "        Return\n",
    "        ------\n",
    "        accuracy score\n",
    "        \"\"\"\n",
    "        correct=0\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i]==gold[i]: #if the predicted result is the same with the gold label\n",
    "                correct+=1\n",
    "        acc=correct/len(predicted)\n",
    "        return acc\n",
    "        </code>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfCvRGRhRebW"
   },
   "source": [
    "Let's create a function that calls the naive bayes model and performs evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4SstOdiRebc"
   },
   "outputs": [],
   "source": [
    "def train_evaluate(train_data,train_label,test_data,test_label):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of train data (number of examples * number of features)\n",
    "    train_label: numpy array of train labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "    test_data: numpy array of test data (number of examples * number of features)\n",
    "    test_label: numpy array of test labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    accuracy score on the test data\n",
    "    \"\"\"\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(train_data,train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(test_data)\n",
    "    #evaluation:\n",
    "    acc=get_accuracy(predicted,test_label)\n",
    "\n",
    "\n",
    "    print ('accuracy',acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CFSUKl5Rebf"
   },
   "source": [
    "Now integrating `train_evaluate()` into cross validation, we will get accuracy per each fold. We then take the average and standard deviation of the accuracy scores across folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxNffzRuRebg"
   },
   "outputs": [],
   "source": [
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct, you should get:\n",
    "\n",
    "    Mean accuracy      : 0.82000\n",
    "    Standard deviation : 0.00657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: The whole pipeline to train on unigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract unigrams from all documents\n",
    "from os import listdir \n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n",
    "\n",
    "# 2. Turn text to feature vectors\n",
    "# collect all unique unigram as all features\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# map unigram features to dimension indexes\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)\n",
    "# Convert unigrams to feature vectors for all documents\n",
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n",
    "\n",
    "# 3. Encode labels\n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)\n",
    "\n",
    "# 4. Evaluation with 3-fold cross validation\n",
    "data=features_unigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhzv2aDmReaG"
   },
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fOlUBWYReaH"
   },
   "source": [
    "Based on `clean_doc_unigrams()` that turns a document into a unigram dictionary, we can create a function `clean_doc_bigrams()` to extract bigram dictionary. (You can refresh yourself of how to create a bigram counter dictionary in module 1.4. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hehFhHqgReaI"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: string of the document text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a counter dictionary with bigram as key and count as value\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # extract bigrams\n",
    "    bigram_dict=Counter() #initialize a bigram dictionary to be updated\n",
    "    tokens=['<start>']+tokens+['<end>'] # add <start> and <end> token\n",
    "    for i in range(len(tokens)): #loop over all the indices of the token list\n",
    "        if i<len(tokens)-1: #if it's not the end of the token list\n",
    "            bigram_current=(tokens[i],tokens[i+1])\n",
    "            bigram_dict[bigram_current]+=1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzesiKPtReaK"
   },
   "source": [
    "Now we can replace the `clean_doc_unigrams()` line with `clean_doc_bigrams()` in `process_docs_unigrams()`, we will rename the function as `process_docs_bigrams()` that counts all the bigrams in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpo6ay2bReaL"
   },
   "outputs": [],
   "source": [
    "def process_docs_bigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "    overall_bigrams_counter: \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a list of bigram counters each representing a document\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    tokens_all=[]\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_bigrams(text)\n",
    "        tokens_all.append(tokens_dict_current)\n",
    "    return tokens_all\n",
    "bigrams_neg=process_docs_bigrams('./data/neg')\n",
    "bigrams_posi=process_docs_bigrams('./data/pos')\n",
    "bigrams_all=bigrams_posi+bigrams_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1KBMkOFReaN"
   },
   "source": [
    "We can follow the same procedure to turn bigram counters per each document into bigram feature presence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vocab=collect_vocab(bigrams_all)\n",
    "bigram2index,index2bigram=create_vocab_feature_mappings(bigram_vocab)\n",
    "features_bigram_presence=create_feature_presence_all(bigrams_all,bigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the same for `features_unigram_presence`. We can then directly train Naive Bayes model with these bigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=features_bigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCaLlXqgRebo"
   },
   "source": [
    "### ❓Final Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbpA7tuVRebo"
   },
   "source": [
    "1. So far we have trained a model and evaluate on the bigram and unigram presence features, can you modify the pipeline to build a model on bigram frequency features with three fold cross validation? Please report mean and accuracy of the results with CROSS_VAL_SEED=42. How do the results compare with unigram presence features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Extract unigrams from all documents\n",
    "from os import listdir \n",
    "# Bigrams for all the negative and positive files\n",
    "bigrams_neg=process_docs_bigrams('./data/neg')\n",
    "bigrams_posi=process_docs_bigrams('./data/pos')\n",
    "# all bigrams\n",
    "bigrams_all=bigrams_posi+bigrams_neg# unigrams for all the positive files\n",
    "\n",
    "\n",
    "# 2. Turn text to feature vectors\n",
    "# collect all unique unigram as all features\n",
    "bigram_vocab=collect_vocab(bigrams_all)\n",
    "# map bigram features to dimension indexes\n",
    "bigram2index,index2bigram=create_vocab_feature_mappings(bigram_vocab)\n",
    "# Convert bigrams to frequency feature vectors for all documents\n",
    "features_bigram_frequency=create_feature_frequency_all(bigrams_all,bigram2index)\n",
    "\n",
    "# 3. Encode labels\n",
    "labels=len(bigrams_posi)*[1]+len(bigrams_neg)*[0]\n",
    "labels=np.array(labels)\n",
    "\n",
    "# 4. Evaluation with 3-fold cross validation\n",
    "data=features_bigram_frequency\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')\n",
    "\n",
    "Mean accuracy      : 0.83599\n",
    "Standard deviation : 0.00396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A potentially important contextual effect of negation: clearly “good” and “not very good” indicate opposite sentiment orientations. Let's try adding the tag NOT to every word between a negation word (“not”, “isn’t”, “didn’t”, etc.) and the first punctuation mark following the negation word. We have provided the functioon `clean_doc_neg_unigrams()` to extract unigrams with negation tags for each document. Can you integrate the function into the pipeline to run models on unigram presence features with negation tags? What is the mean and std of the accuracy averaged over three-fold cross validation? How does it compare with the results from unigram presence features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATION_WORDS=[\"not\",\"n't\"]\n",
    "def clean_doc_neg_unigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: text from a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary of tokens with negation tag\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    neg_tag=False # whether to add neg tag\n",
    "    tokens_negtag=[]\n",
    "    for token in tokens:\n",
    "        if token in NEGATION_WORDS:\n",
    "            neg_tag=True\n",
    "        elif token in [',','.',':',')','(','\"','-','!','?']:\n",
    "            neg_tag=False # stop adding negation tag to tokens after the punctuation mark\n",
    "        else: # for other words\n",
    "            if neg_tag:\n",
    "                token='NEG_'+token # adding negation tag\n",
    "        tokens_negtag.append(token)\n",
    "    tokens_dict=tokens_to_dict(tokens_negtag)\n",
    "    return tokens_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_docs_neg_unigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of unigram tokens with negation tag counter dictionary where each token dictionary records the frequency of each token that occur in a document. \n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    tokens_all=[]\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_neg_unigrams(text)\n",
    "        tokens_all.append(tokens_dict)\n",
    "    return tokens_all\n",
    "\n",
    "# 1. Extract unigrams with neg tags from all documents\n",
    "from os import listdir \n",
    "# Unigrams for all the negative and positive files\n",
    "unigrams_negation_neg=process_docs_neg_unigrams('./data/neg')\n",
    "unigrams_negation_posi=process_docs_neg_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_negation_all=unigrams_negation_neg+unigrams_negation_posi# unigrams for all the positive files\n",
    "\n",
    "\n",
    "# 2. Turn text to feature vectors\n",
    "# collect all unique unigram as all features\n",
    "unigrams_negation_vocab=collect_vocab(unigrams_negation_all)\n",
    "# map unigram features to dimension indexes\n",
    "unigram_negation2index,index2unigram_negation=create_vocab_feature_mappings(unigrams_negation_vocab)\n",
    "# Convert unigrams to presence feature vectors for all documents\n",
    "features_unigram_presence_negation=create_feature_presence_all(unigrams_negation_all,unigram_negation2index)\n",
    "\n",
    "# 3. Encode labels\n",
    "labels=len(unigrams_negation_posi)*[1]+len(unigrams_negation_neg)*[0]\n",
    "labels=np.array(labels)\n",
    "\n",
    "# 4. Evaluation with 3-fold cross validation\n",
    "data=features_unigram_presence_negation\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')\n",
    "\n",
    "Mean accuracy      : 0.82850\n",
    "Standard deviation : 0.00615"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional resources\n",
    "\n",
    "- Naive Bayes chapter from [Manning, Raghavan and Schütze's Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "-  `scikit-learn`'s [implementation of Naive Bayes](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/naive_bayes.py#L669)\n",
    "- Pang, Lee, Vaithyanathan, 2002, Thumbs up? Sentiment Classification using Machine Learning Techniques [pdf](https://www.aclweb.org/anthology/W02-1011.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_2.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
