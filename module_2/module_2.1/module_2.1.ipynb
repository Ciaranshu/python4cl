{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xv4NPtgAReWr"
   },
   "source": [
    "Welcome to module 2.1. In this module, we will start building and testing a sentiment classifier with Naive Bayes!\n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "## ❓ Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ra8qD88ReWu"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii18vTN8ReWx"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AplzSdOIReWz"
   },
   "source": [
    "Today, we are going to focus on a popular NLP classification task: sentiment analysis. What exactly is sentiment? Sentiment relates to the meaning of a word or sequence of words and is usually associated with an opinion or emotion. And analysis? Well, this is the process of looking at data and making inferences; in this case, using machine learning to learn and predict whether a movie review is positive or negative.\n",
    "\n",
    "In this section, we will replicate the experiments from the paper: Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques (https://www.aclweb.org/anthology/W02-1011.pdf). We will extract a number of features including unigrams, bigrams, pos tags etc., and train Naive Bayes models on these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bav1A-D6ReW1"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2Mgv4bfReW3"
   },
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBsHahXeScH7"
   },
   "source": [
    "As usual, we download the files for the notebook from Github. If you're running this notebook locally or on Binder, you may skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kFuAQbpSdRf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-04 16:07:06--  https://github.com/cambridgeltl/python4cl/raw/module_2.1/module_2/module_2.1/data.zip\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/cambridgeltl/python4cl/module_2.1/module_2/module_2.1/data.zip [following]\n",
      "--2020-11-04 16:07:07--  https://raw.githubusercontent.com/cambridgeltl/python4cl/module_2.1/module_2/module_2.1/data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.60.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.60.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4848090 (4.6M) [application/zip]\n",
      "Saving to: ‘data.zip.1’\n",
      "\n",
      "data.zip.1          100%[===================>]   4.62M  11.3MB/s    in 0.4s    \n",
      "\n",
      "2020-11-04 16:07:08 (11.3 MB/s) - ‘data.zip.1’ saved [4848090/4848090]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/cambridgeltl/python4cl/raw/module_2.1/module_2/module_2.1/data.zip\n",
    "!unzip -n -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFew9K1WReW5"
   },
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file “cv000_29416.txt” as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QngDdJeGReW_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mmv7wS0cReXH"
   },
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOSa4383ReXI"
   },
   "source": [
    "We can turn this into a function called load_doc() that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GZg4B9FReXL"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_13EAmfReXS"
   },
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jL_pY341ReXT"
   },
   "source": [
    "Let's first read in these positive and negative files and store them as two list of texts. To navigate the files, we can use Python's `os` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60kbNDGiReXV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv676_22202.txt\n",
      "Loaded cv839_22807.txt\n",
      "Loaded cv155_7845.txt\n",
      "Loaded cv465_23401.txt\n",
      "Loaded cv398_17047.txt\n",
      "Loaded cv206_15893.txt\n",
      "Loaded cv037_19798.txt\n",
      "Loaded cv279_19452.txt\n",
      "Loaded cv646_16817.txt\n",
      "Loaded cv756_23676.txt\n",
      "Loaded cv823_17055.txt\n",
      "Loaded cv747_18189.txt\n",
      "Loaded cv258_5627.txt\n",
      "Loaded cv948_25870.txt\n",
      "Loaded cv744_10091.txt\n",
      "Loaded cv754_7709.txt\n",
      "Loaded cv838_25886.txt\n",
      "Loaded cv131_11568.txt\n",
      "Loaded cv401_13758.txt\n",
      "Loaded cv523_18285.txt\n",
      "Loaded cv073_23039.txt\n",
      "Loaded cv688_7884.txt\n",
      "Loaded cv664_4264.txt\n",
      "Loaded cv461_21124.txt\n",
      "Loaded cv909_9973.txt\n",
      "Loaded cv939_11247.txt\n",
      "Loaded cv368_11090.txt\n",
      "Loaded cv185_28372.txt\n",
      "Loaded cv749_18960.txt\n",
      "Loaded cv836_14311.txt\n",
      "Loaded cv322_21820.txt\n",
      "Loaded cv789_12991.txt\n",
      "Loaded cv617_9561.txt\n",
      "Loaded cv288_20212.txt\n",
      "Loaded cv464_17076.txt\n",
      "Loaded cv904_25663.txt\n",
      "Loaded cv866_29447.txt\n",
      "Loaded cv429_7937.txt\n",
      "Loaded cv212_10054.txt\n",
      "Loaded cv007_4992.txt\n",
      "Loaded cv522_5418.txt\n",
      "Loaded cv109_22599.txt\n",
      "Loaded cv753_11812.txt\n",
      "Loaded cv312_29308.txt\n",
      "Loaded cv294_12695.txt\n",
      "Loaded cv886_19210.txt\n",
      "Loaded cv479_5450.txt\n",
      "Loaded cv867_18362.txt\n",
      "Loaded cv260_15652.txt\n",
      "Loaded cv313_19337.txt\n",
      "Loaded cv317_25111.txt\n",
      "Loaded cv506_17521.txt\n",
      "Loaded cv602_8830.txt\n",
      "Loaded cv710_23745.txt\n",
      "Loaded cv971_11790.txt\n",
      "Loaded cv098_17021.txt\n",
      "Loaded cv960_28877.txt\n",
      "Loaded cv423_12089.txt\n",
      "Loaded cv887_5306.txt\n",
      "Loaded cv291_26844.txt\n",
      "Loaded cv049_21917.txt\n",
      "Loaded cv382_8393.txt\n",
      "Loaded cv915_9342.txt\n",
      "Loaded cv217_28707.txt\n",
      "Loaded cv729_10475.txt\n",
      "Loaded cv862_15924.txt\n",
      "Loaded cv034_29446.txt\n",
      "Loaded cv560_18608.txt\n",
      "Loaded cv684_12727.txt\n",
      "Loaded cv627_12603.txt\n",
      "Loaded cv498_9288.txt\n",
      "Loaded cv044_18429.txt\n",
      "Loaded cv359_6751.txt\n",
      "Loaded cv537_13516.txt\n",
      "Loaded cv089_12222.txt\n",
      "Loaded cv784_16077.txt\n",
      "Loaded cv801_26335.txt\n",
      "Loaded cv692_17026.txt\n",
      "Loaded cv531_26838.txt\n",
      "Loaded cv913_29127.txt\n",
      "Loaded cv319_16459.txt\n",
      "Loaded cv377_8440.txt\n",
      "Loaded cv943_23547.txt\n",
      "Loaded cv691_5090.txt\n",
      "Loaded cv604_23339.txt\n",
      "Loaded cv327_21743.txt\n",
      "Loaded cv718_12227.txt\n",
      "Loaded cv686_15553.txt\n",
      "Loaded cv195_16146.txt\n",
      "Loaded cv170_29808.txt\n",
      "Loaded cv254_5870.txt\n",
      "Loaded cv435_24355.txt\n",
      "Loaded cv184_26935.txt\n",
      "Loaded cv388_12810.txt\n",
      "Loaded cv835_20531.txt\n",
      "Loaded cv541_28683.txt\n",
      "Loaded cv609_25038.txt\n",
      "Loaded cv708_28539.txt\n",
      "Loaded cv386_10229.txt\n",
      "Loaded cv392_12238.txt\n",
      "Loaded cv822_21545.txt\n",
      "Loaded cv758_9740.txt\n",
      "Loaded cv819_9567.txt\n",
      "Loaded cv704_17622.txt\n",
      "Loaded cv458_9000.txt\n",
      "Loaded cv679_28221.txt\n",
      "Loaded cv097_26081.txt\n",
      "Loaded cv740_13643.txt\n",
      "Loaded cv773_20264.txt\n",
      "Loaded cv600_25043.txt\n",
      "Loaded cv416_12048.txt\n",
      "Loaded cv702_12371.txt\n",
      "Loaded cv527_10338.txt\n",
      "Loaded cv055_8926.txt\n",
      "Loaded cv798_24779.txt\n",
      "Loaded cv371_8197.txt\n",
      "Loaded cv549_22771.txt\n",
      "Loaded cv283_11963.txt\n",
      "Loaded cv430_18662.txt\n",
      "Loaded cv921_13988.txt\n",
      "Loaded cv730_10729.txt\n",
      "Loaded cv107_25639.txt\n",
      "Loaded cv705_11973.txt\n",
      "Loaded cv421_9752.txt\n",
      "Loaded cv447_27334.txt\n",
      "Loaded cv739_12179.txt\n",
      "Loaded cv240_15948.txt\n",
      "Loaded cv122_7891.txt\n",
      "Loaded cv460_11723.txt\n",
      "Loaded cv893_26731.txt\n",
      "Loaded cv234_22123.txt\n",
      "Loaded cv803_8584.txt\n",
      "Loaded cv379_23167.txt\n",
      "Loaded cv198_19313.txt\n",
      "Loaded cv556_16563.txt\n",
      "Loaded cv632_9704.txt\n",
      "Loaded cv853_29119.txt\n",
      "Loaded cv023_13847.txt\n",
      "Loaded cv410_25624.txt\n",
      "Loaded cv490_18986.txt\n",
      "Loaded cv755_24881.txt\n",
      "Loaded cv930_14949.txt\n",
      "Loaded cv645_17078.txt\n",
      "Loaded cv497_27086.txt\n",
      "Loaded cv876_9633.txt\n",
      "Loaded cv841_3367.txt\n",
      "Loaded cv318_11146.txt\n",
      "Loaded cv189_24248.txt\n",
      "Loaded cv888_25678.txt\n",
      "Loaded cv210_9557.txt\n",
      "Loaded cv815_23466.txt\n",
      "Loaded cv227_25406.txt\n",
      "Loaded cv748_14044.txt\n",
      "Loaded cv667_19672.txt\n",
      "Loaded cv437_24070.txt\n",
      "Loaded cv988_20168.txt\n",
      "Loaded cv027_26270.txt\n",
      "Loaded cv918_27080.txt\n",
      "Loaded cv768_12709.txt\n",
      "Loaded cv036_18385.txt\n",
      "Loaded cv938_10706.txt\n",
      "Loaded cv362_16985.txt\n",
      "Loaded cv849_17215.txt\n",
      "Loaded cv144_5010.txt\n",
      "Loaded cv320_9693.txt\n",
      "Loaded cv492_19370.txt\n",
      "Loaded cv112_12178.txt\n",
      "Loaded cv558_29376.txt\n",
      "Loaded cv141_17179.txt\n",
      "Loaded cv168_7435.txt\n",
      "Loaded cv944_15042.txt\n",
      "Loaded cv308_5079.txt\n",
      "Loaded cv752_25330.txt\n",
      "Loaded cv356_26170.txt\n",
      "Loaded cv738_10287.txt\n",
      "Loaded cv731_3968.txt\n",
      "Loaded cv519_16239.txt\n",
      "Loaded cv847_20855.txt\n",
      "Loaded cv687_22207.txt\n",
      "Loaded cv854_18955.txt\n",
      "Loaded cv050_12128.txt\n",
      "Loaded cv179_9533.txt\n",
      "Loaded cv489_19046.txt\n",
      "Loaded cv127_16451.txt\n",
      "Loaded cv496_11185.txt\n",
      "Loaded cv614_11320.txt\n",
      "Loaded cv140_7963.txt\n",
      "Loaded cv987_7394.txt\n",
      "Loaded cv851_21895.txt\n",
      "Loaded cv228_5644.txt\n",
      "Loaded cv542_20359.txt\n",
      "Loaded cv225_29083.txt\n",
      "Loaded cv063_28852.txt\n",
      "Loaded cv341_25667.txt\n",
      "Loaded cv563_18610.txt\n",
      "Loaded cv022_14227.txt\n",
      "Loaded cv520_13297.txt\n",
      "Loaded cv102_8306.txt\n",
      "Loaded cv150_14279.txt\n",
      "Loaded cv858_20266.txt\n",
      "Loaded cv936_17473.txt\n",
      "Loaded cv762_15604.txt\n",
      "Loaded cv891_6035.txt\n",
      "Loaded cv445_26683.txt\n",
      "Loaded cv946_20084.txt\n",
      "Loaded cv507_9509.txt\n",
      "Loaded cv330_29675.txt\n",
      "Loaded cv350_22139.txt\n",
      "Loaded cv837_27232.txt\n",
      "Loaded cv347_14722.txt\n",
      "Loaded cv483_18103.txt\n",
      "Loaded cv287_17410.txt\n",
      "Loaded cv777_10247.txt\n",
      "Loaded cv630_10152.txt\n",
      "Loaded cv975_11920.txt\n",
      "Loaded cv633_29730.txt\n",
      "Loaded cv028_26964.txt\n",
      "Loaded cv101_10537.txt\n",
      "Loaded cv171_15164.txt\n",
      "Loaded cv554_14678.txt\n",
      "Loaded cv963_7208.txt\n",
      "Loaded cv775_17966.txt\n",
      "Loaded cv564_12011.txt\n",
      "Loaded cv448_16409.txt\n",
      "Loaded cv681_9744.txt\n",
      "Loaded cv865_28796.txt\n",
      "Loaded cv770_11061.txt\n",
      "Loaded cv264_14108.txt\n",
      "Loaded cv650_15974.txt\n",
      "Loaded cv714_19704.txt\n",
      "Loaded cv607_8235.txt\n",
      "Loaded cv699_7773.txt\n",
      "Loaded cv205_9676.txt\n",
      "Loaded cv297_10104.txt\n",
      "Loaded cv491_12992.txt\n",
      "Loaded cv310_14568.txt\n",
      "Loaded cv475_22978.txt\n",
      "Loaded cv824_9335.txt\n",
      "Loaded cv941_10718.txt\n",
      "Loaded cv965_26688.txt\n",
      "Loaded cv176_14196.txt\n",
      "Loaded cv349_15032.txt\n",
      "Loaded cv612_5396.txt\n",
      "Loaded cv053_23117.txt\n",
      "Loaded cv040_8829.txt\n",
      "Loaded cv370_5338.txt\n",
      "Loaded cv203_19052.txt\n",
      "Loaded cv623_16988.txt\n",
      "Loaded cv937_9816.txt\n",
      "Loaded cv394_5311.txt\n",
      "Loaded cv054_4101.txt\n",
      "Loaded cv139_14236.txt\n",
      "Loaded cv134_23300.txt\n",
      "Loaded cv580_15681.txt\n",
      "Loaded cv986_15092.txt\n",
      "Loaded cv611_2253.txt\n",
      "Loaded cv816_15257.txt\n",
      "Loaded cv393_29234.txt\n",
      "Loaded cv326_14777.txt\n",
      "Loaded cv302_26481.txt\n",
      "Loaded cv010_29063.txt\n",
      "Loaded cv415_23674.txt\n",
      "Loaded cv932_14854.txt\n",
      "Loaded cv003_12683.txt\n",
      "Loaded cv962_9813.txt\n",
      "Loaded cv115_26443.txt\n",
      "Loaded cv409_29625.txt\n",
      "Loaded cv180_17823.txt\n",
      "Loaded cv502_10970.txt\n",
      "Loaded cv615_15734.txt\n",
      "Loaded cv735_20218.txt\n",
      "Loaded cv482_11233.txt\n",
      "Loaded cv324_7502.txt\n",
      "Loaded cv926_18471.txt\n",
      "Loaded cv844_13890.txt\n",
      "Loaded cv845_15886.txt\n",
      "Loaded cv257_11856.txt\n",
      "Loaded cv335_16299.txt\n",
      "Loaded cv634_11989.txt\n",
      "Loaded cv200_29006.txt\n",
      "Loaded cv192_16079.txt\n",
      "Loaded cv759_15091.txt\n",
      "Loaded cv009_29417.txt\n",
      "Loaded cv123_12165.txt\n",
      "Loaded cv024_7033.txt\n",
      "Loaded cv655_12055.txt\n",
      "Loaded cv366_10709.txt\n",
      "Loaded cv208_9475.txt\n",
      "Loaded cv931_18783.txt\n",
      "Loaded cv656_25395.txt\n",
      "Loaded cv828_21392.txt\n",
      "Loaded cv440_16891.txt\n",
      "Loaded cv146_19587.txt\n",
      "Loaded cv671_5164.txt\n",
      "Loaded cv113_24354.txt\n",
      "Loaded cv501_12675.txt\n",
      "Loaded cv493_14135.txt\n",
      "Loaded cv622_8583.txt\n",
      "Loaded cv643_29282.txt\n",
      "Loaded cv703_17948.txt\n",
      "Loaded cv121_18621.txt\n",
      "Loaded cv378_21982.txt\n",
      "Loaded cv902_13217.txt\n",
      "Loaded cv162_10977.txt\n",
      "Loaded cv922_10185.txt\n",
      "Loaded cv105_19135.txt\n",
      "Loaded cv006_17022.txt\n",
      "Loaded cv526_12868.txt\n",
      "Loaded cv390_12187.txt\n",
      "Loaded cv885_13390.txt\n",
      "Loaded cv778_18629.txt\n",
      "Loaded cv535_21183.txt\n",
      "Loaded cv088_25274.txt\n",
      "Loaded cv128_29444.txt\n",
      "Loaded cv525_17930.txt\n",
      "Loaded cv651_11120.txt\n",
      "Loaded cv905_28965.txt\n",
      "Loaded cv592_23391.txt\n",
      "Loaded cv665_29386.txt\n",
      "Loaded cv137_17020.txt\n",
      "Loaded cv534_15683.txt\n",
      "Loaded cv295_17060.txt\n",
      "Loaded cv411_16799.txt\n",
      "Loaded cv499_11407.txt\n",
      "Loaded cv782_21078.txt\n",
      "Loaded cv201_7421.txt\n",
      "Loaded cv883_27621.txt\n",
      "Loaded cv292_7804.txt\n",
      "Loaded cv282_6833.txt\n",
      "Loaded cv065_16909.txt\n",
      "Loaded cv776_21934.txt\n",
      "Loaded cv487_11058.txt\n",
      "Loaded cv969_14760.txt\n",
      "Loaded cv964_5794.txt\n",
      "Loaded cv925_9459.txt\n",
      "Loaded cv979_2029.txt\n",
      "Loaded cv553_26965.txt\n",
      "Loaded cv638_29394.txt\n",
      "Loaded cv795_10291.txt\n",
      "Loaded cv610_24153.txt\n",
      "Loaded cv725_10266.txt\n",
      "Loaded cv035_3343.txt\n",
      "Loaded cv923_11951.txt\n",
      "Loaded cv884_15230.txt\n",
      "Loaded cv352_5414.txt\n",
      "Loaded cv025_29825.txt\n",
      "Loaded cv426_10976.txt\n",
      "Loaded cv223_28923.txt\n",
      "Loaded cv074_7188.txt\n",
      "Loaded cv467_26610.txt\n",
      "Loaded cv114_19501.txt\n",
      "Loaded cv038_9781.txt\n",
      "Loaded cv296_13146.txt\n",
      "Loaded cv514_12173.txt\n",
      "Loaded cv790_16202.txt\n",
      "Loaded cv015_29356.txt\n",
      "Loaded cv547_18043.txt\n",
      "Loaded cv270_5873.txt\n",
      "Loaded cv529_10972.txt\n",
      "Loaded cv485_26879.txt\n",
      "Loaded cv736_24947.txt\n",
      "Loaded cv271_15364.txt\n",
      "Loaded cv289_6239.txt\n",
      "Loaded cv344_5376.txt\n",
      "Loaded cv745_14009.txt\n",
      "Loaded cv165_2389.txt\n",
      "Loaded cv585_23576.txt\n",
      "Loaded cv384_18536.txt\n",
      "Loaded cv763_16486.txt\n",
      "Loaded cv442_15499.txt\n",
      "Loaded cv033_25680.txt\n",
      "Loaded cv933_24953.txt\n",
      "Loaded cv641_13412.txt\n",
      "Loaded cv216_20165.txt\n",
      "Loaded cv601_24759.txt\n",
      "Loaded cv572_20053.txt\n",
      "Loaded cv894_22140.txt\n",
      "Loaded cv618_9469.txt\n",
      "Loaded cv376_20883.txt\n",
      "Loaded cv860_15520.txt\n",
      "Loaded cv914_2856.txt\n",
      "Loaded cv983_24219.txt\n",
      "Loaded cv583_29465.txt\n",
      "Loaded cv584_29549.txt\n",
      "Loaded cv166_11959.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv959_16218.txt\n",
      "Loaded cv004_12641.txt\n",
      "Loaded cv706_25883.txt\n",
      "Loaded cv579_12542.txt\n",
      "Loaded cv947_11316.txt\n",
      "Loaded cv929_1841.txt\n",
      "Loaded cv713_29002.txt\n",
      "Loaded cv226_26692.txt\n",
      "Loaded cv561_9484.txt\n",
      "Loaded cv951_11816.txt\n",
      "Loaded cv495_16121.txt\n",
      "Loaded cv420_28631.txt\n",
      "Loaded cv761_13769.txt\n",
      "Loaded cv346_19198.txt\n",
      "Loaded cv106_18379.txt\n",
      "Loaded cv389_9611.txt\n",
      "Loaded cv488_21453.txt\n",
      "Loaded cv850_18185.txt\n",
      "Loaded cv129_18373.txt\n",
      "Loaded cv436_20564.txt\n",
      "Loaded cv032_23718.txt\n",
      "Loaded cv087_2145.txt\n",
      "Loaded cv075_6250.txt\n",
      "Loaded cv303_27366.txt\n",
      "Loaded cv970_19532.txt\n",
      "Loaded cv174_9735.txt\n",
      "Loaded cv700_23163.txt\n",
      "Loaded cv690_5425.txt\n",
      "Loaded cv781_5358.txt\n",
      "Loaded cv910_21930.txt\n",
      "Loaded cv571_29292.txt\n",
      "Loaded cv890_3515.txt\n",
      "Loaded cv047_18725.txt\n",
      "Loaded cv605_12730.txt\n",
      "Loaded cv454_21961.txt\n",
      "Loaded cv280_8651.txt\n",
      "Loaded cv869_24782.txt\n",
      "Loaded cv920_29423.txt\n",
      "Loaded cv414_11161.txt\n",
      "Loaded cv181_16083.txt\n",
      "Loaded cv433_10443.txt\n",
      "Loaded cv693_19147.txt\n",
      "Loaded cv472_29140.txt\n",
      "Loaded cv990_12443.txt\n",
      "Loaded cv675_22871.txt\n",
      "Loaded cv809_5012.txt\n",
      "Loaded cv232_16768.txt\n",
      "Loaded cv298_24487.txt\n",
      "Loaded cv787_15277.txt\n",
      "Loaded cv056_14663.txt\n",
      "Loaded cv404_21805.txt\n",
      "Loaded cv059_28723.txt\n",
      "Loaded cv451_11502.txt\n",
      "Loaded cv029_19943.txt\n",
      "Loaded cv016_4348.txt\n",
      "Loaded cv546_12723.txt\n",
      "Loaded cv286_26156.txt\n",
      "Loaded cv120_3793.txt\n",
      "Loaded cv061_9321.txt\n",
      "Loaded cv238_14285.txt\n",
      "Loaded cv143_21158.txt\n",
      "Loaded cv157_29302.txt\n",
      "Loaded cv624_11601.txt\n",
      "Loaded cv694_4526.txt\n",
      "Loaded cv757_10668.txt\n",
      "Loaded cv950_13478.txt\n",
      "Loaded cv562_10847.txt\n",
      "Loaded cv608_24647.txt\n",
      "Loaded cv241_24602.txt\n",
      "Loaded cv746_10471.txt\n",
      "Loaded cv889_22670.txt\n",
      "Loaded cv315_12638.txt\n",
      "Loaded cv829_21725.txt\n",
      "Loaded cv202_11382.txt\n",
      "Loaded cv796_17243.txt\n",
      "Loaded cv071_12969.txt\n",
      "Loaded cv082_11979.txt\n",
      "Loaded cv870_18090.txt\n",
      "Loaded cv043_16808.txt\n",
      "Loaded cv391_11615.txt\n",
      "Loaded cv380_8164.txt\n",
      "Loaded cv997_5152.txt\n",
      "Loaded cv998_15691.txt\n",
      "Loaded cv512_17618.txt\n",
      "Loaded cv640_5380.txt\n",
      "Loaded cv550_23226.txt\n",
      "Loaded cv548_18944.txt\n",
      "Loaded cv701_15880.txt\n",
      "Loaded cv954_19932.txt\n",
      "Loaded cv019_16117.txt\n",
      "Loaded cv039_5963.txt\n",
      "Loaded cv626_7907.txt\n",
      "Loaded cv689_13701.txt\n",
      "Loaded cv117_25625.txt\n",
      "Loaded cv518_14798.txt\n",
      "Loaded cv154_9562.txt\n",
      "Loaded cv148_18084.txt\n",
      "Loaded cv428_12202.txt\n",
      "Loaded cv177_10904.txt\n",
      "Loaded cv337_29061.txt\n",
      "Loaded cv374_26455.txt\n",
      "Loaded cv357_14710.txt\n",
      "Loaded cv169_24973.txt\n",
      "Loaded cv727_5006.txt\n",
      "Loaded cv861_12809.txt\n",
      "Loaded cv955_26154.txt\n",
      "Loaded cv982_22209.txt\n",
      "Loaded cv880_29629.txt\n",
      "Loaded cv792_3257.txt\n",
      "Loaded cv325_18330.txt\n",
      "Loaded cv741_12765.txt\n",
      "Loaded cv263_20693.txt\n",
      "Loaded cv539_21865.txt\n",
      "Loaded cv299_17950.txt\n",
      "Loaded cv400_20631.txt\n",
      "Loaded cv825_5168.txt\n",
      "Loaded cv597_26744.txt\n",
      "Loaded cv716_11153.txt\n",
      "Loaded cv996_12447.txt\n",
      "Loaded cv220_28906.txt\n",
      "Loaded cv820_24157.txt\n",
      "Loaded cv272_20313.txt\n",
      "Loaded cv682_17947.txt\n",
      "Loaded cv685_5710.txt\n",
      "Loaded cv978_22192.txt\n",
      "Loaded cv248_15672.txt\n",
      "Loaded cv233_17614.txt\n",
      "Loaded cv647_15275.txt\n",
      "Loaded cv698_16930.txt\n",
      "Loaded cv194_12855.txt\n",
      "Loaded cv806_9405.txt\n",
      "Loaded cv193_5393.txt\n",
      "Loaded cv405_21868.txt\n",
      "Loaded cv707_11421.txt\n",
      "Loaded cv072_5928.txt\n",
      "Loaded cv808_13773.txt\n",
      "Loaded cv147_22625.txt\n",
      "Loaded cv892_18788.txt\n",
      "Loaded cv267_16618.txt\n",
      "Loaded cv441_15276.txt\n",
      "Loaded cv259_11827.txt\n",
      "Loaded cv161_12224.txt\n",
      "Loaded cv896_17819.txt\n",
      "Loaded cv424_9268.txt\n",
      "Loaded cv908_17779.txt\n",
      "Loaded cv639_10797.txt\n",
      "Loaded cv338_9183.txt\n",
      "Loaded cv907_3193.txt\n",
      "Loaded cv574_23191.txt\n",
      "Loaded cv596_4367.txt\n",
      "Loaded cv477_23530.txt\n",
      "Loaded cv080_14899.txt\n",
      "Loaded cv385_29621.txt\n",
      "Loaded cv239_29828.txt\n",
      "Loaded cv246_28668.txt\n",
      "Loaded cv069_11613.txt\n",
      "Loaded cv079_12766.txt\n",
      "Loaded cv152_9052.txt\n",
      "Loaded cv672_27988.txt\n",
      "Loaded cv606_17672.txt\n",
      "Loaded cv419_14799.txt\n",
      "Loaded cv321_14191.txt\n",
      "Loaded cv917_29484.txt\n",
      "Loaded cv555_25047.txt\n",
      "Loaded cv008_29326.txt\n",
      "Loaded cv927_11471.txt\n",
      "Loaded cv231_11028.txt\n",
      "Loaded cv160_10848.txt\n",
      "Loaded cv953_7078.txt\n",
      "Loaded cv457_19546.txt\n",
      "Loaded cv237_20635.txt\n",
      "Loaded cv042_11927.txt\n",
      "Loaded cv742_8279.txt\n",
      "Loaded cv629_16604.txt\n",
      "Loaded cv783_14724.txt\n",
      "Loaded cv993_29565.txt\n",
      "Loaded cv935_24977.txt\n",
      "Loaded cv387_12391.txt\n",
      "Loaded cv972_26837.txt\n",
      "Loaded cv103_11943.txt\n",
      "Loaded cv142_23657.txt\n",
      "Loaded cv306_10859.txt\n",
      "Loaded cv995_23113.txt\n",
      "Loaded cv274_26379.txt\n",
      "Loaded cv949_21565.txt\n",
      "Loaded cv576_15688.txt\n",
      "Loaded cv721_28993.txt\n",
      "Loaded cv508_17742.txt\n",
      "Loaded cv104_19176.txt\n",
      "Loaded cv709_11173.txt\n",
      "Loaded cv723_9002.txt\n",
      "Loaded cv794_17353.txt\n",
      "Loaded cv985_5964.txt\n",
      "Loaded cv879_16585.txt\n",
      "Loaded cv831_16325.txt\n",
      "Loaded cv020_9234.txt\n",
      "Loaded cv785_23748.txt\n",
      "Loaded cv275_28725.txt\n",
      "Loaded cv842_5702.txt\n",
      "Loaded cv543_5107.txt\n",
      "Loaded cv532_6495.txt\n",
      "Loaded cv187_14112.txt\n",
      "Loaded cv011_13044.txt\n",
      "Loaded cv450_8319.txt\n",
      "Loaded cv247_14668.txt\n",
      "Loaded cv236_12427.txt\n",
      "Loaded cv827_19479.txt\n",
      "Loaded cv658_11186.txt\n",
      "Loaded cv712_24217.txt\n",
      "Loaded cv582_6678.txt\n",
      "Loaded cv030_22893.txt\n",
      "Loaded cv094_27868.txt\n",
      "Loaded cv786_23608.txt\n",
      "Loaded cv151_17231.txt\n",
      "Loaded cv364_14254.txt\n",
      "Loaded cv276_17126.txt\n",
      "Loaded cv945_13012.txt\n",
      "Loaded cv767_15673.txt\n",
      "Loaded cv167_18094.txt\n",
      "Loaded cv149_17084.txt\n",
      "Loaded cv158_10914.txt\n",
      "Loaded cv334_0074.txt\n",
      "Loaded cv956_12547.txt\n",
      "Loaded cv589_12853.txt\n",
      "Loaded cv680_10533.txt\n",
      "Loaded cv474_10682.txt\n",
      "Loaded cv354_8573.txt\n",
      "Loaded cv834_23192.txt\n",
      "Loaded cv649_13947.txt\n",
      "Loaded cv108_17064.txt\n",
      "Loaded cv268_20288.txt\n",
      "Loaded cv126_28821.txt\n",
      "Loaded cv912_5562.txt\n",
      "Loaded cv791_17995.txt\n",
      "Loaded cv311_17708.txt\n",
      "Loaded cv521_1730.txt\n",
      "Loaded cv697_12106.txt\n",
      "Loaded cv256_16529.txt\n",
      "Loaded cv463_10846.txt\n",
      "Loaded cv780_8467.txt\n",
      "Loaded cv406_22199.txt\n",
      "Loaded cv715_19246.txt\n",
      "Loaded cv456_20370.txt\n",
      "Loaded cv545_12848.txt\n",
      "Loaded cv728_17931.txt\n",
      "Loaded cv224_18875.txt\n",
      "Loaded cv586_8048.txt\n",
      "Loaded cv397_28890.txt\n",
      "Loaded cv984_14006.txt\n",
      "Loaded cv934_20426.txt\n",
      "Loaded cv578_16825.txt\n",
      "Loaded cv575_22598.txt\n",
      "Loaded cv737_28733.txt\n",
      "Loaded cv057_7962.txt\n",
      "Loaded cv631_4782.txt\n",
      "Loaded cv221_27081.txt\n",
      "Loaded cv262_13812.txt\n",
      "Loaded cv446_12209.txt\n",
      "Loaded cv013_10494.txt\n",
      "Loaded cv095_28730.txt\n",
      "Loaded cv833_11961.txt\n",
      "Loaded cv856_28882.txt\n",
      "Loaded cv215_23246.txt\n",
      "Loaded cv812_19051.txt\n",
      "Loaded cv875_5622.txt\n",
      "Loaded cv573_29384.txt\n",
      "Loaded cv281_24711.txt\n",
      "Loaded cv348_19207.txt\n",
      "Loaded cv734_22821.txt\n",
      "Loaded cv253_10190.txt\n",
      "Loaded cv636_16954.txt\n",
      "Loaded cv957_9059.txt\n",
      "Loaded cv145_12239.txt\n",
      "Loaded cv332_17997.txt\n",
      "Loaded cv802_28381.txt\n",
      "Loaded cv026_29229.txt\n",
      "Loaded cv810_13660.txt\n",
      "Loaded cv229_15200.txt\n",
      "Loaded cv014_15600.txt\n",
      "Loaded cv439_17633.txt\n",
      "Loaded cv422_9632.txt\n",
      "Loaded cv900_10800.txt\n",
      "Loaded cv530_17949.txt\n",
      "Loaded cv455_28866.txt\n",
      "Loaded cv204_8930.txt\n",
      "Loaded cv453_10911.txt\n",
      "Loaded cv625_13518.txt\n",
      "Loaded cv805_21128.txt\n",
      "Loaded cv471_18405.txt\n",
      "Loaded cv961_5578.txt\n",
      "Loaded cv425_8603.txt\n",
      "Loaded cv637_13682.txt\n",
      "Loaded cv826_12761.txt\n",
      "Loaded cv251_23901.txt\n",
      "Loaded cv788_26409.txt\n",
      "Loaded cv300_23302.txt\n",
      "Loaded cv567_29420.txt\n",
      "Loaded cv799_19812.txt\n",
      "Loaded cv222_18720.txt\n",
      "Loaded cv769_8565.txt\n",
      "Loaded cv552_0150.txt\n",
      "Loaded cv644_18551.txt\n",
      "Loaded cv540_3092.txt\n",
      "Loaded cv066_11668.txt\n",
      "Loaded cv153_11607.txt\n",
      "Loaded cv991_19973.txt\n",
      "Loaded cv852_27512.txt\n",
      "Loaded cv173_4295.txt\n",
      "Loaded cv365_12442.txt\n",
      "Loaded cv214_13285.txt\n",
      "Loaded cv077_23172.txt\n",
      "Loaded cv459_21834.txt\n",
      "Loaded cv620_2556.txt\n",
      "Loaded cv673_25874.txt\n",
      "Loaded cv722_7571.txt\n",
      "Loaded cv992_12806.txt\n",
      "Loaded cv733_9891.txt\n",
      "Loaded cv807_23024.txt\n",
      "Loaded cv511_10360.txt\n",
      "Loaded cv118_28837.txt\n",
      "Loaded cv942_18509.txt\n",
      "Loaded cv577_28220.txt\n",
      "Loaded cv898_1576.txt\n",
      "Loaded cv598_18184.txt\n",
      "Loaded cv083_25491.txt\n",
      "Loaded cv290_11981.txt\n",
      "Loaded cv678_14887.txt\n",
      "Loaded cv135_12506.txt\n",
      "Loaded cv588_14467.txt\n",
      "Loaded cv654_19345.txt\n",
      "Loaded cv018_21672.txt\n",
      "Loaded cv048_18380.txt\n",
      "Loaded cv207_29141.txt\n",
      "Loaded cv096_12262.txt\n",
      "Loaded cv516_12117.txt\n",
      "Loaded cv369_14245.txt\n",
      "Loaded cv345_9966.txt\n",
      "Loaded cv695_22268.txt\n",
      "Loaded cv484_26169.txt\n",
      "Loaded cv339_22452.txt\n",
      "Loaded cv001_19502.txt\n",
      "Loaded cv163_10110.txt\n",
      "Loaded cv581_20790.txt\n",
      "Loaded cv277_20467.txt\n",
      "Loaded cv358_11557.txt\n",
      "Loaded cv469_21998.txt\n",
      "Loaded cv642_29788.txt\n",
      "Loaded cv976_10724.txt\n",
      "Loaded cv244_22935.txt\n",
      "Loaded cv058_8469.txt\n",
      "Loaded cv307_26382.txt\n",
      "Loaded cv305_9937.txt\n",
      "Loaded cv355_18174.txt\n",
      "Loaded cv772_12971.txt\n",
      "Loaded cv750_10606.txt\n",
      "Loaded cv243_22164.txt\n",
      "Loaded cv764_12701.txt\n",
      "Loaded cv674_11593.txt\n",
      "Loaded cv980_11851.txt\n",
      "Loaded cv351_17029.txt\n",
      "Loaded cv111_12253.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cv551_11214.txt\n",
      "Loaded cv402_16097.txt\n",
      "Loaded cv871_25971.txt\n",
      "Loaded cv878_17204.txt\n",
      "Loaded cv989_17297.txt\n",
      "Loaded cv719_5581.txt\n",
      "Loaded cv084_15183.txt\n",
      "Loaded cv188_20687.txt\n",
      "Loaded cv766_7983.txt\n",
      "Loaded cv857_17527.txt\n",
      "Loaded cv340_14776.txt\n",
      "Loaded cv872_13710.txt\n",
      "Loaded cv771_28466.txt\n",
      "Loaded cv230_7913.txt\n",
      "Loaded cv559_0057.txt\n",
      "Loaded cv266_26644.txt\n",
      "Loaded cv125_9636.txt\n",
      "Loaded cv566_8967.txt\n",
      "Loaded cv628_20758.txt\n",
      "Loaded cv218_25651.txt\n",
      "Loaded cv353_19197.txt\n",
      "Loaded cv417_14653.txt\n",
      "Loaded cv999_14636.txt\n",
      "Loaded cv265_11625.txt\n",
      "Loaded cv462_20788.txt\n",
      "Loaded cv093_15606.txt\n",
      "Loaded cv897_11703.txt\n",
      "Loaded cv899_17812.txt\n",
      "Loaded cv068_14810.txt\n",
      "Loaded cv536_27221.txt\n",
      "Loaded cv568_17065.txt\n",
      "Loaded cv940_18935.txt\n",
      "Loaded cv399_28593.txt\n",
      "Loaded cv438_8500.txt\n",
      "Loaded cv683_13047.txt\n",
      "Loaded cv211_9955.txt\n",
      "Loaded cv663_14484.txt\n",
      "Loaded cv662_14791.txt\n",
      "Loaded cv245_8938.txt\n",
      "Loaded cv116_28734.txt\n",
      "Loaded cv594_11945.txt\n",
      "Loaded cv099_11189.txt\n",
      "Loaded cv427_11693.txt\n",
      "Loaded cv510_24758.txt\n",
      "Loaded cv373_21872.txt\n",
      "Loaded cv590_20712.txt\n",
      "Loaded cv657_25835.txt\n",
      "Loaded cv952_26375.txt\n",
      "Loaded cv570_28960.txt\n",
      "Loaded cv219_19874.txt\n",
      "Loaded cv199_9721.txt\n",
      "Loaded cv895_22200.txt\n",
      "Loaded cv967_5626.txt\n",
      "Loaded cv480_21195.txt\n",
      "Loaded cv533_9843.txt\n",
      "Loaded cv958_13020.txt\n",
      "Loaded cv840_18033.txt\n",
      "Loaded cv046_10613.txt\n",
      "Loaded cv743_17023.txt\n",
      "Loaded cv190_27176.txt\n",
      "Loaded cv544_5301.txt\n",
      "Loaded cv342_20917.txt\n",
      "Loaded cv213_20300.txt\n",
      "Loaded cv613_23104.txt\n",
      "Loaded cv868_12799.txt\n",
      "Loaded cv813_6649.txt\n",
      "Loaded cv648_17277.txt\n",
      "Loaded cv811_22646.txt\n",
      "Loaded cv800_13494.txt\n",
      "Loaded cv717_17472.txt\n",
      "Loaded cv336_10363.txt\n",
      "Loaded cv408_5367.txt\n",
      "Loaded cv136_12384.txt\n",
      "Loaded cv859_15689.txt\n",
      "Loaded cv051_10751.txt\n",
      "Loaded cv164_23451.txt\n",
      "Loaded cv587_20532.txt\n",
      "Loaded cv595_26420.txt\n",
      "Loaded cv974_24303.txt\n",
      "Loaded cv814_20316.txt\n",
      "Loaded cv765_20429.txt\n",
      "Loaded cv363_29273.txt\n",
      "Loaded cv881_14767.txt\n",
      "Loaded cv278_14533.txt\n",
      "Loaded cv060_11754.txt\n",
      "Loaded cv504_29120.txt\n",
      "Loaded cv470_17444.txt\n",
      "Loaded cv017_23487.txt\n",
      "Loaded cv666_20301.txt\n",
      "Loaded cv031_19540.txt\n",
      "Loaded cv301_13010.txt\n",
      "Loaded cv599_22197.txt\n",
      "Loaded cv494_18689.txt\n",
      "Loaded cv843_17054.txt\n",
      "Loaded cv906_12332.txt\n",
      "Loaded cv331_8656.txt\n",
      "Loaded cv591_24887.txt\n",
      "Loaded cv021_17313.txt\n",
      "Loaded cv832_24713.txt\n",
      "Loaded cv919_18155.txt\n",
      "Loaded cv090_0049.txt\n",
      "Loaded cv182_7791.txt\n",
      "Loaded cv760_8977.txt\n",
      "Loaded cv395_11761.txt\n",
      "Loaded cv092_27987.txt\n",
      "Loaded cv172_12037.txt\n",
      "Loaded cv503_11196.txt\n",
      "Loaded cv873_19937.txt\n",
      "Loaded cv981_16679.txt\n",
      "Loaded cv209_28973.txt\n",
      "Loaded cv269_23018.txt\n",
      "Loaded cv432_15873.txt\n",
      "Loaded cv418_16562.txt\n",
      "Loaded cv659_21483.txt\n",
      "Loaded cv528_11669.txt\n",
      "Loaded cv293_29731.txt\n",
      "Loaded cv249_12674.txt\n",
      "Loaded cv877_29132.txt\n",
      "Loaded cv903_18981.txt\n",
      "Loaded cv818_10698.txt\n",
      "Loaded cv119_9909.txt\n",
      "Loaded cv635_0984.txt\n",
      "Loaded cv774_15488.txt\n",
      "Loaded cv478_15921.txt\n",
      "Loaded cv660_23140.txt\n",
      "Loaded cv110_27832.txt\n",
      "Loaded cv052_29318.txt\n",
      "Loaded cv314_16095.txt\n",
      "Loaded cv669_24318.txt\n",
      "Loaded cv804_11763.txt\n",
      "Loaded cv711_12687.txt\n",
      "Loaded cv524_24885.txt\n",
      "Loaded cv468_16844.txt\n",
      "Loaded cv846_29359.txt\n",
      "Loaded cv329_29293.txt\n",
      "Loaded cv677_18938.txt\n",
      "Loaded cv064_25842.txt\n",
      "Loaded cv078_16506.txt\n",
      "Loaded cv473_7869.txt\n",
      "Loaded cv328_10908.txt\n",
      "Loaded cv977_4776.txt\n",
      "Loaded cv966_28671.txt\n",
      "Loaded cv309_23737.txt\n",
      "Loaded cv076_26009.txt\n",
      "Loaded cv797_7245.txt\n",
      "Loaded cv444_9975.txt\n",
      "Loaded cv375_9932.txt\n",
      "Loaded cv911_21695.txt\n",
      "Loaded cv557_12237.txt\n",
      "Loaded cv505_12926.txt\n",
      "Loaded cv178_14380.txt\n",
      "Loaded cv186_2396.txt\n",
      "Loaded cv360_8927.txt\n",
      "Loaded cv668_18848.txt\n",
      "Loaded cv316_5972.txt\n",
      "Loaded cv323_29633.txt\n",
      "Loaded cv696_29619.txt\n",
      "Loaded cv449_9126.txt\n",
      "Loaded cv124_3903.txt\n",
      "Loaded cv396_19127.txt\n",
      "Loaded cv100_12406.txt\n",
      "Loaded cv156_11119.txt\n",
      "Loaded cv652_15653.txt\n",
      "Loaded cv500_10722.txt\n",
      "Loaded cv045_25077.txt\n",
      "Loaded cv383_14662.txt\n",
      "Loaded cv304_28489.txt\n",
      "Loaded cv619_13677.txt\n",
      "Loaded cv732_13092.txt\n",
      "Loaded cv261_11855.txt\n",
      "Loaded cv538_28485.txt\n",
      "Loaded cv726_4365.txt\n",
      "Loaded cv476_18402.txt\n",
      "Loaded cv616_29187.txt\n",
      "Loaded cv372_6654.txt\n",
      "Loaded cv973_10171.txt\n",
      "Loaded cv603_18885.txt\n",
      "Loaded cv817_3675.txt\n",
      "Loaded cv515_18484.txt\n",
      "Loaded cv864_3087.txt\n",
      "Loaded cv901_11934.txt\n",
      "Loaded cv070_13249.txt\n",
      "Loaded cv916_17034.txt\n",
      "Loaded cv751_17208.txt\n",
      "Loaded cv720_5383.txt\n",
      "Loaded cv361_28738.txt\n",
      "Loaded cv196_28898.txt\n",
      "Loaded cv670_2666.txt\n",
      "Loaded cv517_20616.txt\n",
      "Loaded cv343_10906.txt\n",
      "Loaded cv621_15984.txt\n",
      "Loaded cv333_9443.txt\n",
      "Loaded cv793_15235.txt\n",
      "Loaded cv848_10061.txt\n",
      "Loaded cv041_22364.txt\n",
      "Loaded cv367_24065.txt\n",
      "Loaded cv509_17354.txt\n",
      "Loaded cv133_18065.txt\n",
      "Loaded cv130_18521.txt\n",
      "Loaded cv242_11354.txt\n",
      "Loaded cv724_15265.txt\n",
      "Loaded cv285_18186.txt\n",
      "Loaded cv132_5423.txt\n",
      "Loaded cv381_21673.txt\n",
      "Loaded cv403_6721.txt\n",
      "Loaded cv452_5179.txt\n",
      "Loaded cv284_20530.txt\n",
      "Loaded cv830_5778.txt\n",
      "Loaded cv138_13903.txt\n",
      "Loaded cv183_19826.txt\n",
      "Loaded cv863_7912.txt\n",
      "Loaded cv431_7538.txt\n",
      "Loaded cv924_29397.txt\n",
      "Loaded cv779_18989.txt\n",
      "Loaded cv250_26462.txt\n",
      "Loaded cv434_5641.txt\n",
      "Loaded cv593_11931.txt\n",
      "Loaded cv994_13229.txt\n",
      "Loaded cv197_29271.txt\n",
      "Loaded cv661_25780.txt\n",
      "Loaded cv513_7236.txt\n",
      "Loaded cv175_7375.txt\n",
      "Loaded cv407_23928.txt\n",
      "Loaded cv968_25413.txt\n",
      "Loaded cv413_7893.txt\n",
      "Loaded cv466_20092.txt\n",
      "Loaded cv928_9478.txt\n",
      "Loaded cv062_24556.txt\n",
      "Loaded cv874_12182.txt\n",
      "Loaded cv005_29357.txt\n",
      "Loaded cv653_2107.txt\n",
      "Loaded cv273_28961.txt\n",
      "Loaded cv565_29403.txt\n",
      "Loaded cv443_22367.txt\n",
      "Loaded cv412_25254.txt\n",
      "Loaded cv855_22134.txt\n",
      "Loaded cv882_10042.txt\n",
      "Loaded cv002_17424.txt\n",
      "Loaded cv481_7930.txt\n",
      "Loaded cv191_29539.txt\n",
      "Loaded cv235_10704.txt\n",
      "Loaded cv000_29416.txt\n",
      "Loaded cv086_19488.txt\n",
      "Loaded cv067_21192.txt\n",
      "Loaded cv569_26750.txt\n",
      "Loaded cv821_29283.txt\n",
      "Loaded cv091_7899.txt\n",
      "Loaded cv486_9788.txt\n",
      "Loaded cv255_15267.txt\n",
      "Loaded cv159_29374.txt\n",
      "Loaded cv085_15286.txt\n",
      "Loaded cv081_18241.txt\n",
      "Loaded cv012_29411.txt\n",
      "Loaded cv252_24974.txt\n"
     ]
    }
   ],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6SlpyplReXd"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dQ4Ol0plReXe"
   },
   "source": [
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ngOZpkTQReXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bf82-7OFReXm"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tU3WDGsMReXn"
   },
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8LbJ8xKIReXo"
   },
   "source": [
    "In this section, we will look at cleaning and extracting features from the movie review data. In machine learning, a feature is a measurable piece of data that can be used for analysis. For sentiment analysis, we can extract unigrams (bag of words) as features to predict the sentiment polarity. Since models can only understand numbers rather than words, we also need to convert words into some form of numerical values, which we will delve into the details later. \n",
    "\n",
    "We will start from splitting the text to extract unigrams. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0rSLnjtReXp"
   },
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qJxTridReXq"
   },
   "source": [
    "First, let’s load one document and look at the raw tokens split by white space. We will use the load_doc() function developed in the previous section. We can use the split() function to split the loaded document into tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3eSu9nvReXr",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', \"what's\", 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind-fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', \"didn't\", 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', \"it's\", 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', \"what's\", 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', \"don't\", 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', \"film's\", 'biggest', 'problem', '.', \"it's\", 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half-way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', \"didn't\", 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', \"don't\", 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', \"might've\", 'been', 'a', 'pretty', 'decent', 'teen', 'mind-fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', \"character's\", 'unraveling', '.', 'overall', ',', 'the', 'film', \"doesn't\", 'stick', 'because', 'it', \"doesn't\", 'entertain', ',', \"it's\", 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', \"it's\", 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', \"where's\", 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7/10', ')', '-', 'blair', 'witch', '2', '(', '7/10', ')', '-', 'the', 'crow', '(', '9/10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4/10', ')', '-', 'lost', 'highway', '(', '10/10', ')', '-', 'memento', '(', '10/10', ')', '-', 'the', 'others', '(', '9/10', ')', '-', 'stir', 'of', 'echoes', '(', '8/10', ')']\n"
     ]
    }
   ],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gD30qS3jReXx"
   },
   "source": [
    "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
    "\n",
    "- Removing tokens that are just punctuation (e.g. ‘-‘).\n",
    "\n",
    "- Removing tokens that contain numbers (e.g. ’10/10′).\n",
    "\n",
    "- Remove tokens that don’t have much meaning (e.g. ‘and’)\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- We can remove tokens that are just punctuation or contain numbers by using an `isalpha()` function to check on each token.\n",
    "- We can remove English stop words using the list loaded using `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dKAO0iwPReXx"
   },
   "source": [
    "We add the above preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGGrrkNzReX0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: tqdm in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: regex in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: click in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "Requirement already satisfied: joblib in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "/Users/liuqianchu/miniconda3/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/liuqianchu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# install nltk using pip in the jupyter notebook\n",
    "# and download the stopword lists\n",
    "!pip install nltk\n",
    "!python -m nltk.downloader stopwords \n",
    "\n",
    "# import the stopwods\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wy_YRg6csEg2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'pretty', 'decent', 'teen', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "# remove tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeYaaXFDReX5"
   },
   "source": [
    "As we are mainly interested in the frequency and presence of each unigram, we can store the unigram features using the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary of unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AVTG7u5FReX9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'movie': 6, 'pretty': 5, 'film': 5, 'make': 5, 'teen': 4, 'get': 3, 'one': 3, 'even': 3, 'like': 3, 'two': 2, 'go': 2, 'see': 2, 'cool': 2, 'idea': 2, 'bad': 2, 'since': 2, 'films': 2, 'lost': 2, 'highway': 2, 'memento': 2, 'good': 2, 'problem': 2, 'simply': 2, 'world': 2, 'audience': 2, 'going': 2, 'coming': 2, 'dead': 2, 'others': 2, 'look': 2, 'scenes': 2, 'things': 2, 'biggest': 2, 'secret': 2, 'hide': 2, 'minutes': 2, 'entertaining': 2, 'really': 2, 'part': 2, 'actually': 2, 'strangeness': 2, 'little': 2, 'sense': 2, 'still': 2, 'guess': 2, 'sagemiller': 2, 'away': 2, 'throughout': 2, 'apparently': 2, 'way': 2, 'crow': 2, 'plot': 1, 'couples': 1, 'church': 1, 'party': 1, 'drink': 1, 'drive': 1, 'accident': 1, 'guys': 1, 'dies': 1, 'girlfriend': 1, 'continues': 1, 'life': 1, 'nightmares': 1, 'deal': 1, 'watch': 1, 'sorta': 1, 'find': 1, 'critique': 1, 'generation': 1, 'touches': 1, 'presents': 1, 'package': 1, 'makes': 1, 'review': 1, 'harder': 1, 'write': 1, 'generally': 1, 'applaud': 1, 'attempt': 1, 'break': 1, 'mold': 1, 'mess': 1, 'head': 1, 'ways': 1, 'making': 1, 'types': 1, 'folks': 1, 'snag': 1, 'correctly': 1, 'seem': 1, 'taken': 1, 'neat': 1, 'concept': 1, 'executed': 1, 'terribly': 1, 'problems': 1, 'well': 1, 'main': 1, 'jumbled': 1, 'starts': 1, 'normal': 1, 'downshifts': 1, 'fantasy': 1, 'member': 1, 'dreams': 1, 'characters': 1, 'back': 1, 'strange': 1, 'apparitions': 1, 'disappearances': 1, 'looooot': 1, 'chase': 1, 'tons': 1, 'weird': 1, 'happen': 1, 'explained': 1, 'personally': 1, 'mind': 1, 'trying': 1, 'unravel': 1, 'every': 1, 'give': 1, 'clue': 1, 'kind': 1, 'fed': 1, 'obviously': 1, 'got': 1, 'big': 1, 'seems': 1, 'want': 1, 'completely': 1, 'final': 1, 'five': 1, 'thrilling': 1, 'engaging': 1, 'meantime': 1, 'sad': 1, 'arrow': 1, 'dig': 1, 'flicks': 1, 'figured': 1, 'point': 1, 'start': 1, 'bit': 1, 'bottom': 1, 'line': 1, 'movies': 1, 'always': 1, 'sure': 1, 'given': 1, 'password': 1, 'enter': 1, 'understanding': 1, 'mean': 1, 'showing': 1, 'melissa': 1, 'running': 1, 'visions': 1, 'plain': 1, 'lazy': 1, 'okay': 1, 'people': 1, 'chasing': 1, 'know': 1, 'need': 1, 'giving': 1, 'us': 1, 'different': 1, 'offering': 1, 'insight': 1, 'studio': 1, 'took': 1, 'director': 1, 'chopped': 1, 'shows': 1, 'decent': 1, 'somewhere': 1, 'suits': 1, 'decided': 1, 'turning': 1, 'music': 1, 'video': 1, 'edge': 1, 'would': 1, 'actors': 1, 'although': 1, 'wes': 1, 'bentley': 1, 'seemed': 1, 'playing': 1, 'exact': 1, 'character': 1, 'american': 1, 'beauty': 1, 'new': 1, 'neighborhood': 1, 'kudos': 1, 'holds': 1, 'entire': 1, 'feeling': 1, 'unraveling': 1, 'overall': 1, 'stick': 1, 'entertain': 1, 'confusing': 1, 'rarely': 1, 'excites': 1, 'feels': 1, 'redundant': 1, 'runtime': 1, 'despite': 1, 'ending': 1, 'explanation': 1, 'craziness': 1, 'came': 1, 'oh': 1, 'horror': 1, 'slasher': 1, 'flick': 1, 'packaged': 1, 'someone': 1, 'assuming': 1, 'genre': 1, 'hot': 1, 'kids': 1, 'also': 1, 'wrapped': 1, 'production': 1, 'years': 1, 'ago': 1, 'sitting': 1, 'shelves': 1, 'ever': 1, 'whatever': 1, 'skip': 1, 'joblo': 1, 'nightmare': 1, 'elm': 1, 'street': 1, 'blair': 1, 'witch': 1, 'salvation': 1, 'stir': 1, 'echoes': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TuCta1E-ReYC"
   },
   "source": [
    "We can put this into a function called `clean_doc_unigrams()` and test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6UgUxOZReYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal', 'watch', 'movie', 'sorta', 'find', 'critique', 'movie', 'teen', 'generation', 'touches', 'cool', 'idea', 'presents', 'bad', 'package', 'makes', 'review', 'even', 'harder', 'one', 'write', 'since', 'generally', 'applaud', 'films', 'attempt', 'break', 'mold', 'mess', 'head', 'lost', 'highway', 'memento', 'good', 'bad', 'ways', 'making', 'types', 'films', 'folks', 'snag', 'one', 'correctly', 'seem', 'taken', 'pretty', 'neat', 'concept', 'executed', 'terribly', 'problems', 'movie', 'well', 'main', 'problem', 'simply', 'jumbled', 'starts', 'normal', 'downshifts', 'fantasy', 'world', 'audience', 'member', 'idea', 'going', 'dreams', 'characters', 'coming', 'back', 'dead', 'others', 'look', 'like', 'dead', 'strange', 'apparitions', 'disappearances', 'looooot', 'chase', 'scenes', 'tons', 'weird', 'things', 'happen', 'simply', 'explained', 'personally', 'mind', 'trying', 'unravel', 'film', 'every', 'give', 'clue', 'get', 'kind', 'fed', 'biggest', 'problem', 'obviously', 'got', 'big', 'secret', 'hide', 'seems', 'want', 'hide', 'completely', 'final', 'five', 'minutes', 'make', 'things', 'entertaining', 'thrilling', 'even', 'engaging', 'meantime', 'really', 'sad', 'part', 'arrow', 'dig', 'flicks', 'like', 'actually', 'figured', 'point', 'strangeness', 'start', 'make', 'little', 'bit', 'sense', 'still', 'make', 'film', 'entertaining', 'guess', 'bottom', 'line', 'movies', 'like', 'always', 'make', 'sure', 'audience', 'even', 'given', 'secret', 'password', 'enter', 'world', 'understanding', 'mean', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'visions', 'minutes', 'throughout', 'movie', 'plain', 'lazy', 'okay', 'get', 'people', 'chasing', 'know', 'really', 'need', 'see', 'giving', 'us', 'different', 'scenes', 'offering', 'insight', 'strangeness', 'going', 'movie', 'apparently', 'studio', 'took', 'film', 'away', 'director', 'chopped', 'shows', 'pretty', 'decent', 'teen', 'movie', 'somewhere', 'guess', 'suits', 'decided', 'turning', 'music', 'video', 'little', 'edge', 'would', 'make', 'sense', 'actors', 'pretty', 'good', 'part', 'although', 'wes', 'bentley', 'seemed', 'playing', 'exact', 'character', 'american', 'beauty', 'new', 'neighborhood', 'biggest', 'kudos', 'go', 'sagemiller', 'holds', 'throughout', 'entire', 'film', 'actually', 'feeling', 'unraveling', 'overall', 'film', 'stick', 'entertain', 'confusing', 'rarely', 'excites', 'feels', 'pretty', 'redundant', 'runtime', 'despite', 'pretty', 'cool', 'ending', 'explanation', 'craziness', 'came', 'oh', 'way', 'horror', 'teen', 'slasher', 'flick', 'packaged', 'look', 'way', 'someone', 'apparently', 'assuming', 'genre', 'still', 'hot', 'kids', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'sitting', 'shelves', 'ever', 'since', 'whatever', 'skip', 'joblo', 'coming', 'nightmare', 'elm', 'street', 'blair', 'witch', 'crow', 'crow', 'salvation', 'lost', 'highway', 'memento', 'others', 'stir', 'echoes']\n"
     ]
    }
   ],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9__t3RWKReYI"
   },
   "source": [
    "Finally, we can add the above preprocessing steps into a function `process_docs_unigram()` to process all the files in a directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA4RU1KRReYK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n"
     ]
    }
   ],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "process_docs_unigrams('./data/neg')\n",
    "process_docs_unigrams('./data/pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlQMfCX6ReYQ"
   },
   "source": [
    "### Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb62eNVSReYR"
   },
   "source": [
    "To prepare the features as input to the model, we need to convert each word token into a numerical value so that the model can process them. To represent unigrams, we can create a vector (ie. a list of numbers) with the dimension size of the vocabulay, and the value in each dimension stores the frequency or presence of a specific word. \n",
    "\n",
    "\n",
    "For example, suppose we have five words (apple,banana,red,dog,is) in the vocabulary which are represented as five dimensions in the feature vecotors. We also have a document (document 1) containing the following words: \n",
    "\n",
    "document 1: \"apple is red\"\n",
    "\n",
    "We add '1' for the words present and '0' for words not present in the document, and create the following: \n",
    "\n",
    "|document no.|apple|banana|red|dog|is\n",
    "|------|------|------|------|------|------|\n",
    "|document 1 |1|0|1|0|1|\n",
    "\n",
    "We thus can represent the document as a feature vector [1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPWBwd2jReYS"
   },
   "source": [
    "The first step now is to create a mapping between dimension index of the vector and the word in the vocabulary. Let's first loop over the documents to collect all the words. Here, we use a dictionary that updates the words and their counts while we process all the documents. To do this, we can define a function `update_counter()` to collect the words and their counts from each token dictioanry of each document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1yXSKWDaReYU"
   },
   "outputs": [],
   "source": [
    "def update_counter(overall_vocab_counter,token_dict):\n",
    "    for w in token_dict:\n",
    "        overall_vocab_counter[w]+=token_dict[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rjQ0xS0RReYY"
   },
   "source": [
    "We can now define an `overall_vocab_counter` and then integrate the `update_counter()` function into `process_docs_unigrams()` to update `overall_vocab_counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rw30VUGkReYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "overall_vocab_counter=Counter()\n",
    "def process_docs_unigrams(directory,overall_vocab_counter):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "        update_counter(overall_vocab_counter,tokens_dict_current)\n",
    "process_docs_unigrams('./data/neg',overall_vocab_counter)\n",
    "process_docs_unigrams('./data/pos',overall_vocab_counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "Let's check the compiled `overall_vocab_counter`, how many words are there in total after preprocessing? And what are the top 5 most frequent words?\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>There are 37607 words in total</p>\n",
    "    <p>The 5 most frequent words are: \n",
    "        ('film', 8849),\n",
    " ('one', 5514),\n",
    " ('movie', 5429),\n",
    " ('like', 3543),\n",
    " ('even', 2554),</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kXVnFnpReYp"
   },
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor dimension index from this `overall_vocab_counter`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IX4zO6tGReYq"
   },
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(overall_vocab_counter):\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(overall_vocab_counter.keys()): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "vocab2index,index2vocab=create_vocab_feature_mappings(overall_vocab_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRuZAgH3ReYv"
   },
   "source": [
    "Now, let's turn each document into a unigram feature vector. We can either represent in each dimension cell the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "\n",
    "\n",
    "Let's design two functions `create_feature_presence()` and `create_feature_frequency()`  to turn the token dictionary for each document into these two types of features. \n",
    "\n",
    "To create the `create_feature_presence()`, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVKBHpEqReYw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_feature_presence(tokens_dict_current,vocab2index):\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in tokens_dict_current:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd8XWczOReY1"
   },
   "source": [
    "### Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AQhMs1Q1ReY1"
   },
   "source": [
    "Notice that we have created `numpy` arrays here to represent feature vectors.  A `numpy` array is just like a `list` but with smaller memory and faster access. \n",
    "\n",
    "Below, we introduce several ways to create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LcbIondQReY2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1 [0. 0.]\n",
      "vector2 [1 2 3]\n",
      "vector3 [3. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Create a numpy array of zeros with dimension 2\n",
    "vector1=np.zeros(2)\n",
    "# create a numpy array from a list [1,2,3]\n",
    "vector2=np.array([1,2,3])\n",
    "# create an empty array of dimension 3 with arbitary data\n",
    "vector3=np.empty(3)\n",
    "print ('vector1',vector1)\n",
    "print ('vector2',vector2)\n",
    "print ('vector3',vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTzXgRTuReY6"
   },
   "source": [
    "We can also concatenate two numpy arrays of the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vSNpS0TJReY7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3., 3., 2., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((vector2,vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq3Do1UxReZE"
   },
   "source": [
    "So far, we have created numpy arrays of one dimension. Let's try creating a 2-D array (also called a matrix). We can pass dimension size (also called axes) as (a,b) where a is the number of rows in the matrix, and b specifies the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TzQII4NNReZF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix1 [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "matrix2 [[1 2 3]\n",
      " [2 3 4]]\n",
      "matrix3 []\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "matrix1=np.zeros((3,3)) \n",
    "# this is a matrix of zeros that has 3 vectors, and within each vector there are 4 items. \n",
    "print ('matrix1',matrix1)\n",
    "# a matrix from nested list\n",
    "matrix2=np.array([[1,2,3],[2,3,4]])\n",
    "print ('matrix2',matrix2)\n",
    "# an empty matrix usually used as initialisation. It will print as an empty list\n",
    "matrix3=np.empty((0,4))\n",
    "print ('matrix3',matrix3)\n",
    "#To check the axes of an array, you can retrieve the shape attribute like this:\n",
    "print (matrix2.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJI7IUkMReZJ"
   },
   "source": [
    "Numpy arrays are mutable. Therefore, we can change values in the vector. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ky4YPO0ReZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3]\n",
      "[[1 2 0]\n",
      " [2 3 4]]\n",
      "[[0 2 3]\n",
      " [2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "vector2[0]=0 # change the first item in vector3 to 0\n",
    "print (vector2)\n",
    "matrix2[0][2]=0 # change the third item of the first vector to 0\n",
    "print (matrix2)\n",
    "matrix2[0]=vector2 # change the first vector in matrix2 to vector3\n",
    "print (matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjSFnyhwReZP"
   },
   "source": [
    "We can also slice a numpy array with an index array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r9u1f_TReZS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's select the values at index 1,2 of vector2\n",
    "vector2[[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z1gqUZ4XReZa"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWuzZ5KpReZb"
   },
   "source": [
    "Can you try implementing the function`create_feature_frequency()` to extract unigram frequency? (You can modify on the basis of `create_feature_presence()`)\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yK92gQ75ReZc"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency(tokens_dict_current,vocab2index):\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in tokens_dict_current:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=tokens_dict_current[w]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U3W3TtcJReZh"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>simple change the line vector[index]=1 from create_feature_presence() to vector[index]=token_dict_current[w]</p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDhe6PPjReZi"
   },
   "source": [
    "Now let's create another function `process_docs_unigrams_presence()` on the basis of `process_docs_unigrams()` to loop over the documents again and convert the token dictionary in each document to feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSB84BTTReZj"
   },
   "outputs": [],
   "source": [
    "def process_docs_unigrams_presence(directory,vocab2index):\n",
    "    # loop over the directory to extract filenames that have the right extension\n",
    "    filenames=[filename for filename in listdir(directory) if filename.endswith(\".txt\")]\n",
    "    # since we know how many files we will process, and the vocabulary size, we can initialize our result matrix as an empty array with the shape of (file number, vocabulary size)\n",
    "    \n",
    "    unigram_presence_result=np.empty((len(filenames),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for file_i,filename in enumerate(filenames):\n",
    "        \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "        # convert bag of words in each document into unigram features\n",
    "        unigram_presence=create_feature_presence(tokens_dict_current,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        # we append the unigram_feature in each document to the unigram_feature_result array and update the array. \n",
    "        # to ensure the correct dimension size, we nest the unigram_feature vector so that it becomes a 2-d array as the initialization in unigram_feature_result\n",
    "        unigram_presence_result[file_i]=unigram_presence\n",
    "    return unigram_presence_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z4WeZ1bReZm"
   },
   "source": [
    "Now we are ready to extract features from both the negative and positive directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xC6L5sQMReZm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/pos\n",
      "./data/neg\n"
     ]
    }
   ],
   "source": [
    "unigram_presence_positive=process_docs_unigrams_presence('./data/pos',vocab2index)\n",
    "unigram_presence_negative=process_docs_unigrams_presence('./data/neg',vocab2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDsoUC8hReZq"
   },
   "source": [
    "Let's take a look of the feature representation for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ptzvx4twReZr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_presence_negative[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUwcH3xkReZv"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_hJeToEReZw"
   },
   "source": [
    "Can you follow the codes above to extract unigrams' frequency features using the `create_feature_frequency()` functions you defined in quiz 3? Please store the features into `unigram_frequency_positive` and `unigram_frequency_negative` for positive and negative directories respectively. \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6g6uwOwSReZx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kvb5s1w-ReZ2"
   },
   "source": [
    "### ❓ Quiz  \n",
    "\n",
    "Let's check the first positive review's unigram presence feature, can you use the mapping in `index2vocab` to reveal what unigrams are present in this review? \n",
    "Please answer: Which words in the following list are present?\n",
    "A. gayness\n",
    "B. fabulous\n",
    "C. snappiness\n",
    "D. happiness\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khPgk5R1ReZ3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[index2vocab[i] for i,item in enumerate(unigram_presence_positive[0]) if item==1]\n",
    "'happiness' in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nU52Lmt7ReZ6"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p> Code:</p>\n",
    "    <p>wordlist=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]</p>\n",
    "    <p> A,C are present in the review </p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGNbpwvAReZ7"
   },
   "source": [
    "### Save the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ntPhUTCrReZ7"
   },
   "source": [
    "We can also save the prepared features for the reviews ready for modeling.\n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas.\n",
    "\n",
    "To store a numpy array, we can use the `numpy`'s `save()` function. `save()` takes two arguments: the first is an the filename to be written, and the second argument is the numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tE3m4vnpReZ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('unigram_presence_positive.npy',unigram_presence_positive)\n",
    "np.save('unigram_presence_negative.npy',unigram_presence_negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gL_Qwr_1ReaC"
   },
   "source": [
    "We can load the postiive data from the files by:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBEc1d1DReaC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load('unigram_presence_positive.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nhzv2aDmReaG"
   },
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fOlUBWYReaH"
   },
   "source": [
    "Based on `clean_doc_unigrams()` that turns a document into a unigram dictionary, we can create a function `clean_doc_bigrams()` to extract bigram dictionary. (You can refresh yourself of how to create a bigram counter dictionary in module 1.4. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hehFhHqgReaI"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_bigrams(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # extract bigrams\n",
    "    bigram_dict=Counter() #initialize a bigram dictionary to be updated\n",
    "    tokens=['<start>']+tokens+['<end>'] # add <start> and <end> token\n",
    "    for i in range(len(tokens)): #loop over all the indices of the token list\n",
    "        if i<len(tokens)-1: #if it's not the end of the token list\n",
    "            bigram_current=(tokens[i],tokens[i+1])\n",
    "            bigram_dict[bigram_current]+=1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzesiKPtReaK"
   },
   "source": [
    "Now we can replace the `clean_doc_unigrams()` line with `clean_doc_bigrams()` in `process_docs_unigrams()`, we will rename the function as `process_docs_bigrams()` that counts all the bigrams in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpo6ay2bReaL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/neg\n",
      "./data/pos\n"
     ]
    }
   ],
   "source": [
    "overall_bigrams_counter=Counter()\n",
    "def process_docs_bigrams(directory,overall_bigrams_counter):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_bigrams(text)\n",
    "        update_counter(overall_bigrams_counter,tokens_dict_current)\n",
    "process_docs_bigrams('./data/neg',overall_bigrams_counter)\n",
    "process_docs_bigrams('./data/pos',overall_bigrams_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1KBMkOFReaN"
   },
   "source": [
    "Once we have `overall_bigrams_counter`, we can pass it to `create_vocab_feature_mappings()` to create index-bigram mappings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AINegqGReaO"
   },
   "outputs": [],
   "source": [
    "bigram2index,index2bigram=create_vocab_feature_mappings(overall_bigrams_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LC7xA5tbReaR"
   },
   "source": [
    "Now we can modify `process_docs_unigrams_presence()` to process bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xBVnhPLyReaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/pos\n",
      "./data/neg\n"
     ]
    }
   ],
   "source": [
    "def process_docs_bigrams_presence(directory,bigram2index):\n",
    "    # loop over the directory to extract filenames that have the right extension\n",
    "    filenames=[filename for filename in listdir(directory) if filename.endswith(\".txt\")]\n",
    "    # since we know how many files we will process, and the vocabulary size, we can initialize our result matrix as an empty array with the shape of (file number, vocabulary size)\n",
    "    \n",
    "    bigram_presence_result=np.empty((len(filenames),len(bigram2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for file_i,filename in enumerate(filenames):\n",
    "        \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        bigrams_dict_current = clean_doc_bigrams(text)\n",
    "        # convert bag of words in each document into unigram features\n",
    "        bigram_presence=create_feature_presence(bigrams_dict_current,bigram2index)\n",
    "        # We assign the bigram fearture of the current document to the correct positon in the bigram_presence_result matrix. \n",
    "        # we append the bigram_feature in each document to the bigram_feature_result array and update the array. \n",
    "        # to ensure the correct dimension size, we nest the unigram_feature vector so that it becomes a 2-d array as the initialization in unigram_feature_result\n",
    "        bigram_presence_result[file_i]=bigram_presence\n",
    "    return bigram_presence_result\n",
    "bigram_presence_positive=process_docs_bigrams_presence('./data/pos',bigram2index)\n",
    "bigram_presence_negative=process_docs_bigrams_presence('./data/neg',bigram2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pa-ncVNQReaV"
   },
   "outputs": [],
   "source": [
    "# We then save the bigram features:\n",
    "np.save('bigram_presence_positive.npy',bigram_presence_positive)\n",
    "np.save('bigram_presence_negative.npy',bigram_presence_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2W8h1ByjReaY"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r8ADZKR1ReaZ"
   },
   "source": [
    "Now we have features in the form of bigrams. Now what does each dimension represent for a vector now? How many dimensions do we have for each vector? You can inspect `bigram_presence_positive` to answer these questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHhMYkawReaa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VgTk8uTReac"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Each dimension corresponds to a bigram. There are 463119 bigrams and therfore the dimension size of each vector is 463119. </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8bO5CiDRead"
   },
   "source": [
    "other features afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qH_mYfLrReae"
   },
   "source": [
    "#### Unigrams + POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glZAvmyZReag"
   },
   "source": [
    "#### Adjective Unigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qkkftelGReag"
   },
   "source": [
    "#### Unigrams above certain frequency threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RXq0GHvvReah"
   },
   "source": [
    "#### Unigrams + Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUQ_-39-Reah"
   },
   "source": [
    "#### Unigrams + Bigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d4AsqDygReaj"
   },
   "source": [
    "Negation (extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgQWG19jReak"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD_SGP7aReak"
   },
   "source": [
    "### Prepare train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuHpK9tfReak"
   },
   "source": [
    "Now it's time to prepare this dataset for model evlauation. We want to train a model that is generalisable to unseen data. Therefore, we can split the dataset into train and test where the model is trained on the train set and tested on the unseen test set. Usually the train-test split is 70% and 30%. Let's make a random split of 70%/30% from our data. We will also ensure that we have an equal number of positibe and negative examples by making the split in both positive and negative datasets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gpmPH4YkReal"
   },
   "source": [
    "Let's first make train test split for the positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FTBm3plTReal"
   },
   "outputs": [],
   "source": [
    "\n",
    "# make the split from positive data\n",
    "# get the indexes for the positive data\n",
    "positive_indices=list(range(len(unigram_presence_positive)))\n",
    "# randomly shuffle the indices\n",
    "import random\n",
    "random.shuffle(positive_indices)\n",
    "# get the training and test data indices\n",
    "train_positive_indices=positive_indices[:int(len(positive_indices)*(0.7))]\n",
    "test_positive_indices=positive_indices[int(len(positive_indices)*(0.7)):]\n",
    "# now let's use the indices to extract train and test features\n",
    "unigram_presence_positive_train=unigram_presence_positive[train_positive_indices]\n",
    "unigram_presence_positive_test=unigram_presence_positive[test_positive_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLvkcWHKReao"
   },
   "source": [
    "We can wrap up the above into a function that takes into feature data and returns train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MctvBQyhReaq"
   },
   "outputs": [],
   "source": [
    "def train_test_split(features,train_percentage=0.7):\n",
    "    # get the indexes for the feature data\n",
    "    indices=list(range(len(features)))\n",
    "    # randomly shuffle the indices\n",
    "    random.shuffle(indices)\n",
    "    # get the training and test data indices\n",
    "    train_indices=indices[:int(len(indices)*(train_percentage))]\n",
    "    test_indices=indices[int(len(indices)*(train_percentage)):]\n",
    "    # now let's use the indices to extract train and test features\n",
    "    fearture_train=features[train_indices]\n",
    "    feature_test=features[test_indices]\n",
    "    return fearture_train,feature_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE9LgwUDReat"
   },
   "source": [
    "Let's apply the function `train_test_split()` to negative unigram presence features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFs1xqunReat"
   },
   "outputs": [],
   "source": [
    "positive_train,positive_test=train_test_split(unigram_presence_positive)\n",
    "negative_train,negative_test=train_test_split(unigram_presence_negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NpsdoJRReay"
   },
   "source": [
    "Now we can create the train and test sets for all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8gmS1PgReaz"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data=np.concatenate((positive_train,negative_train))\n",
    "test_data=np.concatenate((positive_test,negative_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AEDH7-WERea3"
   },
   "source": [
    "at the same time, we should also find a way to represent the labels (positive or negative) in numeric ways for the model to train on. Let's create numpy arrays of labels of 1s and 0s. Let's say 1 corresponds to positive reviews and 0 corresponds to negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K12juH1zRea4"
   },
   "outputs": [],
   "source": [
    " \n",
    "train_label=len(positive_train)*[1]+len(negative_train)*[0]\n",
    "test_label=len(positive_test)*[1]+len(negative_test)*[0]\n",
    "train_label=np.array(train_label)\n",
    "test_label=np.array(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "58PX-s5tRebM"
   },
   "source": [
    "Let's wrap up the above into a function that takes both positive and negative feature data, and split into train and test data and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "69Lzjq0_RebM"
   },
   "outputs": [],
   "source": [
    "def produce_data_label_splits(positive_feature,negative_feature):\n",
    "    #create train and test splits on positive and negative data's features\n",
    "    positive_train_feature,positive_test_feature=train_test_split(positive_feature)\n",
    "    negative_train_feature,negative_test_feature=train_test_split(negative_feature)\n",
    "    #concatenate positive and negative features into the total train test data\n",
    "    train_feature=np.concatenate((positive_train_feature,negative_train_feature))\n",
    "    test_feature=np.concatenate((positive_test_feature,negative_test_feature))\n",
    "    #create label arrays\n",
    "    train_label=len(positive_train_feature)*[1]+len(negative_train_feature)*[0]\n",
    "    test_label=len(positive_test_feature)*[1]+len(negative_test_feature)*[0]\n",
    "    train_label=np.array(train_label)\n",
    "    test_label=np.array(test_label)\n",
    "    return train_feature,test_feature,train_label,test_label\n",
    "train_data,test_data,train_label,test_label=produce_data_label_splits(unigram_presence_positive,unigram_presence_negative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p2ScVYh9RebR"
   },
   "source": [
    "### Naive Bayes Model and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YnKOr56PRebS"
   },
   "source": [
    "Let's try fitting the features and labels into a naive bayes model using the `MultinomialNB` package in `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04CNQg8eRebT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Downloading https://files.pythonhosted.org/packages/d9/78/44fb6f0842e93d401040cc06db1a9787c9c16df15c8970cdc8999587a322/scikit_learn-0.23.2-cp36-cp36m-macosx_10_9_x86_64.whl (7.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.2MB 34kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/liuqianchu/miniconda3/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Collecting scipy>=0.19.1 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/55/9c/b17c492bc3141d679e4bda9d40b348d438a0a81f5be4866552d552145901/scipy-1.5.3-cp36-cp36m-macosx_10_9_x86_64.whl (28.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 28.8MB 10kB/s  eta 0:00:01                          | 4.6MB 21.2MB/s eta 0:00:02    35% |███████████▍                    | 10.3MB 9.5MB/s eta 0:00:02    39% |████████████▊                   | 11.4MB 633kB/s eta 0:00:28    41% |█████████████▎                  | 11.9MB 4.2MB/s eta 0:00:05    49% |███████████████▉                | 14.3MB 13.9MB/s eta 0:00:02    52% |████████████████▊               | 15.0MB 13.1MB/s eta 0:00:02    60% |███████████████████▍            | 17.4MB 7.2MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Building wheels for collected packages: sklearn\n",
      "  Running setup.py bdist_wheel for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/liuqianchu/Library/Caches/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: scipy, threadpoolctl, scikit-learn, sklearn\n",
      "Successfully installed scikit-learn-0.23.2 scipy-1.5.3 sklearn-0.0 threadpoolctl-2.1.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# initialize a multinomial naive bayes model\n",
    "model = MultinomialNB()\n",
    "# fit the model with features and labels for the training data\n",
    "model.fit(train_data,train_label)\n",
    "#evaluate the fitted model on the test set\n",
    "predicted=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DHgKdlJRebW"
   },
   "source": [
    "`predicted` is now an array of predictions in 1s and 0s. Let's compare it with the gold labels and calculate accuracy following:\n",
    "\n",
    "accruracy= number of correct examples/number of total examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfCvRGRhRebW"
   },
   "source": [
    "Let's loop over the examples to count correct ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5UMpbl6RebX",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8133333333333334\n"
     ]
    }
   ],
   "source": [
    "correct=0\n",
    "for i in range(len(predicted)):\n",
    "    if predicted[i]==test_label[i]: #if the predicted result is the same with the gold label\n",
    "        correct+=1\n",
    "acc=correct/len(predicted)\n",
    "\n",
    "print ('accuracy',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXXAgwQCRebc"
   },
   "source": [
    "Let's wrap up above to create a function that calls the naive bayes model and performs evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L4SstOdiRebc"
   },
   "outputs": [],
   "source": [
    "def evaluate(train_data,train_label,test_data,test_label):\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(train_data,train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(test_data)\n",
    "    #evaluation:\n",
    "    correct=0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i]==test_label[i]: #if the predicted result is the same with the gold label\n",
    "            correct+=1\n",
    "    acc=correct/len(predicted)\n",
    "\n",
    "\n",
    "    print ('accuracy',acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3CFSUKl5Rebf"
   },
   "source": [
    "To run train test preparation and model evaluation, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxNffzRuRebg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.8533333333333334\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data,train_label,test_label=produce_data_label_splits(unigram_presence_positive,unigram_presence_negative)\n",
    "acc=evaluate(train_data,train_label,test_data,test_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaNOVarMRebj"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9cM44j5Rebj"
   },
   "source": [
    "Everytime we run `produce_data_label_splits()`, we are making a new random split of the dataset. Try run the function on unigram presence features 10 times, and evaluate with `evaluate()` to report the average accuracy results with standard deviation. Does the result change a lot? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5u_sq0zARebj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.825\n",
      "accuracy 0.8183333333333334\n",
      "accuracy 0.8383333333333334\n",
      "accuracy 0.85\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-eab0963cc9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproduce_data_label_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_presence_positive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munigram_presence_negative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-f2ddf881bab5>\u001b[0m in \u001b[0;36mproduce_data_label_splits\u001b[0;34m(positive_feature, negative_feature)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#concatenate positive and negative features into the total train test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_train_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative_train_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtest_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_test_feature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative_test_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#create label arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_train_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_train_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "accs=[]\n",
    "for _ in range(10):\n",
    "    train_data,test_data,train_label,test_label=produce_data_label_splits(unigram_presence_positive,unigram_presence_negative)\n",
    "    acc=evaluate(train_data,train_label,test_data,test_label)\n",
    "    accs.append(acc)\n",
    "acc_mean=np.mean(np.array(accs))\n",
    "acc_std=np.std(np.array(accs))\n",
    "print ('mean',acc_mean,'std',acc_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmNxR8BERebn"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>The following result is from 10 random runs and your result may vary a little from this:</p>\n",
    "    <p>mean: 0.8388; standard : 0.01 </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eCaLlXqgRebo"
   },
   "source": [
    "### ❓ Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbpA7tuVRebo"
   },
   "source": [
    "So far we have trained a model and evaluate on the unigram presence features (`unigram_presence_positive` and `unigram_presence_negative`), can you try to build another model on bigram presence features? (Hint, you can simply change the arguments to the `produce_data_label_splits()`)\n",
    "You can write your code below and report the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-mHntH4Rebp"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>train_data,test_data,train_label,test_label=produce_data_label_splits(bigram_presence_positive,bigram_presence_negative)</p>\n",
    " <p>acc=evaluate(train_data,train_label,test_data,test_label)</p>\n",
    "    <p>One run of the model gives 0.84. Remember that your result can be a bit different due to the random split.  </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ws8HGCQiRebp"
   },
   "source": [
    "## Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrZHH9NGRebq"
   },
   "source": [
    "## Other features"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_2.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
