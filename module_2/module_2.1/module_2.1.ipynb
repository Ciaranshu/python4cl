{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to module 2.1. In this module, we will start building and testing a sentiment classifier with Naive Bayes!\n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "## Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>Sentiment Anlaysis</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are going to focus on a popular NLP classification task: sentiment analysis. What exactly is sentiment? Sentiment relates to the meaning of a word or sequence of words and is usually associated with an opinion or emotion. And analysis? Well, this is the process of looking at data and making inferences; in this case, using machine learning to learn and predict whether a movie review is positive or negative.\n",
    "\n",
    "In this section, we will replicate the experiments from the paper: Thumbs up? Sentiment Classification using Machine Learning\n",
    "Techniques (https://www.aclweb.org/anthology/W02-1011.pdf). We will extract a number of features including unigrams, bigrams, pos tags etc., and train Naive Bayes models on these features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file “cv000_29416.txt” as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can turn this into a function called load_doc() that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first read in these positive and negative files and store them as two list of texts. To navigate the files, we can use Python's `os` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 1:\n",
    "\n",
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will look at cleaning and extracting features from the movie review data. In machine learning, a feature is a measurable piece of data that can be used for analysis. For sentiment analysis, we can extract unigrams (bag of words) as features to predict the sentiment polarity. Since models can only understand numbers rather than words, we also need to convert words into some form of numerical values, which we will delve into the details later. \n",
    "\n",
    "We will start from splitting the text to extract unigrams. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s load one document and look at the raw tokens split by white space. We will use the load_doc() function developed in the previous section. We can use the split() function to split the loaded document into tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the raw tokens can give us a lot of ideas of things to try, such as:\n",
    "\n",
    "- Removing tokens that are just punctuation (e.g. ‘-‘).\n",
    "\n",
    "- Removing tokens that contain numbers (e.g. ’10/10′).\n",
    "\n",
    "- Remove tokens that don’t have much meaning (e.g. ‘and’)\n",
    "\n",
    "Some ideas:\n",
    "\n",
    "- We can remove tokens that are just punctuation or contain numbers by using an `isalpha()` function to check on each token.\n",
    "- We can remove English stop words using the list loaded using `NLTK`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the above preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install nltk using pip in the jupyter notebook\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "# remove tokens that are not alphabetic\n",
    "tokens = [word for word in tokens if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = [w for w in tokens if not w in stop_words]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are mainly interested in the frequency and presence of each unigram, we can store the unigram features using the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary of unigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can put this into a function called `clean_doc_unigrams()` and test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add the above preprocessing steps into a function `process_docs_unigram()` to process all the files in a directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "process_docs_unigrams('./data/neg')\n",
    "process_docs_unigrams('./data/pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare the features as input to the model, we need to convert each word token into a numerical value so that the model can process them. To represent unigrams, we can create a vector (ie. a list of numbers) with the dimension size of the vocabulay, and the value in each dimension stores the frequency or presence of a specific word. \n",
    "\n",
    "\n",
    "For example, suppose we have five words (apple,banana,red,dog,is) in the vocabulary which are represented as five dimensions in the feature vecotors. We also have a document (document 1) containing the following words: \n",
    "\n",
    "document 1: \"apple is red\"\n",
    "\n",
    "We add '1' for the words present and '0' for words not present in the document, and create the following: \n",
    "\n",
    "|document no.|apple|banana|red|dog|is\n",
    "|------|------|------|------|------|------|\n",
    "|document 1 |1|0|1|0|1|\n",
    "\n",
    "We thus can represent the document as a feature vector [1,0,1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step now is to create a mapping between dimension index of the vector and the word in the vocabulary. Let's first loop over the documents to collect all the words. Here, we use a dictionary that updates the words and their counts while we process all the documents. To do this, we can define a function `update_counter()` to collect the words and their counts from each token dictioanry of each document :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_counter(overall_vocab_counter,token_dict):\n",
    "    for w in token_dict:\n",
    "        overall_vocab_counter[w]+=token_dict[w]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define an `overall_vocab_counter` and then integrate the `update_counter()` function into `process_docs_unigrams()` to update `overall_vocab_counter`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "overall_vocab_counter=Counter()\n",
    "def process_docs_unigrams(directory,overall_vocab_counter):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "        update_counter(overall_vocab_counter,tokens_dict_current)\n",
    "process_docs_unigrams('./data/neg',overall_vocab_counter)\n",
    "process_docs_unigrams('./data/pos',overall_vocab_counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 2:\n",
    "\n",
    "Let's check the compiled `overall_vocab_counter`, how many words are there in total after preprocessing? And what are the top 5 most frequent words?\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>There are 37607 words in total</p>\n",
    "    <p>The 5 most frequent words are: \n",
    "        ('film', 8849),\n",
    " ('one', 5514),\n",
    " ('movie', 5429),\n",
    " ('like', 3543),\n",
    " ('even', 2554),</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor dimension index from this `overall_vocab_counter`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(overall_vocab_counter):\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(overall_vocab_counter.keys()): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "vocab2index,index2vocab=create_vocab_feature_mappings(overall_vocab_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's turn each document into a unigram feature vector. We can either represent in each dimension cell the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "\n",
    "\n",
    "Let's design two functions `create_feature_presence()` and `create_feature_frequency()`  to turn the token dictionary for each document into these two types of features. \n",
    "\n",
    "To create the `create_feature_presence()`, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_feature_presence(tokens_dict_current,vocab2index):\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in tokens_dict_current:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have created `numpy` arrays here to represent feature vectors.  A `numpy` array is just like a `list` but with smaller memory and faster access. \n",
    "\n",
    "Below, we introduce several ways to create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array of zeros with dimension 2\n",
    "vector1=np.zeros(2)\n",
    "# create a numpy array from a list [1,2,3]\n",
    "vector2=np.array([1,2,3])\n",
    "# create an empty array of dimension 3 with arbitary data\n",
    "vector3=np.empty(3)\n",
    "print ('vector1',vector1)\n",
    "print ('vector2',vector2)\n",
    "print ('vector3',vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have created numpy arrays of one dimension. Let's try creating a 2-D array (also called a matrix). We can pass dimension size (also called axes) as (a,b) where a is the number of rows in the matrix, and b specifies the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1=np.zeros((3,3)) \n",
    "# this is a matrix of zeros that has 3 vectors, and within each vector there are 4 items. \n",
    "print ('matrix1',matrix1)\n",
    "# a matrix from nested list\n",
    "matrix2=np.array([[1,2,3],[2,3,4]])\n",
    "print ('matrix2',matrix2)\n",
    "# an empty matrix usually used as initialisation. It will print as an empty list\n",
    "matrix3=np.empty((0,4))\n",
    "print ('matrix3',matrix3)\n",
    "#To check the axes of an array, you can retrieve the shape attribute like this:\n",
    "print (matrix2.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy arrays are mutable. Therefore, we can change values in the vector. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2[0]=0 # change the first item in vector3 to 0\n",
    "print (vector2)\n",
    "matrix2[0][2]=0 # change the third item of the first vector to 0\n",
    "print (matrix2)\n",
    "matrix2[0]=vector2 # change the first vector in matrix2 to vector3\n",
    "print (matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 3:\n",
    "\n",
    "Can you try implementing the function`create_feature_frequency()`?\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_frequency(tokens_dict_current,vocab2index):\n",
    "    # create a numpy array with dimension size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in tokens_dict_current:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=tokens_dict_current[w]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>simple change the line vector[index]=1 from create_feature_presence() to vector[index]=token_dict_current[w]</p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another function `process_docs_unigrams_presence()` on the basis of `process_docs_unigrams()` to loop over the documents again and convert the token dictionary in each document to feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_unigrams_presence(directory,vocab2index):\n",
    "    # loop over the directory to extract filenames that have the right extension\n",
    "    filenames=[filename for filename in listdir(directory) if filename.endswith(\".txt\")]\n",
    "    # since we know how many files we will process, and the vocabulary size, we can initialize our result matrix as an empty array with the shape of (file number, vocabulary size)\n",
    "    \n",
    "    unigram_presence_result=np.empty((len(filenames),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for file_i,filename in enumerate(filenames):\n",
    "        \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_unigrams(text)\n",
    "        # convert bag of words in each document into unigram features\n",
    "        unigram_presence=create_feature_presence(tokens_dict_current,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        # we append the unigram_feature in each document to the unigram_feature_result array and update the array. \n",
    "        # to ensure the correct dimension size, we nest the unigram_feature vector so that it becomes a 2-d array as the initialization in unigram_feature_result\n",
    "        unigram_presence_result[file_i]=unigram_presence\n",
    "    return unigram_presence_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract features from both the negative and positive directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unigram_presence_positive=process_docs_unigrams_presence('./data/pos',vocab2index)\n",
    "unigram_presence_negative=process_docs_unigrams_presence('./data/neg',vocab2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of the feature representation for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_presence_neg[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 4:\n",
    "\n",
    "Can you follow the codes above to extract unigrams' frequency features using the `create_feature_frequency()` functions you defined in quiz 3? Please store the features into `unigram_frequency_positive` and `unigram_frequency_negative` for positive and negative directories respectively. \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 5:\n",
    "\n",
    "Let's check the first positive review's unigram presence feature, can you use the mapping in `index2vocab` to reveal what unigrams are present in this review? \n",
    "Please answer: Which words in the following list are present?\n",
    "A. gayness\n",
    "B. fabulous\n",
    "C. snappiness\n",
    "D. happiness\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]\n",
    "'happiness' in a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p> Code:</p>\n",
    "    <p>wordlist=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]</p>\n",
    "    <p> A,C are present in the review </p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save the prepared features for the reviews ready for modeling.\n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas.\n",
    "\n",
    "To store a numpy array, we can use the `numpy`'s `save()` function. `save()` takes two arguments: the first is an the filename to be written, and the second argument is the numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('unigram_presence_positive.npy',unigram_presence_positive)\n",
    "np.save('unigram_presence_negative.npy',unigram_presence_negative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the postiive data from the files by:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load('unigram_presence_positive.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on `clean_doc_unigrams()` that turns a document into a unigram dictionary, we can create a function `clean_doc_bigrams()` to extract bigram dictionary. (You can refresh yourself of how to create a bigram counter dictionary in module 1.4. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_bigrams(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # extract bigrams\n",
    "    bigram_dict=Counter() #initialize a bigram dictionary to be updated\n",
    "    tokens=['<start>']+tokens+['<end>'] # add <start> and <end> token\n",
    "    for i in range(len(tokens)): #loop over all the indices of the token list\n",
    "        if i<len(tokens)-1: #if it's not the end of the token list\n",
    "            bigram_current=(tokens[i],tokens[i+1])\n",
    "            bigram_dict[bigram_current]+=1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replace the `clean_doc_unigrams()` line with `clean_doc_bigrams()` in `process_docs_unigrams()`, we will rename the function as `process_docs_bigrams()` that counts all the bigrams in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_bigrams_counter=Counter()\n",
    "def process_docs_bigrams(directory,overall_bigrams_counter):\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_bigrams(text)\n",
    "        update_counter(overall_bigrams_counter,tokens_dict_current)\n",
    "process_docs_bigrams('./data/neg',overall_bigrams_counter)\n",
    "process_docs_bigrams('./data/pos',overall_bigrams_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `overall_bigrams_counter`, we can pass it to `create_vocab_feature_mappings()` to create index-bigram mappings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram2index,index2bigram=create_vocab_feature_mappings(overall_bigrams_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can modify `process_docs_unigrams_presence()` to process bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_docs_bigrams_presence(directory,bigram2index):\n",
    "    # loop over the directory to extract filenames that have the right extension\n",
    "    filenames=[filename for filename in listdir(directory) if filename.endswith(\".txt\")]\n",
    "    # since we know how many files we will process, and the vocabulary size, we can initialize our result matrix as an empty array with the shape of (file number, vocabulary size)\n",
    "    \n",
    "    bigram_presence_result=np.empty((len(filenames),len(bigram2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    for file_i,filename in enumerate(filenames):\n",
    "        \n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        bigrams_dict_current = clean_doc_bigrams(text)\n",
    "        # convert bag of words in each document into unigram features\n",
    "        bigram_presence=create_feature_presence(bigrams_dict_current,bigram2index)\n",
    "        # We assign the bigram fearture of the current document to the correct positon in the bigram_presence_result matrix. \n",
    "        # we append the bigram_feature in each document to the bigram_feature_result array and update the array. \n",
    "        # to ensure the correct dimension size, we nest the unigram_feature vector so that it becomes a 2-d array as the initialization in unigram_feature_result\n",
    "        bigram_presence_result[file_i]=bigram_presence\n",
    "    return bigram_presence_result\n",
    "bigram_presence_positive=process_docs_bigrams_presence('./data/pos',bigram2index)\n",
    "bigram_presence_negative=process_docs_bigrams_presence('./data/neg',bigram2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then save the bigram features:\n",
    "np.save('bigram_presence_positive.npy',bigram_presence_positive)\n",
    "np.save('bigram_presence_negative.npy',bigram_presence_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice quiz 6:\n",
    "\n",
    "Now we have features in the form of bigrams. Now what does each dimension represent for a vector now? How many dimensions do we have for each vector? You can inspect `bigram_presence_positive` to answer these questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Each dimension corresponds to a bigram. There are 463119 bigrams and therfore the dimension size of each vector is 463119. </p>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjective Unigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams above certain frequency threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams + Bigrams (Quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negation (extension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
