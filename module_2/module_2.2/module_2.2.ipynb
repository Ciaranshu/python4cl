{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to module 2.2! In the last module, we investigated how to use `scikit-learn` to build a simple Naive Bayes model to perform sentiment analysis. In this notebook we will look behind the scenes: we will build our own Naive Bayes model from scratch. We will reuse some of the code we wrote in the previous notebook - but this time, we'll write the maths from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Pre-module quiz\n",
    "\n",
    "Why is Naive Bayes \"naive\"?\n",
    "\n",
    "A. Because it's the most basic, i.e. \"naive\" classifier we can build\n",
    "\n",
    "B. Because it \"naively\" assumes that the probabilities of features (i.e., in our case, words) are independent of each other\n",
    "\n",
    "C. Because the guy who invented it tought it was a cool name\n",
    "\n",
    "D. Because it \"naively\" assumes that the probabilities of features (i.e., in our case, words) are dependent of each other\n",
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>The correct answer is B - Naive Bayes assumes that the probability of finding a certain word is independent from the probability of finding another word. So, for example, in the domain of movie reviews, it assumes that the probabilities of finding the words <code>Indiana</code> and <code>Jones</code> are not correlated, even if in practice we know that this is not the case.</p>\n",
    "\n",
    "</details> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Computational Linguists 1.2: Breaking down Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of last notebook\n",
    "\n",
    "Module 2.1 introduced lots of new concepts and libraries - pre-processing, feature vectors, `numpy`, and so on. In fact, we have seen how to import data from a research dataset, how to clean them by removing punctuation and stop-words, how to use `numpy` to prepare test and training data for a model, and how to use `scikit-learn` to train, test and evaluate a simple Naive Bayes classifier for sentiment analysis.\n",
    "\n",
    "However, we didn't look 'behind the scenes' at how Naive Bayes actually works. This module will guide you through your first hand-written machine learning model, by showing you how to write the maths for Naive Bayes yourself. We will introduce the concept of \"class\", and we'll debug and improve our model by adding smoothing.\n",
    "\n",
    "If you're interested, you can also just go and look the code of `scikit-learn`'s implementation [here](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/naive_bayes.py#L669)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes refresher\n",
    "\n",
    "> Note: this section borrows heavily from the Naive Bayes chapter of the lecture notes. Please refer to them or to the J&M for more details.\n",
    "\n",
    "Naive Bayes is a simple classifier based on two assumptions:\n",
    "- The **bag-of-words** assumption: word ordering doesn't matter. We represent each document in our dataset as a list of pairs $(word_i,frequency_i)$.\n",
    "- The **conditional independence** assumption: the probability of one word appearing in a sentence is by no means correlated to the occurrence of another word.\n",
    "\n",
    "These two assumptions heavily simplify our model, since they completely disregard grammar and and eventual domain knowledge (e.g. the `Indiana Jones` example in the pre-module quiz); but as we know now, they allow us to build a surprisingly efficient classifier.\n",
    "\n",
    "Let's quickly go through the maths of Naive Bayes. Remember that given a document $d$ and a set of classes $C$, the probability the document belonging to a class $c \\in C$ is $\\hat{c}$, and it is defined as:\n",
    "\n",
    "$$ \\hat{c} = \\text{argmax}_{c \\in C} P(c \\mid d)$$\n",
    "\n",
    "However, the Bayes Rule tells us that\n",
    "\n",
    "$$ P(c \\mid d) = \\frac{P(c) \\ P(d \\mid c)}{P(d)} $$\n",
    "\n",
    "Allowing us to rewrite \n",
    "\n",
    "$$\\begin{align} \n",
    "\\hat{c} &= \\text{argmax}_{c \\in C} P(c \\mid d) \\\\\n",
    "        &= \\text{argmax}_{c \\in C} \\frac{P(c) \\ P(d \\mid c)}{P(d)}\n",
    "\\end{align}$$\n",
    "\n",
    "However, the probability $d$ is constant for each class $c$, hence we can remove it, leaving only:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\text{argmax}_{c \\in C} \n",
    "    \\underbrace{P(c) \\ P(d \\mid c)}_\\text{likelihood}\n",
    "    \\underbrace{P(d)}_\\text{prior}\n",
    "$$\n",
    "\n",
    "Where the $prior$ is the **prior probability** of the class $c$ and the $likelihood$ is the probability of finding $d$ given the class $c$.\n",
    "\n",
    "Using words as features, we can represent $d$ as a list of words $w_1, \\dots , w_n$, hence \n",
    "\n",
    "$$\n",
    "\\hat{c} = \\text{argmax}_{c \\in C} \n",
    "    \\underbrace{P(c) \\ P(w_1, \\dots , w_n \\mid c)}_\\text{likelihood} \n",
    "    \\underbrace{P(d)}_\\text{prior}\n",
    "$$\n",
    "\n",
    "However, $P(w_1, \\dots , w_n \\mid c)$ may be prohibitively hard to calculate, since we would need to estimate the probability of every possible combination of words. Here, the **conditional independence** assumption comes to the rescue, assuming the probability of the words (i.e. features) are independent, allowing us to finally rewrite\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{c} &= \\text{argmax}_{c \\in C} \n",
    "       \\underbrace{P(d)}_\\text{prior}\n",
    "       \\underbrace{P(c) \\ P(w_1 \\mid c) \\times \\dots \\times P(w_n \\mid c)}_\\text{likelihood} \n",
    "       \\\\\n",
    "       &= \\text{argmax}_{c \\in C} \n",
    "       \\underbrace{P(d)}_\\text{prior}\n",
    "       \\underbrace{\\prod_{w \\in d}{P(w \\mid c)}}_\\text{likelihood} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does all of this mean in practice? Well, that if we have a document $d$, all we need to know to classify it is:\n",
    "- The *priors*, i.e. the probability of document $d$ to belong to each class $c$\n",
    "- The *likelihoods*, i.e. the probabilities for each word $w_i$ of the document to belong to each class $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes: a simple implementation\n",
    "\n",
    "Let's begin with a simple example from the Post Lecture exercises (taken from J&M-3, exercise 4.2). Given the following short movie reviews, each labeled with a genre, either comedy or action:\n",
    "\n",
    "| review                      | class  |\n",
    "|-----------------------------|--------|\n",
    "| fun, couple, love, love     | comedy |\n",
    "| fast, furious, shoot        | action |\n",
    "| couple, fly, fast, fun, fun | comedy |\n",
    "| furious, shoot, shoot, fun  | action |\n",
    "| fly, fast, shoot, love      | action |\n",
    "\n",
    "And a new document D: \n",
    "\n",
    "| review                     | class  |\n",
    "|----------------------------|--------|\n",
    "| fast, couple, shoot, fly   | ?      |\n",
    "\n",
    "Compute the most likely class for D.\n",
    "\n",
    "Let's start by saving our documents in some vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [\n",
    "    ['fun', 'couple', 'love', 'love'],\n",
    "    ['fast', 'furious', 'shoot'],\n",
    "    ['couple', 'fly', 'fast', 'fun', 'fun'],\n",
    "    ['furious', 'shoot', 'shoot', 'fun'],    \n",
    "    ['fly', 'fast', 'shoot', 'love']]\n",
    "\n",
    "train_labels = ['comedy', 'action', 'comedy', 'action', 'action']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the priors\n",
    "\n",
    "Remember what we needed to do? The first step is to compute the **priors**. Let's do that with a simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action', 'comedy'}\n",
      "{'action': 3, 'comedy': 2}\n"
     ]
    }
   ],
   "source": [
    "# What are our classes?\n",
    "classes = set(train_labels)\n",
    "print(classes)\n",
    "\n",
    "# initialise the priors\n",
    "priors = {}\n",
    "for _class in classes:\n",
    "    priors[_class] = 0\n",
    "\n",
    "# count how many train example in each class\n",
    "for _class in classes:\n",
    "    for label in train_labels:\n",
    "        if _class == label:\n",
    "            priors[_class] += 1\n",
    "\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>üíª Try it yourself!</h3>**\n",
    "\n",
    "Now the priors are not *normalised*, i.e. we have to bring each prior in the range $[0,1]$. Can you do that in the following cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p><pre><code>\n",
    "for _class in classes:\n",
    "    priors[_class] = priors[_class] / len(train_labels)\n",
    "    </code></pre></p>\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've got the priors correct, you should have $P(comedy) = 0.4$ and $P(action)=0.6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>üíª Try it yourself!</h3>**\n",
    "\n",
    "Now let's wrap everything nicely into a function. Can you complete the cell below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_priors(labels):\n",
    "    '''\n",
    "    Computes the priors for a set of labels.\n",
    "    '''\n",
    "\n",
    "    # What are our classes?\n",
    "    classes = set(labels)\n",
    "\n",
    "    # initialise the priors\n",
    "    priors = {}\n",
    "\n",
    "    # ...?\n",
    "\n",
    "    # count how many train example in each class\n",
    "    \n",
    "    # ...?\n",
    "    \n",
    "    # normalise the priors\n",
    "    \n",
    "    # ...?\n",
    "    \n",
    "    return priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p><pre><code>\n",
    "def compute_priors(labels):\n",
    "    '''\n",
    "    Computes the priors for a set of labels.\n",
    "    '''\n",
    "    \n",
    "    # What are our classes?\n",
    "    classes = set(labels)\n",
    "\n",
    "    # initialise the priors\n",
    "    priors = {}\n",
    "    for _class in classes:\n",
    "        priors[_class] = 0\n",
    "\n",
    "    # count how many train example in each class\n",
    "    for _class in classes:\n",
    "        for label in labels:\n",
    "            if _class == label:\n",
    "                priors[_class] += 1\n",
    "    \n",
    "    # normalise the priors\n",
    "    for _class in classes:\n",
    "        priors[_class] = priors[_class] / len(labels)\n",
    "        \n",
    "    return priors\n",
    "    </code></pre></p>\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 0.6, 'comedy': 0.4}\n"
     ]
    }
   ],
   "source": [
    "priors = compute_priors(train_labels)\n",
    "print(priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the vocabulary\n",
    "\n",
    "Since we need to compute the likelihoods for all words in our vocabulary, we need to find all the words in our corpus first. Let's build our vocabulary with a simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(lines):\n",
    "    '''\n",
    "    Creates a vocabulary from test.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lines: a list of lists of words\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a set with all the words in the lines.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    words = set()\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            words.add(word)\n",
    "            \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'couple', 'fast', 'fly', 'fun', 'furious', 'love', 'shoot'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_vocabulary(train_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to compute the likelihoods of the words w.r.t. to each class. To do that, we can build a `dict` for each class, where `dict[word] = P(word|class)`. To do that we can slightly modify the function `create_vocab_dict` that we defined in [Module 1.4](../../module_1/module_1.4/module_1.4.ipynb). The function we defined was the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab_dict(lines):\n",
    "    '''\n",
    "    Collect vocabulary counts from text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_processed_arg: a list of lists of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with words (str) as keys and counts(int) as values\n",
    "    vocab={\n",
    "    'SONNETS': 1\n",
    "    }\n",
    "    '''\n",
    "    vocab={}# create an empty vocabulary dictionary to store words as keys and counts as values later. \n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            if word in vocab:\n",
    "                vocab[word]+=1 # update the count for an existing word\n",
    "            else:\n",
    "                vocab[word]=1 # initilize the count for a new word\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fun': 4, 'couple': 2, 'love': 3, 'fast': 3, 'furious': 1, 'shoot': 4, 'fly': 2, 'curious': 1}\n"
     ]
    }
   ],
   "source": [
    "print(create_vocab_dict(train_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>üíª Try it yourself!</h3>**\n",
    "\n",
    "How can we modify this function to give us the likelihoods for each class? Modify the function below, directly derived from `create_vocab_dict`, to return the likelihoods instead of the raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihoods(lines, vocabulary):\n",
    "    '''\n",
    "    Computes the likelihoods of words in a list of strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_processed_arg: a list of list of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with words (str) as keys and likelihoods(floats) as values\n",
    "    vocab={\n",
    "    'SONNETS': 0.01\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # -------------------------#\n",
    "    #      E X E R C I S E     #\n",
    "    # -------------------------#\n",
    "    # create an empty vocabulary dictionary to store words \n",
    "    # as keys and counts as values later. \n",
    "    \n",
    "    likelihoods = {}\n",
    "    \n",
    "    # populate the likelihoods\n",
    "    # hint: iterate through the vocabulary and initialise\n",
    "    # a new element of the likelihoods dict to 0\n",
    "\n",
    "    \n",
    "    \n",
    "    # ~      end exercise    ~ #    \n",
    "    \n",
    "    # Now we iterate through the lines\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            likelihoods[word] +=1 \n",
    "\n",
    "    # -------------------------#\n",
    "    #      E X E R C I S E     #\n",
    "    # -------------------------#\n",
    "    # how long are our documents?\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # write your code here\n",
    "    # (hint: sum the length of the lines in total_tokens!)\n",
    "    \n",
    "    for word in likelihoods:\n",
    "        likelihoods[word] = # Write your code here\n",
    "    \n",
    "    # ~      end exercise    ~ #\n",
    "\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p><pre><code>\n",
    "def compute_likelihoods(lines, vocabulary):\n",
    "    '''\n",
    "    Computes the likelihoods of words in a list of strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_processed_arg: a list of list of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with words (str) as keys and likelihoods(floats) as values\n",
    "    vocab={\n",
    "    'SONNETS': 0.01\n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # -------------------------#\n",
    "    #      E X E R C I S E     #\n",
    "    # -------------------------#\n",
    "    # create an empty vocabulary dictionary to store words \n",
    "    # as keys and counts as values later. \n",
    "    \n",
    "    likelihoods = {}\n",
    "    \n",
    "    # populate the likelihoods\n",
    "    # hint: iterate through the vocabulary and initialise\n",
    "    # a new element of the likelihoods dict to 0\n",
    "    for word in vocabulary:\n",
    "        likelihoods[word] = 0\n",
    "    \n",
    "    \n",
    "    # ~      end exercise    ~ #    \n",
    "    \n",
    "    # Now we iterate through the lines\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            likelihoods[word] +=1 \n",
    "\n",
    "    # -------------------------#\n",
    "    #      E X E R C I S E     #\n",
    "    # -------------------------#\n",
    "    # how long are our documents?\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # write your code here\n",
    "    # (hint: sum the length of the lines in total_tokens!)\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            total_tokens +=1\n",
    "    \n",
    "    for word in likelihoods:\n",
    "        likelihoods[word] = likelihoods[word]/total_tokens\n",
    "    \n",
    "    # ~      end exercise    ~ #\n",
    "\n",
    "    return likelihoods\n",
    "    </code></pre></p>\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'curious': 0.05, 'fly': 0.1, 'shoot': 0.2, 'love': 0.15, 'fun': 0.2, 'fast': 0.15, 'furious': 0.05, 'couple': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(compute_likelihoods(train_docs, create_vocabulary(train_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this method computes the likelihoods of the words in the whole vocabulary, irregardless of the classes. To get the likelihoods for each class we need to do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fast', 'furious', 'shoot'], ['furious', 'shoot', 'shoot', 'fun'], ['fly', 'fast', 'shoot', 'love']]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_likelihoods() missing 1 required positional argument: 'smoothing'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-f339d4179b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_likelihoods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: compute_likelihoods() missing 1 required positional argument: 'smoothing'"
     ]
    }
   ],
   "source": [
    "target_class = 'action'\n",
    "target_docs = []\n",
    "vocabulary = create_vocabulary(train_docs)\n",
    "\n",
    "# enumerate builds an (index, doc) list, hence allowing\n",
    "# us to retrieve the label for each doc\n",
    "for i, doc in enumerate(train_docs):\n",
    "    if train_labels[i] == target_class:\n",
    "        target_docs.append(doc)\n",
    "        \n",
    "print(target_docs)\n",
    "print(compute_likelihoods(target_docs, vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>üíª Try it yourself!</h3>**\n",
    "\n",
    "Now we have all the instruments to build our training function. Can you complete it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-531bed73424a>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-531bed73424a>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    for #...\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train_naive_bayes(documents, labels):\n",
    "    \n",
    "    classes = set(labels)\n",
    "    \n",
    "    # compute the priors\n",
    "    priors = compute_priors(labels)\n",
    "    vocabulary = create_vocabulary(documents)\n",
    "    \n",
    "    # this dict will contain the likelihoods, e.g.\n",
    "    # likelihoods['action'] = {'fast': 0.2, 'furious': 0.1...\n",
    "    likelihoods = {}\n",
    "    \n",
    "    # -------------------------#\n",
    "    #      E X E R C I S E     #\n",
    "    # -------------------------#\n",
    "    for _class in classes:\n",
    "        # get the documents belonging to each class:\n",
    "        class_docs = []\n",
    "\n",
    "        # put your code here\n",
    "        for #...\n",
    "        \n",
    "        # compute the likelihood of this class\n",
    "        likelihoods[_class] = # hint: use compute_likekihood defined above\n",
    "        \n",
    "    # ~      end exercise    ~ #\n",
    "        \n",
    "    return priors, likelihoods    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p><pre><code>\n",
    "def train_naive_bayes(documents, labels):\n",
    "\n",
    "    classes = set(labels)\n",
    "\n",
    "    # compute the priors\n",
    "    priors = compute_priors(labels)\n",
    "    vocabulary = create_vocabulary(documents)\n",
    "\n",
    "    # this dict will contain the likelihoods, e.g.\n",
    "    # likelihoods['action'] = {'fast': 0.2, 'furious': 0.1...\n",
    "    likelihoods = {}\n",
    "\n",
    "    for _class in classes:\n",
    "        # get the documents belonging to each class:\n",
    "        class_docs = []\n",
    "\n",
    "        for i, doc in enumerate(documents):\n",
    "            if labels[i] == _class:\n",
    "                class_docs.append(doc)\n",
    "\n",
    "        # compute the likelihood of this class\n",
    "        likelihoods[_class] = compute_likelihoods(class_docs, vocabulary)\n",
    "\n",
    "    return priors, likelihoods \n",
    "    </code></pre></p>\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors:\n",
      "{'action': 0.6, 'comedy': 0.4}\n",
      "\n",
      "Likelihoods:\n",
      "{'action': {'curious': 0.09090909090909091, 'fly': 0.09090909090909091, 'shoot': 0.36363636363636365, 'love': 0.09090909090909091, 'fun': 0.09090909090909091, 'fast': 0.18181818181818182, 'furious': 0.09090909090909091, 'couple': 0.0}, 'comedy': {'curious': 0.0, 'fly': 0.1111111111111111, 'shoot': 0.0, 'love': 0.2222222222222222, 'fun': 0.3333333333333333, 'fast': 0.1111111111111111, 'furious': 0.0, 'couple': 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "priors, likelihoods = train_naive_bayes(train_docs, train_labels)\n",
    "\n",
    "print('Priors:')\n",
    "print(priors)\n",
    "\n",
    "print('')\n",
    "print('Likelihoods:')\n",
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict unknown classes\n",
    "\n",
    "So now we have trained our model. How can we predict the likelihood of new sentences belonging to each class? Remember from above that\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\text{argmax}_{c \\in C} \n",
    "       \\underbrace{P(d)}_\\text{prior}\n",
    "       \\underbrace{\\prod_{w \\in d}{P(w \\mid c)}}_\\text{likelihood} \n",
    "$$\n",
    "\n",
    "So, to predict the class of our new sentence `fast, couple, shoot, fly`, we need to:\n",
    "- for each class `c`, we need to calculate `prob_c` by\n",
    "    - multiplying the probability of each word `w` given that class `c`\n",
    "\n",
    "Then, select the maximum of our `prob_c`s. So, can we predict the class of `fast, couple, shoot, fly`? Let's write a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_predict(document, priors, likelihoods):\n",
    "    '''\n",
    "    Predicts the label for a document given the trained\n",
    "    priors and likelihoods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    document: the document to analyse\n",
    "    priors: the trained priors\n",
    "    likelihoods: the trained likelihoods\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    The probability for each class.\n",
    "    '''\n",
    "    \n",
    "    classes_probabilities = {}\n",
    "\n",
    "    # unpack the dictionary and iterate \n",
    "    # through the priors\n",
    "    for label, prior in priors.items():\n",
    "        \n",
    "        # initialise the probability of a class to its prior\n",
    "        prob_class = prior\n",
    "        for word in document:\n",
    "            if word in likelihoods[label]:\n",
    "                # multiply the prior for the likelihood of each word\n",
    "                prob_class = prob_class*likelihoods[label][word]\n",
    "        classes_probabilities[label] = prob_class\n",
    "        \n",
    "    return classes_probabilities\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Let's try to a new document with some random words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 8.19616146438085e-05, 'comedy': 0.0003657978966620942}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = ['fast', 'fun', 'love', 'fly']\n",
    "bayes_predict(document, priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for this document, `comedy` is the most likely class! \n",
    "\n",
    "Now let's try with the document for the assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 0.0, 'comedy': 0.0}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = ['fast', 'couple', 'shoot', 'fly']\n",
    "bayes_predict(document, priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the classes are both zero! How come? Well, we didn't apply any smoothing, so obviously at some point we are multiplying the likelihoods by zero, since $P(couple \\ \\mid action) = 0$ and $P(shoot \\mid love) = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding 1-smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>üíª Try it yourself!</h3>**\n",
    "\n",
    "How can we modify `compute_likelihoods` to add 1-smoothing? Please update the function below.\n",
    "\n",
    "Note that:\n",
    "- We added a parameter (`smoothing`) to select the smoothing mode\n",
    "- You will need to change how to normalise the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_likelihoods(lines, vocabulary, smoothing):\n",
    "    '''\n",
    "    Computes the likelihoods of words in a list of strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lines: a list of list of words\n",
    "    vocabulary: the vocabulary of the training corpus\n",
    "    smooething: the smoothing method to use\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with the probability for each class\n",
    "    '''\n",
    "    \n",
    "    likelihoods = {}\n",
    "\n",
    "    # populate the likelihoods\n",
    "    for word in vocabulary:\n",
    "        likelihoods[word] = 0\n",
    " \n",
    "\n",
    "    # Now we iterate through the lines to count\n",
    "    # the appearances of each word\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            likelihoods[word] +=1 \n",
    "\n",
    "    # how long are our documents?\n",
    "    total_tokens = 0\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            total_tokens +=1\n",
    "\n",
    "    # Apply smoothing, if needed\n",
    "    for word in likelihoods:\n",
    "        if smoothing == 'none':\n",
    "            \n",
    "            for line in lines:\n",
    "                for word in line:\n",
    "                    total_tokens +=1\n",
    "\n",
    "            for word in likelihoods:\n",
    "                likelihoods[word] = likelihoods[word]/total_tokens\n",
    "            \n",
    "        elif smoothing == 'add1':\n",
    "            # -------------------------#\n",
    "            #      E X E R C I S E     #\n",
    "            # -------------------------#\n",
    "            # calculate the smoothing parameter for each word.\n",
    "            smoothing_param = # ...?\n",
    "            \n",
    "            likelihoods[word] = # ...?\n",
    "            # ~      end exercise    ~ #\n",
    "        else:\n",
    "            print('Unknown smoothing!')\n",
    "            return\n",
    "\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p><pre><code>\n",
    "def compute_likelihoods(lines, vocabulary, smoothing):\n",
    "    '''\n",
    "    Computes the likelihoods of words in a list of strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lines: a list of list of words\n",
    "    vocabulary: the vocabulary of the training corpus\n",
    "    smooething: the smoothing method to use\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    a dictionary with the probability for each class\n",
    "    '''\n",
    "    \n",
    "    likelihoods = {}\n",
    "\n",
    "    # populate the likelihoods\n",
    "    for word in vocabulary:\n",
    "        likelihoods[word] = 0\n",
    " \n",
    "\n",
    "    # Now we iterate through the lines to count\n",
    "    # the appearances of each word\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            likelihoods[word] +=1 \n",
    "\n",
    "    # how long are our documents?\n",
    "    total_tokens = 0\n",
    "    for line in lines:\n",
    "        for word in line:\n",
    "            total_tokens +=1\n",
    "\n",
    "    # Apply smoothing, if needed\n",
    "    for word in likelihoods:\n",
    "        if smoothing == 'none':\n",
    "            \n",
    "            for line in lines:\n",
    "                for word in line:\n",
    "                    total_tokens +=1\n",
    "\n",
    "            for word in likelihoods:\n",
    "                likelihoods[word] = likelihoods[word]/total_tokens\n",
    "            \n",
    "        elif smoothing == 'add1':\n",
    "            # -------------------------#\n",
    "            #      E X E R C I S E     #\n",
    "            # -------------------------#\n",
    "            # calculate the smoothing parameter for each word.\n",
    "            smoothing_param = total_tokens + len(vocabulary)\n",
    "            \n",
    "            likelihoods[word] = (likelihoods[word] + 1)/smoothing_param\n",
    "            # ~      end exercise    ~ #\n",
    "        else:\n",
    "            print('Unknown smoothing!')\n",
    "            return\n",
    "\n",
    "    return likelihoods\n",
    "    </code></pre></p>\n",
    "\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update `train_naive_bayes` to instruct it to use smoothing, and let's see if the results we obtain are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(documents, labels, smoothing):\n",
    "\n",
    "    classes = set(labels)\n",
    "\n",
    "    # compute the priors\n",
    "    priors = compute_priors(labels)\n",
    "    vocabulary = create_vocabulary(documents)\n",
    "\n",
    "    # this dict will contain the likelihoods, e.g.\n",
    "    # likelihoods['action'] = {'fast': 0.2, 'furious': 0.1...\n",
    "    likelihoods = {}\n",
    "\n",
    "    for _class in classes:\n",
    "        # get the documents belonging to each class:\n",
    "        class_docs = []\n",
    "\n",
    "        for i, doc in enumerate(documents):\n",
    "            if labels[i] == _class:\n",
    "                class_docs.append(doc)\n",
    "\n",
    "        # compute the likelihood of this class\n",
    "        likelihoods[_class] = compute_likelihoods(class_docs, vocabulary, smoothing)\n",
    "\n",
    "    return priors, likelihoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors:\n",
      "{'action': 0.6, 'comedy': 0.4}\n",
      "\n",
      "Likelihoods:\n",
      "{'action': {'fly': 0.1111111111111111, 'shoot': 0.2777777777777778, 'love': 0.1111111111111111, 'fun': 0.1111111111111111, 'fast': 0.16666666666666666, 'furious': 0.16666666666666666, 'couple': 0.05555555555555555}, 'comedy': {'fly': 0.125, 'shoot': 0.0625, 'love': 0.1875, 'fun': 0.25, 'fast': 0.125, 'furious': 0.0625, 'couple': 0.1875}}\n"
     ]
    }
   ],
   "source": [
    "priors, likelihoods = train_naive_bayes(train_docs, train_labels, 'add1')\n",
    "\n",
    "print('Priors:')\n",
    "print(priors)\n",
    "\n",
    "print('')\n",
    "print('Likelihoods:')\n",
    "print(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 0.00017146776406035664, 'comedy': 7.324218750000001e-05}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = ['fast', 'couple', 'shoot', 'fly']\n",
    "bayes_predict(document, priors, likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We successfully implemented Naive Bayes in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Naive Bayes: `argmax` and log-likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other smoothing functions\n",
    "\n",
    "See [this paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.8978&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-classify the *Thumbs up?* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of loader functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences between our model and `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
