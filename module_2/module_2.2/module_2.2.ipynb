{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xv4NPtgAReWr"
   },
   "source": [
    "Welcome to module 2.1. In this module, we will use a popular Python module `scikit-learn` to build a sentiment classifier with Naive Bayes! We will introduce the concept of feature as numerical representation of the input data. We will experiment with different types of features to investiage their impact on training. \n",
    "At the end of the module, you should be able to:\n",
    "\n",
    "* understand features and how to extract them from input data\n",
    "* experiment with an off-the-shelf Naive Bayes model\n",
    "* be familar with the pipeline for bulding a machine learning model incluidng data cleaning, feature extraction, and model evaluation.\n",
    "\n",
    "\n",
    "Let's first refresh your memory on the Naive Bayes model. \n",
    "\n",
    "### ‚ùì Pre-module quiz\n",
    "\n",
    "Say that we have two events: Fire and Smoke. $P(Fire)$ is the probability of a fire (or in other words, how often a fire occurs), $P(Smoke)$ is the probability of seeing smoke (how often we see smoke). We want to know $P(Fire|Smoke)$, that is, how often fire occurs when we see smoke. Suppose we know the following:\n",
    "\n",
    "$P(Fire)=0.01$\n",
    "\n",
    "$P(Smoke)=0.1$\n",
    "\n",
    "$P(Smoke|Fire)=0.9$ (ie. 90\\% of the fire makes smoke)\n",
    "\n",
    "\n",
    "Can you work out $P(Fire|Smoke)$?\n",
    "\n",
    "A. 0.1\n",
    "\n",
    "B. 0.09\n",
    "\n",
    "C. 0.01\n",
    "\n",
    "D. 0.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ra8qD88ReWu"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p>\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii18vTN8ReWx"
   },
   "source": [
    "# Sentiment Analysis Task Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AplzSdOIReWz"
   },
   "source": [
    "Our task of focus today is a popular NLP classification task: sentiment analysis. What exactly is sentiment? Sentiment is an opinion or emotion (usually positive or negative) that is reflected in a sequence of words. And analysis? Well, this is the process of looking at data and making inferences about the sentiment; in our case, we will use machine learning to learn and predict whether a movie review is positive or negative.\n",
    "\n",
    "We will replicate some of the experiments from the paper: [Thumbs up? Sentiment Classification using Machine Learning Techniques](https://www.aclweb.org/anthology/W02-1011.pdf). We will extract a number of features including unigrams, bigrams etc., and train Naive Bayes models on these features. (Notice that we use the second release of the data which is preprocessed and cleaned rather than the original data in the paper. Therefore, the results are not directly comparable. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bav1A-D6ReW1"
   },
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2Mgv4bfReW3"
   },
   "source": [
    "The data for this tutorial is stored in the `./data` folder. The two subdirectories `./data/pos` and `./data/neg` contain samples of IMDb positive and negative movie reviews. Each line of a review text file is a tokenized sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBsHahXeScH7"
   },
   "source": [
    "As usual, we download the files for the notebook from Github. If you're running this notebook locally or on Binder, you may skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kFuAQbpSdRf"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/cambridgeltl/python4cl/raw/master/module_2/module_2.2/data.zip\n",
    "!unzip -n -q data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFew9K1WReW5"
   },
   "source": [
    "We can load an individual text file by opening it, reading in the ASCII text, and closing the file. For example, we can load the first negative review file ‚Äúcv000_29416.txt‚Äù as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QngDdJeGReW_"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load one file\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "# open the file as read only\n",
    "file = open(filename, 'r')\n",
    "# read all text\n",
    "text = file.read()\n",
    "print (text)\n",
    "# close the file\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mmv7wS0cReXH"
   },
   "source": [
    "This loads the document as ASCII and preserves any white space, like new lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOSa4383ReXI"
   },
   "source": [
    "We can turn this into a function called `load_doc()` that takes a filename of the document to load and returns the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GZg4B9FReXL"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: the filename to extract text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    text in strings\n",
    "    \"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_13EAmfReXS"
   },
   "source": [
    "We can process each directory in turn by first getting a list of files in the directory using the `listdir()` function from the `os` module, then loading each file in turn.\n",
    "\n",
    "For example, we can load each document in the negative directory using the `load_doc()` function to do the actual loading. Below, we define a `process_docs()` function to load all documents in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60kbNDGiReXV"
   },
   "outputs": [],
   "source": [
    "from os import listdir \n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of of documents, where each document is string text\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    docs=[] # a list of review texts\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        doc = load_doc(path)\n",
    "        print('Loaded %s' % filename)\n",
    "        docs.append(doc)\n",
    "    return docs\n",
    " \n",
    "# specify directory to load\n",
    "directory = 'data/neg'\n",
    "docs=process_docs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6SlpyplReXd"
   },
   "source": [
    "**<h3>üíª Try it yourself!</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ4Ol0plReXe"
   },
   "source": [
    "Use the predefined `process_docs()` function to read in negative texts and positive reviews. How many reviews are there for each class? \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngOZpkTQReXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bf82-7OFReXm"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>1000 positive and 1000 negative reviews</p>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU3WDGsMReXn"
   },
   "source": [
    "# Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LbJ8xKIReXo"
   },
   "source": [
    "So far we have obtained the text of each document. If we follow the previous module's definition on Naive Bayes, we could take bag of word represention for each document, and compute the likelihood $P(d \\mid c)$ as:\n",
    "\n",
    "$P(d \\mid c) = P(w_1, \\dots , w_n \\mid c) $\n",
    "\n",
    "This is however only one way to represent the document. We could for example select some of the words (eg. more informative words) or use bigrams instead of unigram words, or combine unigrams and bigrams to represent the document. We could even add information such as document length, date of creation into the representation. All these measurable properties that we can use to represent the input data are referred to as $features$. We can replace $w_1, w_2, \\dots, w_n$ with the more general form: $f_1,f_2,\\dots,f_n$ to indicate individual features. The following is the new formulation of the likelihood. \n",
    "\n",
    "$P(d \\mid c) = P(f_1, \\dots , f_n \\mid c) $\n",
    "\n",
    "Practically, we can group a document's feature values into a list: [f1,f2..fn]. This will be a numerical representation of the document that can be fed directly into a model. We call this list of numbers as a feature vector. \n",
    "\n",
    "Below, We delve deeper into the concept of features and feature vectors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlQMfCX6ReYQ"
   },
   "source": [
    "## Feature vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb62eNVSReYR"
   },
   "source": [
    "The feature vector has a fixed length corresponding to the number of features. Each position of the list represents the value of a feature. For example, to represent unigrams in a document, we can create a vector with the same length of the vocabulary, and the value in each each position of the list stores the frequency or presence of a specific word. \n",
    "\n",
    "\n",
    "As an example, suppose we have eight words (apple,banana,red,dog,is,the,and,yellow) in the vocabulary which are represented as eight features. We also have two documents as in below:\n",
    "\n",
    "document 1: \"the apple is red and the banana is yellow\"\n",
    "document 2: \"the red dog\"\n",
    "\n",
    "To produce a feature vector to represent unigram presence in a document, we can write '1' in a position where the correspoding word is present in the document, and we will write '0' to indicate the word is not present. Below is a table view of the vectors. \n",
    "\n",
    "\n",
    "document no.  |  apple | banana | red | dog | is | the | and | yellow |\n",
    "------|------|------|------|------|------|------|------|------|\n",
    "document 1 |1|1|1|0|1|1|1|1|\n",
    "document 2 |0|0|1|1|0|1|0|0|\n",
    "\n",
    "\n",
    "The feature vector for document 1 becomes `[1,1,1,0,1,1,1,1]`\n",
    "\n",
    "To produce a feature vector that represent frequency of each unigram in a document, we can count the number of occurence of each unigram word and write down the number in the corresponding position. \n",
    "\n",
    "document no. | apple | banana | red | dog | is | the | and | yellow |\n",
    "------|------|------|------|------|------|------|------|------|\n",
    "document 1 |1|1|1|0|2|2|1|1|\n",
    "document 2 |0|0|1|1|0|1|0|0|\n",
    "\n",
    "\n",
    "Now the feature vector for document 1 becomes `[1,1,1,0,2,2,1,1]`\n",
    "\n",
    "When the data consists of more than one document as in our example, we will have multiple feature vectors as representations of our data. We can stack the vectors into a list of vectors. This is often referred to as matrix. \n",
    "In our example, we have a 2*8 matrix where 2 is the number of documents, and 8 is the number of features\n",
    "\n",
    "The presence feature matrix for our toy data is: `[[1,1,1,0,1,1,1,1],[0,0,1,1,0,1,0,0]]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liYnqiYFReYh"
   },
   "source": [
    "### ‚ùì Quiz  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cBV4TfZReYi"
   },
   "source": [
    "Following the examples, please write below both the unigram presence and unigram frequency feature vectors for the document text 'the red dog and the red apple'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tP2Mwd1MReYj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t-UZXkuReYo"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>Unigram presence feature vector: [1,0,1,1,0,1,1,0]</p>\n",
    "    <p>Unigram frequency feature vector: [1,0,2,1,0,2,1,0]</p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5jdWB3QfSFtq"
   },
   "source": [
    "### ‚ùì Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nav-EVdhSFtq"
   },
   "source": [
    "What is the matrix consisting of the unigram presence vectors of the following documents:\n",
    "\n",
    "document1: 'the apple is red and the banana is yellow'\n",
    "\n",
    "document2: 'the red dog and the red apple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO1HIx0uSFtq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOXnAGI2SFtq"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p>The matrix is: [[1,1,1,0,1,1,1,1], [1,0,2,1,0,2,1,0]] </p>\n",
    "\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xd8XWczOReY1"
   },
   "source": [
    "## Numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAgQIXIy2zNH"
   },
   "source": [
    "The feature vectors are commonly represented as `numpy` arrays in Python. Let's spend some time to understand what `numpy` is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQhMs1Q1ReY1"
   },
   "source": [
    "A `numpy` array is just like a `list` but with smaller memory and faster access. \n",
    "\n",
    "Below, we introduce several ways to create a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcbIondQReY2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Create a numpy array of zeros with size 2\n",
    "vector1=np.zeros(2)\n",
    "# create a numpy array from a list [1,2,3]\n",
    "vector2=np.array([1,2,3])\n",
    "# create an empty array of size 3 with arbitary data\n",
    "vector3=np.empty(3)\n",
    "print ('vector1',vector1)\n",
    "print ('vector2',vector2)\n",
    "print ('vector3',vector3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTzXgRTuReY6"
   },
   "source": [
    "We can also concatenate two numpy arrays of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSNpS0TJReY7"
   },
   "outputs": [],
   "source": [
    "np.concatenate((vector2,vector3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq3Do1UxReZE"
   },
   "source": [
    "So far, we have created numpy arrays of one dimension (i.e. an array of numbers). Let's try creating a 2-D array (also called a matrix, or a nested array). We can pass dimension size (also called axes) as (a,b) where a is the number of rows in the matrix, and b specifies the number of columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzQII4NNReZF"
   },
   "outputs": [],
   "source": [
    "matrix1=np.zeros((3,3)) \n",
    "# this is a matrix of zeros that has 3 vectors, and within each vector there are 4 items. \n",
    "print ('matrix1',matrix1)\n",
    "# a matrix from nested list\n",
    "matrix2=np.array([[1,2,3],[2,3,4]])\n",
    "print ('matrix2',matrix2)\n",
    "# an empty matrix usually used as initialisation. It will print as an empty list\n",
    "matrix3=np.empty((0,4))\n",
    "print ('matrix3',matrix3)\n",
    "#To check the axes of an array, you can retrieve the shape attributes as tuple of dimension sizes like this:\n",
    "print (matrix2.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJI7IUkMReZJ"
   },
   "source": [
    "Numpy arrays are mutable. Therefore, we can change values in the vector. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ky4YPO0ReZL"
   },
   "outputs": [],
   "source": [
    "vector2[0]=0 # change the first item in vector3 to 0\n",
    "print (vector2)\n",
    "matrix2[0][2]=0 # change the third item of the first vector to 0\n",
    "print (matrix2)\n",
    "matrix2[0]=vector2 # change the first vector in matrix2 to vector3\n",
    "print (matrix2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjSFnyhwReZP"
   },
   "source": [
    "We can also slice a numpy array with an index array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7r9u1f_TReZS"
   },
   "outputs": [],
   "source": [
    "#Let's select the values at index 1,2 of vector2\n",
    "vector2[[1,2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0rSLnjtReXp"
   },
   "source": [
    "## Extracting unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qJxTridReXq"
   },
   "source": [
    "Now let's extract unigram features from our data. First, let‚Äôs load one document and look at the raw tokens split by white space. We will use the `load_doc()` function developed in the previous section. We can use the `split()` function to split the loaded document into unigram tokens separated by white space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3eSu9nvReXr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the document\n",
    "filename = 'data/neg/cv000_29416.txt'\n",
    "text = load_doc(filename)\n",
    "# split into tokens by white space\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeYaaXFDReX5"
   },
   "source": [
    "To keep track of the frequency and presence for each token in the document, we will use the `Counter` dictionary from `collections` module. Let's write a function `tokens_to_dict()` that turns a list of unigram tokens into a counter dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVTG7u5FReX9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def tokens_to_dict(tokens):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens: a list of tokens in a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary that records the number of occurrence for each token in a document\n",
    "    \"\"\"\n",
    "    token2count=Counter()\n",
    "    for token in tokens:\n",
    "        token2count[token]+=1\n",
    "    return token2count\n",
    "\n",
    "tokens_dict=tokens_to_dict(tokens)\n",
    "print (tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuCta1E-ReYC"
   },
   "source": [
    "We can put all above preprocessing steps into a function `clean_doc_unigrams()`. This function will preprocess the data and extract unigram tokens from the document. We then test it on another review, this time a positive review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6UgUxOZReYD"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_unigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: text from a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary of tokens\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    tokens_dict=tokens_to_dict(tokens)\n",
    "    return tokens_dict\n",
    " \n",
    "# load the document\n",
    "filename = 'data/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens_dict = clean_doc_unigrams(text)\n",
    "print(tokens_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9__t3RWKReYI"
   },
   "source": [
    "Finally, we can integrate the above preprocessing `clean_doc_unigrams()` into the data processing pipeline for all the files in a directory. We do so by the function `process_docs_unigram()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uA4RU1KRReYK"
   },
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_docs_unigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of unigram token counter dictionary where each token dictionary records the frequency of each token that occur in a document. \n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    print (directory)\n",
    "    tokens_all=[]\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict = clean_doc_unigrams(text)\n",
    "        tokens_all.append(tokens_dict)\n",
    "    return tokens_all\n",
    "\n",
    "\n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvNjVd19SFtr"
   },
   "source": [
    "## Turn text to feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPWBwd2jReYS"
   },
   "source": [
    "Now that we have the count of each present unigrams for each document, we can convert these unigram counts into feature vectors. But before that, we first need to collect all the features to establish the size of the feature vector. Here, the features are unigram words in the vocabulary. To do this, we define a function `collect_vocab()` to collect all the unique words in the vocabulary from `unigrams_all`, the list of token dictionary from all the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rw30VUGkReYZ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def collect_vocab(tokens_all):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document. \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a set of unique tokens in the vocabulary of tokens_all\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab=set() #Here, we create a `set()` to store all the unique words.\n",
    "    for token_lst in tokens_all: # iterate through token dictionary for each document\n",
    "        for token in token_lst:\n",
    "            # add a word in the vocab set. If a word already exists in vocab, it will not be added twice.\n",
    "            vocab.add(token)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# Let's check how many words we have in the vocab\n",
    "print (len(unigram_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kXVnFnpReYp"
   },
   "source": [
    "Now let's create the mapping between vocabulary and feature vecor position index from `unigram_vocab`. We can define a function `create_vocab_feature_mappings()` to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX4zO6tGReYq"
   },
   "outputs": [],
   "source": [
    "def create_vocab_feature_mappings(vocab):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab: a set of unique features in the vocabulary\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    vocab2index: a mapping dictionary with feature as key and position index as value\n",
    "    index2vocab: a mapping dicitoanry with position index as key and feature as value\n",
    "    \"\"\"\n",
    "    vocab2index={}\n",
    "    index2vocab={}\n",
    "    for i,w in enumerate(vocab): # iterate through the words in the vocabulary\n",
    "        vocab2index[w]=i\n",
    "        index2vocab[i]=w\n",
    "    return vocab2index,index2vocab\n",
    "\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRuZAgH3ReYv"
   },
   "source": [
    "Now, we can use `unigram2index` mapping dictionary to turn each document's token dictionary representation into a unigram feature vector. Remember, we can either represent in each vector item the freqency of the words, or use 1 or 0 to represent whether a word occurs or not. \n",
    "Let's design two functions `create_feature_presence()` and `create_feature_frequency()`  to turn the token dictionary for each document into these two types of features. \n",
    "\n",
    "To create the `create_feature_presence()`, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVKBHpEqReYw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_feature_presence(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=1\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1gqUZ4XReZa"
   },
   "source": [
    "**<h3>üíª Try it yourself!</h3>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWuzZ5KpReZb"
   },
   "source": [
    "Can you try implementing the function`create_feature_frequency()` to extract unigram frequency? (You can modify on the basis of `create_feature_presence()`)\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK92gQ75ReZc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3W3TtcJReZh"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    \n",
    "<code>\n",
    "   def create_feature_frequency(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=token_dict[w]\n",
    "    return vector\n",
    "</code>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziNNYlhj2zNJ"
   },
   "outputs": [],
   "source": [
    "def create_feature_frequency(token_dict,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    token_dict: a token counter dictionary for a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "    Return\n",
    "    ------\n",
    "    a feature vector for the document\n",
    "    \"\"\"\n",
    "    # create a numpy array with size of the vocabulary size\n",
    "    vector=np.zeros(len(vocab2index))\n",
    "    for w in token_dict:\n",
    "        index=vocab2index[w]\n",
    "        vector[index]=token_dict[w]\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDhe6PPjReZi"
   },
   "source": [
    "Now let's join the dots to create the function `create_feature_presence_all()` to loop over the unigrams from all the documents in `unigrams_all` and convert the token dictionary in each document to feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSB84BTTReZj"
   },
   "outputs": [],
   "source": [
    "def create_feature_presence_all(tokens_all,vocab2index):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "    vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a presence feature vector for the document\n",
    "    \"\"\"\n",
    "   \n",
    "    # since we know the number of documents, and the vocabulary size, we can initialize our result matrix as an empty matrix (2-D array) with the shape of (number of document, vocabulary size)\n",
    "    \n",
    "    presence_features=np.empty((len(tokens_all),len(vocab2index))) #initialize the result array as an empty array ready to be appended through the loop. \n",
    "    for doc_i,token_dict in enumerate(tokens_all):\n",
    "        # convert bag of word dictionary in each document into unigram features\n",
    "        presence_feature=create_feature_presence(token_dict,vocab2index)\n",
    "        # We assign the unigram fearture of the current document to the correct positon in the unigram_presence_result matrix. \n",
    "        presence_features[doc_i]=presence_feature\n",
    "    return presence_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z4WeZ1bReZm"
   },
   "source": [
    "Now we are ready to extract features from unigrams collected from all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC6L5sQMReZm",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUwcH3xkReZv"
   },
   "source": [
    "**<h3>üíª Try it yourself!</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_hJeToEReZw"
   },
   "source": [
    "Can you follow the code above to extract all the documents' unigram frequency features (using the `create_feature_frequency()` functions you defined in quiz 3)? You can name your function as `create_feature_frequency_all()`. Please store the features into the variable named as `features_unigram_frequency`. \n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS_N3Uqb2zNK"
   },
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    \n",
    "<code>\n",
    "   def create_feature_frequency_all(tokens_all,vocab2index):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "        vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "        Return\n",
    "        ------\n",
    "        a frequency feature vector for the document\n",
    "        \"\"\"\n",
    "        frequency_features=np.empty((len(tokens_all),len(vocab2index))) \n",
    "        for doc_i,token_dict in enumerate(tokens_all):\n",
    "            frequency_feature=create_feature_frequency(token_dict,vocab2index)\n",
    "            frequency_features[doc_i]=frequency_feature\n",
    "        return frequency_features\n",
    "</code>\n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmJ8Z94Z2zNK"
   },
   "outputs": [],
   "source": [
    " def create_feature_frequency_all(tokens_all,vocab2index):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        tokens_all: a list of token dictionaries where each token dictionary is extracted from a document\n",
    "        vocab2index: a mapping dictionary with feature as key and feature index as value\n",
    "        Return\n",
    "        ------\n",
    "        a frequency feature vector for the document\n",
    "        \"\"\"\n",
    "        frequency_features=np.empty((len(tokens_all),len(vocab2index))) \n",
    "        for doc_i,token_dict in enumerate(tokens_all):\n",
    "            frequency_feature=create_feature_frequency(token_dict,vocab2index)\n",
    "            frequency_features[doc_i]=frequency_feature\n",
    "        return frequency_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kvb5s1w-ReZ2"
   },
   "source": [
    "### ‚ùì Quiz  \n",
    "\n",
    "Let's check the first review's unigram presence features, can you use the mapping in `index2unigram` to reveal what unigrams are present in this review? \n",
    "Please answer: Which words in the following list are present?\n",
    "A. gayness\n",
    "B. fabulous\n",
    "C. snappiness\n",
    "D. happiness\n",
    "\n",
    "You can write your code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU52Lmt7ReZ6"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <p> Code:</p>\n",
    "    <code>wordlist=[index2vocab[i] for i,item in enumerate(unigram_presence_pos[0]) if item==1]</code>\n",
    "    <p> A,C are present in the review </p>\n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIOfxwtuSFtr"
   },
   "source": [
    "## Encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEDH7-WERea3"
   },
   "source": [
    "At the same time, we should also find a way to represent the labels (positive or negative) in numeric ways for the model to train on. Let's create numpy arrays of labels of 1s and 0s. Let's say 1 corresponds to positive reviews and 0 corresponds to negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mhBsENESFtr"
   },
   "source": [
    "Remember we concatenate positive and negative data when producing `unigrams_all` and `features_unigram_presence`, we will follow the same data order to create the numpy array of labels that tell us the sentiment for each feature vector in `features_unigram_presence`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K12juH1zRea4"
   },
   "outputs": [],
   "source": [
    " \n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGNbpwvAReZ7"
   },
   "source": [
    "## Save the features and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntPhUTCrReZ7"
   },
   "source": [
    "We can also save the prepared features and labels. \n",
    "\n",
    "This is a good practice as it decouples the data preparation from modeling, allowing you to focus on modeling and circle back to data preparation if you have new ideas.\n",
    "\n",
    "To store a numpy array, we can use the `numpy`'s `save()` function. `save()` takes two arguments: the first is the file to be written, and the second argument is the numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tE3m4vnpReZ9"
   },
   "outputs": [],
   "source": [
    "\n",
    "np.save('features_unigram_presence.npy',features_unigram_presence)\n",
    "np.save('labels_unigram_presence.npy',labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL_Qwr_1ReaC"
   },
   "source": [
    "We can load the postiive data from the files by:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBEc1d1DReaC"
   },
   "outputs": [],
   "source": [
    "np.load('features_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgQWG19jReak"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OD_SGP7aReak"
   },
   "source": [
    "## Prepare train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuHpK9tfReak"
   },
   "source": [
    "Now it's time to prepare this dataset for model evlauation. We want to train a model that is generalisable to unseen data. Therefore, we can split the dataset into train and test where the model is trained on the train set and tested on the unseen test set. Usually the train-test split is 70% and 30%. \n",
    "\n",
    "We will follow the paper to adopt a 3-fold cross validation. We will use `scikit-learn`'s `KFold` to do that. In addition, to ensure that we have an equal number of positive and negative examples, we make the split in both positive and negative datasets respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpmPH4YkReal"
   },
   "source": [
    "Let's first load the unigram presence features and their labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53Ki9zrpSFts"
   },
   "outputs": [],
   "source": [
    "data=np.load('features_unigram_presence.npy')\n",
    "labels=np.load('labels_unigram_presence.npy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyHUagn2SFts"
   },
   "source": [
    "produce the indices for the positive and the negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5VANDk0SFts"
   },
   "outputs": [],
   "source": [
    "posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "neg_id=[i for i,label in enumerate(labels) if label==0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq43x_gVSFts"
   },
   "source": [
    "Initialize `KFold` from `scikit-learn` and generate cross validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7FSzqD_4SFts"
   },
   "outputs": [],
   "source": [
    "# import KFold and random\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "\n",
    "# set the number of folds\n",
    "num_folds=3\n",
    "\n",
    "# set random seed for cross validation\n",
    "CROSS_VAL_SEED = 42\n",
    "\n",
    "# create the KFold object and create the splits for positive data\n",
    "kf_posi = KFold(n_splits=num_folds)\n",
    "kf_posi.get_n_splits(posi_id)\n",
    "kf_posi = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "# create the KFold object and create the splits for negative data\n",
    "kf_neg = KFold(n_splits=num_folds)\n",
    "kf_neg.get_n_splits(posi_id)\n",
    "kf_neg = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "# loop over the folds to create train and test data/label. \n",
    "\n",
    "for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "    print(f'Running fold {fold_idx+1}...')\n",
    "    \n",
    "    # Get the current fold for positive data\n",
    "    fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "    fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "    \n",
    "    # Get the current fold for negative data\n",
    "    train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "    fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "    fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "    \n",
    "    # ensure that we have balanced classes in both train and test\n",
    "    assert len(fold_train_posi)==len(fold_train_neg)\n",
    "    assert len(fold_test_posi)==len(fold_test_neg)\n",
    "    # combine all train and test for the current fold\n",
    "    fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "    fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "    \n",
    "    # use the indexes in fold_train and fold_test to slice data and labels\n",
    "    fold_train_data=data[fold_train]\n",
    "    fold_test_data=data[fold_test]\n",
    "    fold_train_label=labels[fold_train]\n",
    "    fold_test_label=labels[fold_test]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49bNBGnESFts"
   },
   "source": [
    "Again, we can wrap the above into a function `create_kfold_validadtion()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yU1YyUi7SFts"
   },
   "outputs": [],
   "source": [
    "def create_kfold_validation(data,labels,num_folds):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: a numpy array of features\n",
    "    labels: a numpy array of labels (1=positive, 0=negative)\n",
    "    num_folds: the number of folds in cross validation\n",
    "\n",
    "    Yields (works as a generator)\n",
    "    ------\n",
    "    fold_train_data: current fold of training data as numpy arrays\n",
    "    fold_test_data: current fold of test data as numpy arrays\n",
    "    fold_train_label: current fold of training labels as numpy arrays\n",
    "    fold_test_label: current fold of test labels as numpy arrays\n",
    "    \"\"\"\n",
    "    #produce the indices for the positive and the negative data\n",
    "    posi_id=[i for i,label in enumerate(labels) if label==1]\n",
    "    neg_id=[i for i,label in enumerate(labels) if label==0]\n",
    "\n",
    "    # set random seed for cross validation\n",
    "    cross_val_seed = 42\n",
    "\n",
    "    # create the KFold object and create the splits for positive data\n",
    "    kf_posi = KFold(n_splits=num_folds)\n",
    "    kf_posi.get_n_splits(posi_id)\n",
    "    kf_posi = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "    posi_folds=list(kf_posi.split(posi_id))\n",
    "\n",
    "    # create the KFold object and create the splits for negative data\n",
    "    kf_neg = KFold(n_splits=num_folds)\n",
    "    kf_neg.get_n_splits(posi_id)\n",
    "    kf_neg = KFold(n_splits=num_folds, random_state=CROSS_VAL_SEED, shuffle=True)\n",
    "    neg_folds=list(kf_neg.split(neg_id))\n",
    "\n",
    "    # loop over the folds to create train and test data/label. \n",
    "\n",
    "    for fold_idx, (train_index_posi, test_index_posi) in enumerate(posi_folds):\n",
    "\n",
    "        print(f'Running fold {fold_idx+1}...')\n",
    "\n",
    "        # Get the current fold for positive data\n",
    "        fold_train_posi = np.array(posi_id)[train_index_posi]\n",
    "        fold_test_posi = np.array(posi_id)[test_index_posi]\n",
    "\n",
    "        # Get the current fold for negative data\n",
    "        train_index_neg,test_index_neg=neg_folds[fold_idx]\n",
    "        fold_train_neg=np.array(neg_id)[train_index_neg]\n",
    "        fold_test_neg=np.array(neg_id)[test_index_neg]\n",
    "\n",
    "        # ensure that we have balanced classes in both train and test\n",
    "        assert len(fold_train_posi)==len(fold_train_neg)\n",
    "        assert len(fold_test_posi)==len(fold_test_neg)\n",
    "        # combine all train and test for the current fold\n",
    "        print ('combining positive and negative for train and test')\n",
    "        fold_train=np.concatenate([fold_train_posi,fold_train_neg])\n",
    "        fold_test=np.concatenate([fold_test_posi,fold_test_neg])\n",
    "        \n",
    "        \n",
    "        # use the indexes in fold_train and fold_test to slice data and labels\n",
    "        print ('slice training and test data')\n",
    "        fold_train_data=data[fold_train]\n",
    "        fold_test_data=data[fold_test]\n",
    "        fold_train_label=labels[fold_train]\n",
    "        fold_test_label=labels[fold_test]\n",
    "        print ('yield')\n",
    "        \n",
    "        assert len(fold_train_data)==len(fold_train_label)\n",
    "        assert len(fold_test_data)==len(fold_test_label)\n",
    "        yield fold_train_data,fold_test_data,fold_train_label,fold_test_label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgmXWpxVSFts"
   },
   "source": [
    "When we call `create_kfold_validation()`, we are initializing a generator. We can loop over the generator to produce `fold_train_data,fold_test_data,fold_train_label,fold_test_label`  for every fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km38EdLmSFts"
   },
   "outputs": [],
   "source": [
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(np.array(data),labels,num_folds=3):\n",
    "    print ('length of train data',len(fold_train_data))\n",
    "    print ('length of test data',len(fold_test_data))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2ScVYh9RebR"
   },
   "source": [
    "## Naive Bayes Model and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnKOr56PRebS"
   },
   "source": [
    "Let's try fitting the features and labels from each validation fold into a naive bayes model using the `MultinomialNB` package in `sklearn`. `MultinomialNB` is a Naive Bayes classifier for multinomial models (i.e. for multiclass problems). It implements additive smoothing by default. For more details, please see [scikit-learn's multinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04CNQg8eRebT"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(fold_train_data,fold_train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(fold_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DHgKdlJRebW"
   },
   "source": [
    "In each fold, `predicted` is an array of predictions in 1s and 0s. Let's compare it with the gold labels and calculate accuracy following:\n",
    "\n",
    "<code>accruracy= number of correct examples/number of total examples</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2W8h1ByjReaY"
   },
   "source": [
    "**<h3>üíª Try it yourself!</h3>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8ADZKR1ReaZ"
   },
   "source": [
    "Please complete the function `get_accuracy()` below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JHhMYkawReaa"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(predicted,gold):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    predicted: a list or numpy array of predicted labels in the form of 1=positive, 0=negative\n",
    "    gold: a list or numpy array of gold labels in the form of 1=positive, 0=negative\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    accuracy score\n",
    "    \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VgTk8uTReac"
   },
   "source": [
    "\n",
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "    <code>\n",
    "    def get_accuracy(predicted,gold):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        predicted: a list or numpy array of predicted labels in the form of 1=positive, 0=negative\n",
    "        gold: a list or numpy array of gold labels in the form of 1=positive, 0=negative<br>\n",
    "        Return\n",
    "        ------\n",
    "        accuracy score\n",
    "        \"\"\"\n",
    "        correct=0\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted[i]==gold[i]: #if the predicted result is the same with the gold label\n",
    "                correct+=1\n",
    "        acc=correct/len(predicted)\n",
    "        return acc\n",
    "        </code>\n",
    "  \n",
    "\n",
    "   \n",
    "\n",
    "<!--   <p>\\begin{equation*}P(Fire|Smoke) =  \\frac{P(Fire)*P(Smoke|Fire)}{P(Smoke)} \\end{equation*} </p>\n",
    "<p>\\begin{equation*}=\\frac{0.01 * 0.9}{ 0.1} \\end{equation*}</p>\n",
    "<p>\\begin{equation*}=0.09\\end{equation*}</p> -->\n",
    "\n",
    "</details> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfCvRGRhRebW"
   },
   "source": [
    "Let's create a function that calls the naive bayes model and performs evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4SstOdiRebc"
   },
   "outputs": [],
   "source": [
    "def train_evaluate(train_data,train_label,test_data,test_label):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data: numpy array of train data (number of examples * number of features)\n",
    "    train_label: numpy array of train labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "    test_data: numpy array of test data (number of examples * number of features)\n",
    "    test_label: numpy array of test labels in the form of [1,0,1...] where 1 = positive, 0 = negative\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    accuracy score on the test data\n",
    "    \"\"\"\n",
    "    # initialize a multinomial naive bayes model\n",
    "    model = MultinomialNB()\n",
    "    # fit the model with features and labels for the training data\n",
    "    model.fit(train_data,train_label)\n",
    "    #evaluate the fitted model on the test set\n",
    "    predicted=model.predict(test_data)\n",
    "    #evaluation:\n",
    "    acc=get_accuracy(predicted,test_label)\n",
    "\n",
    "\n",
    "    print ('accuracy',acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CFSUKl5Rebf"
   },
   "source": [
    "Now integrating `train_evaluate()` into cross validation, we will get accuracy per each fold. We then take the average and standard deviation of the accuracy scores across folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxNffzRuRebg"
   },
   "outputs": [],
   "source": [
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEadQarHSFts"
   },
   "source": [
    "If everything is correct, you should get:\n",
    "\n",
    "    Mean accuracy      : 0.82000\n",
    "    Standard deviation : 0.00657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyiIrS26SFts"
   },
   "source": [
    "# Recap: The whole pipeline to train on unigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDZjB70OSFts"
   },
   "outputs": [],
   "source": [
    "# 1. Extract unigrams from all documents\n",
    "from os import listdir \n",
    "# unigrams for all the negative files\n",
    "unigrams_neg=process_docs_unigrams('./data/neg')\n",
    "# unigrams for all the positive files\n",
    "unigrams_posi=process_docs_unigrams('./data/pos')\n",
    "# all unigrams\n",
    "unigrams_all=unigrams_posi+unigrams_neg\n",
    "\n",
    "# 2. Turn text to feature vectors\n",
    "# collect all unique unigram as all features\n",
    "unigram_vocab=collect_vocab(unigrams_all)\n",
    "# map unigram features to vector position indexes\n",
    "unigram2index,index2unigram=create_vocab_feature_mappings(unigram_vocab)\n",
    "# Convert unigrams to feature vectors for all documents\n",
    "features_unigram_presence=create_feature_presence_all(unigrams_all,unigram2index)\n",
    "\n",
    "# 3. Encode labels\n",
    "labels=len(unigrams_posi)*[1]+len(unigrams_neg)*[0]\n",
    "labels=np.array(labels)\n",
    "\n",
    "# 4. Evaluation with 3-fold cross validation\n",
    "data=features_unigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbwE6ybqSFtt"
   },
   "source": [
    "# Other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nhzv2aDmReaG"
   },
   "source": [
    "## Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fOlUBWYReaH"
   },
   "source": [
    "Based on `clean_doc_unigrams()` that turns a document into a unigram dictionary, we can create a function `clean_doc_bigrams()` to extract bigram dictionary. (You can refresh yourself of how to create a bigram counter dictionary in module 1.4. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hehFhHqgReaI"
   },
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_doc_bigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: string of the document text\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a counter dictionary with bigram as key and count as value\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "  \n",
    "    # extract bigrams\n",
    "    bigram_dict=Counter() #initialize a bigram dictionary to be updated\n",
    "    tokens=['<start>']+tokens+['<end>'] # add <start> and <end> token\n",
    "    for i in range(len(tokens)): #loop over all the indices of the token list\n",
    "        if i<len(tokens)-1: #if it's not the end of the token list\n",
    "            bigram_current=(tokens[i],tokens[i+1])\n",
    "            bigram_dict[bigram_current]+=1\n",
    "    return bigram_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzesiKPtReaK"
   },
   "source": [
    "Now we can replace the `clean_doc_unigrams()` line with `clean_doc_bigrams()` in `process_docs_unigrams()`, we will rename the function as `process_docs_bigrams()` that counts all the bigrams in the documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpo6ay2bReaL"
   },
   "outputs": [],
   "source": [
    "def process_docs_bigrams(directory):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: a directory containing positive/negative samples from the Thumbs\n",
    "    Up! dataset.\n",
    "    overall_bigrams_counter: \n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    a list of bigram counters each representing a document\n",
    "    \"\"\"\n",
    "    # walk through all files in the folder\n",
    "    tokens_all=[]\n",
    "    print (directory)\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        text=load_doc(path)\n",
    "        # clean documents\n",
    "        tokens_dict_current = clean_doc_bigrams(text)\n",
    "        tokens_all.append(tokens_dict_current)\n",
    "    return tokens_all\n",
    "bigrams_neg=process_docs_bigrams('./data/neg')\n",
    "bigrams_posi=process_docs_bigrams('./data/pos')\n",
    "bigrams_all=bigrams_posi+bigrams_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1KBMkOFReaN"
   },
   "source": [
    "We can follow the same procedure to turn bigram counters per each document into bigram feature presence vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_wqVaJPSFtt"
   },
   "outputs": [],
   "source": [
    "bigram_vocab=collect_vocab(bigrams_all)\n",
    "bigram2index,index2bigram=create_vocab_feature_mappings(bigram_vocab)\n",
    "features_bigram_presence=create_feature_presence_all(bigrams_all,bigram2index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCk487cQSFtt"
   },
   "source": [
    "The labels are the same for `features_unigram_presence`. We can then directly train Naive Bayes model with these bigram presence features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9vapFuJSFtt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=features_bigram_presence\n",
    "acc_list=[]\n",
    "for fold_train_data,fold_test_data,fold_train_label,fold_test_label in create_kfold_validation(data,labels,num_folds=3):\n",
    "    acc=train_evaluate(fold_train_data,fold_train_label,fold_test_data,fold_test_label)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "# mean and standard deviation\n",
    "print(f'Mean accuracy      : { np.mean(acc_list):.5f}')\n",
    "print(f'Standard deviation : { np.std(acc_list):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJKnfmc72zNN"
   },
   "source": [
    "If everything goes right, you should see the following:\n",
    "\n",
    "Mean accuracy      : 0.85150<br>\n",
    "Standard deviation : 0.00414"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCaLlXqgRebo"
   },
   "source": [
    "# ‚ùìFinal Quiz  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbpA7tuVRebo"
   },
   "source": [
    "Part 1: So far we have trained a model and evaluate on the bigram and unigram presence features, can you modify the pipeline to build a model on bigram frequency features with three fold cross validation? Please report mean and accuracy of the results with CROSS_VAL_SEED=42. How do the results compare with unigram presence features?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHJ2j6agSFtt",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL8CYT4sSFtt"
   },
   "source": [
    "Part 2: A potentially important contextual effect is from negation: clearly ‚Äúgood‚Äù and ‚Äúnot good‚Äù indicate opposite sentiment orientations. Let's try adding the tag NOT to every word between a negation word (‚Äúnot‚Äù, ‚Äúisn‚Äôt‚Äù, ‚Äúdidn‚Äôt‚Äù, etc.) and the first punctuation mark following the negation word. We have provided the functioon `clean_doc_neg_unigrams()` to extract unigrams with negation tags for each document. Can you integrate the function into the pipeline to run models on unigram presence features with negation tags? What is the mean and std of the accuracy averaged over three-fold cross validation? How does it compare with the results from unigram presence features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JbaicwbZSFtt"
   },
   "outputs": [],
   "source": [
    "NEGATION_WORDS=[\"not\",\"n't\"]\n",
    "def clean_doc_neg_unigrams(doc):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: text from a document\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A counter dictionary of tokens with negation tag\n",
    "    \"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    neg_tag=False # whether to add neg tag\n",
    "    tokens_negtag=[]\n",
    "    for token in tokens:\n",
    "        if token in NEGATION_WORDS:\n",
    "            neg_tag=True\n",
    "        elif token in [',','.',':',')','(','\"','-','!','?']:\n",
    "            neg_tag=False # stop adding negation tag to tokens after the punctuation mark\n",
    "        else: # for other words\n",
    "            if neg_tag:\n",
    "                token='NEG_'+token # adding negation tag\n",
    "        tokens_negtag.append(token)\n",
    "    tokens_dict=tokens_to_dict(tokens_negtag)\n",
    "    return tokens_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjUeuxiTSFtt",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oYrdS1fSFtt"
   },
   "source": [
    "# Additional resources\n",
    "\n",
    "\n",
    "\n",
    "- Naive Bayes chapter from [Manning, Raghavan and Sch√ºtze's Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)\n",
    "-  `scikit-learn`'s [implementation of Naive Bayes](https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/naive_bayes.py#L669)\n",
    "- Pang, Lee, Vaithyanathan, 2002, Thumbs up? Sentiment Classification using Machine Learning Techniques [pdf](https://www.aclweb.org/anthology/W02-1011.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_2.1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
