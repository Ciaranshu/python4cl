{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlGf-HQ3DO2A"
   },
   "source": [
    "<div style='background-image: url(\"../../resources/section_header.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: x-large ; font-weight: 900 ; color: rgba(0 , 115 , 207 , 0.9) ; line-height: 100%\">Python for Computational Linguists</div>\n",
    "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 115 , 207, 0.7)\">2.3 Part-of-speech Tagging</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJewFf6DDO2C"
   },
   "source": [
    "# Module 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHXX_pVRDO2F"
   },
   "source": [
    "We agreed to be flexible on the content and just to include as much as could comfortably fit into 2 hours with the potential for extensional activities.\n",
    " material from http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf\n",
    " data from\n",
    "http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html\n",
    "\n",
    "* Summary of key objectives\n",
    "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
    "** understand that part of speech tagging is an important early application of sequence classification;\n",
    "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
    "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
    "\n",
    "* Quick introduction (with material taken from the NLTK book and the lecture notes):\n",
    "** What is part of speech tagging?\n",
    "** What is a tagged corpus?\n",
    "** Using spaCy’s part of speech tagger to process a sequence of words\n",
    "Quick reminder of how to use spaCy’s POS tagger and some of the issues we raised, e.g. about domain adaptation, data quantity and quality. E.g. get the students to POS tag two contrasting sentences, e.g. lexically ambiguous homophones - one with a common homonym sense and another with a rare homonym sense (e.g. ‘It is wrong to object to this object’ or ‘I must present the present on his birthday’, ‘The insurance for the invalid was invalid’), or one sentence with a neologism and one without a neologism or one with a nonsense word/unknown word that is not a common noun (e.g. ‘he was scrobbling’). Encourage the students to play with the tagger by submitting their own challenging sentences.  Get the students to notice the set of POS tags and their meanings.  Get the students to see what happens if you normalise the text to lower case. \n",
    "In Module 1 we saw how to call spaCy’s POS tagger using a function call.  Whilst spaCy’s POS tagger is very good, we’re going to be going hands on by building our own POS simple n-gram tagger using the NLTK library.\n",
    "* Today’s goal: building and testing a part of speech tagger with NLTK\n",
    "* Getting started\n",
    "** Exploring tagged corpora (section 5.2 in the NLTK book)\n",
    "Look at several POS tagged corpora, e.g. the Brown corpus and the GENIA corpus (see data at http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html). \n",
    "Try to get the students thinking about the POS tag set – what do the categories mean?  Get the students to explore the corpora and gain insights, e.g. most frequent POS tag, least frequent POS tag, plotting the curve of the POS tag frequency distribution.  What kinds of words occur in the noun, adjective, adverb category?  \n",
    "Look at some n-gram sequences of POS tags to get the students thinking about n-grams.\n",
    "Look at some words which are POS ambiguous.\n",
    "** Mapping words to properties using Python dictionaries (section 5.3 in NLTK)\n",
    "Get the students thinking about linguistic objects as data structures, e.g. a dictionary could map from a headword to a POS or to a specific sense, e.g. in WordNet or Wikipedia. \n",
    "Dictionaries in Python again. Defining a POS dictionary by hand.  Creating a POS dictionary using a tagged corpus. \n",
    "** Creating a bigram dictionary\n",
    "Page 196 in the NLTK book shows how to create a dictionary of words and tags to perform bigram tagging;\n",
    "If we try to create an n-gram tagger for larger values of n what will happen?  We’ll encounter the sparse data problem and also start to reach the limits of computer memory.\n",
    "* Automatic tagging (section 5.4 in the NLTK book)\n",
    "** Separating training and testing data\n",
    "From page 225 in the NLTK book – uses the Brown corpus to make a 90:10 split\n",
    "Get the students to fill in a LaTeX table that characterises the data: distribution of tags in the whole corpus, in the 90% training set and the 10% testing set.\n",
    "** A simple default tagger \n",
    "Assigns the NN to each word in the Brown training corpus\n",
    "Evaluate it using the Brown testing corpus with recall, precision and F1 for each category\n",
    "Get the students to measure the out of vocabulary rate between the training and test data sets\n",
    "-\tAsk the students to report the number of types and tokens in the 90% training set, the 10% testing set and the OOV rate.\n",
    "Get the students to look and learn from the evaluation data\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "** A regular expression tagger  cut out\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "Again encourage the students to look and learn from the evaluation data\n",
    "Get the students to construct new regular expression patterns and to evaluate them\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** The most frequent 100 unigram tagger\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "Again encourage the students to look and learn from the evaluation data\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** A simple bi-gram tagger\n",
    "Emphasize that a real n-gram tagger would do decoding using a dynamic programming algorithm called Viterbi (the students will have heard of this in the lecture).  Today in the interests of time we are not going to do decoding, we’re just going to assign the most frequent POS sequence to each bigram that matches (if indeed a bigram does match).\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "Low accuracy because of data sparsity – most bigrams were never seen during training.\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** Combining a models with backoff (e.g. unigram plus bigram tagger)\n",
    "** Further insights from evaluation using a confusion matrix\n",
    "* Reminder of key objectives\n",
    "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
    "** understand what a part of speech tag set is;\n",
    "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
    "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
    "\n",
    "* Homework: \n",
    "** (1 point): Run your bigram tagger on this made-up sentence ‘@Will WOOOHOO Will the New jPhone bOut 2night? Soexcited :)‘. Does the tagger get the correct answer?  Write down two challenges the tagger has incorrectly resolved.  looking here for an OOV word (WOOOHOOO) and  failure to disambiguate homonyms (e.g. Will).  The tagger may also not handle slang (2night), neologisms (jPhone) and tokenization errors (Soexcited).\n",
    "** (2 points): Write a program to find out:\n",
    "a.\twhich word has the most POS tags in the GENIA corpus.\n",
    "b.\twhat percentage of words in the GENIA corpus are ambiguous, i.e. have more than one POS tag.\n",
    "** (2 points):  Write a program to replace low frequency words (words with a frequency of 5 or less) with UNK in the Brown corpus.  How much does this improve your backoff tagger’s F-score performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ufigb6jFDO2H"
   },
   "source": [
    "This module is built based on the lecture note about part-of-speech tagging and Chapter 5 of the [book](http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf), and students are strongly recommended to go through them.\n",
    "\n",
    "In this module, we are going to\n",
    "- a\n",
    "- b\n",
    "- c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4k9F2MOdDO2J"
   },
   "source": [
    "# Python for Computational Linguists 1.4: Part-of-speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GgJipxvDO2L"
   },
   "source": [
    "### What is POS tag?\n",
    "For each word in context, we can assign a **lexical category**, such as noun, verbs, adjectives, etc.\n",
    "These categories are often referred to as a word's part-of-speech tag or POS tag.\n",
    "The set of all POS tags is called a **tagset**, and the activity of assigning these tags to words is referred to as **part-of-speech tagging** or simply as **POS tagging**.\n",
    "\n",
    "The number of categories we require for our tagset often depends on both linguistic and practical considerations.\n",
    "Very commonly used tagsets include the 87-tag Brown set, 45-tag Penn Treebank set, the 61-tag CLAWS 5 (C5), and the 17-tag Universal POS tagset. \n",
    "[Here](https://universaldependencies.org/u/pos/) is the list of tags of Universal POS tagset.\n",
    "Please check lecture note for more detials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n5rNkKVYDO2M"
   },
   "source": [
    "### Why do we want to do POS tagging?\n",
    "As the POS tag of a word will give a large amount of information about this word and its neighbours, POS tagging can help understanding better texts, beneficial to many dowstream NLP tasks such as:\n",
    "\n",
    "1. Disambiguating word senses. Consider the following example for the heterophone *content*:\n",
    "    1. There was very little *content* to the essay.\n",
    "    2. The sleepy pug puppy was very *content*.\n",
    "  \n",
    "  or homonyms:\n",
    "  - It is wrong to *object* to this *object*.\n",
    "  - I must *present* the *present* on his birthday.\n",
    "  - The insurance for the *invalid* was *invalid*.\n",
    "\n",
    "2. Mitigating the issue of data sparsity. \n",
    "    - Label named entities like people, places or organizations as part of information extraction systems. In the following example, both *Chase Manhattan* and *J.P. Morgan* should be detected as proper nouns:\n",
    "        - Chase Manhattan and its merger partner J.P. Morgan.\n",
    "    \n",
    "    - Deal with out-of-vocabulary (OOV) words such as neologism words or acronyms in the social media: \n",
    "        - @username its #awesome u gonna ♥ it Chk out our cooool project on some_url + RT it.\n",
    "\n",
    "\n",
    "More generally, being able to automatically perform POS tagging will help reduce the laborious human effort required to parse a sentence, and it will be the main goal of this module, which is to build such an automatic model/tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czrMx4pUDO2S"
   },
   "source": [
    "## ❓ Pre-module quiz\n",
    "\n",
    "TODO: any quiz more interactive????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIuXAYe_DO2U"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1De39wDDO2W"
   },
   "source": [
    "Before building our own tagger, we are going to use the NLTK library to load some existing tagger to perform POS tagging, and some tagged corpora will be introduced for further training and evaluations of our models.\n",
    "First let's import the [NLTK](https://www.nltk.org/) library and download some necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T6S2SodjDO2X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yz568/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a6rVLyc-DO2i"
   },
   "source": [
    "Given the input text, we first tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MZLMREilDO2j"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'this', 'module', 'and', 'NLTK', 'library', 'very', 'very', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "text = 'I like this module and NLTK library very very much!'\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMsX-kdtDO2q"
   },
   "source": [
    "We can see that the tokenized text is a list where each element is a token. \n",
    "\n",
    "Now let's tag the tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ynlt2jeDO2r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('like', 'VBP'), ('this', 'DT'), ('module', 'NN'), ('and', 'CC'), ('NLTK', 'NNP'), ('library', 'JJ'), ('very', 'RB'), ('very', 'RB'), ('much', 'JJ'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_text = nltk.pos_tag(tokenized_text)\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYmkVSXXDO2z"
   },
   "source": [
    "The returned tagged text is a list, where each element is a tuple. For each tuple, the first element is the previous token, and the second element is its corresponding tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3tWv279DO20"
   },
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g., `nltk.help.upenn_tagset('PRP')`, or a regular expression, e.g., `nltk.help.upenn_brown_tagset('NN.*')`. Some corpora have README files with tagset documentation; see `nltk.name.readme()`, substituting in the name of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "1_cqoTAsDO21"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "Use the current tagger to tag the previous text examples to see if the key words (heterophone, homonym, named entity and neologism) are correctly tagged.\n",
    "What are possible explanations to the wrong words?\n",
    "What happens if we normalise the text to lower case?\n",
    "Play with the tagger by submitting your own challenging sentences.\n",
    "\n",
    "- There was very little *content* to the essay.\n",
    "- The sleepy pug puppy was very *content*.\n",
    "- It is wrong to *object* to this *object*.\n",
    "- I must *present* the *present* on his birthday.\n",
    "- The insurance for the *invalid* was *invalid*.\n",
    "- They *refuse* to permit us to obtain the *refuse* permit\n",
    "- Chase Manhattan and its merger partner J.P. Morgan.\n",
    "- @username its #awesome u gonna ♥ it Chk out our cooool project on some_url + RT it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgDVZ4Q3DO24"
   },
   "source": [
    "TODO: have answers ready and ask students to identify any wrong ones, and ask them why? \n",
    "change the sentences to let the tagger change the decision wrong -> correct, correct -> wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzs8c8g1DO25"
   },
   "source": [
    "### Connection with Text Classification\n",
    "\n",
    "As we have seen in previous modules, text classification is the task where we assign a label to the whole sequence of text, e.g. document. POS tagging can be also viewd similarly. Instead of assigning a label on the document level, we assign a label to each word or token. This type of task is usually called **sequence labeling** or **sequence classification**, and POS tagging is one of the most important tasks of sequence classification in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBKhxCS4DO26"
   },
   "source": [
    "### Tagged corpora\n",
    "POS taggers using machine learning techniques generally require annotated data to train on.\n",
    "For all tagging models, including those that do not require any training data like regex- or rule-based models, an annotated evaluation dataset is also required to measure the performance of these models by comparing the model outputs with the gold standard annotations.\n",
    "\n",
    "Tagged corpora, generally tagged by an human expert, have the form similar to the following:\n",
    "\n",
    "```\n",
    "The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
    "```\n",
    "Different datasets might have different forms, but each word (token) in sentences will be generally accompanied by a POS tag to form the pair **word/tag**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "reStt-rwDO27"
   },
   "source": [
    "Let's explore the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) and the [Penn Treebank Corpus](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf) annotated with POS tags.\n",
    "Both corpora are already included in NLTK with POS tags, and we can get the full text by calling `tagged_words()` or `tagged_sents()` when the corpus is also segmented into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUODO8NMDO28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# read Brown Corpus\n",
    "print(nltk.corpus.brown.tagged_words())\n",
    "print(nltk.corpus.brown.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkbiwdJvDO3C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# read Penn Treebank Corpus\n",
    "print(nltk.corpus.treebank.tagged_words())\n",
    "print(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOwsxj-5DO3N"
   },
   "source": [
    "Did you notice that the tags used in the two corpora seem different? They are indeed using different tagsets, and to investigate what each tag category means, we can check both tagsets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-9yjw0ADO3P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(: opening parenthesis\n",
      "    (\n",
      "): closing parenthesis\n",
      "    )\n",
      "*: negator\n",
      "    not n't\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ? ; ! :\n",
      ":: colon\n",
      "    :\n",
      "ABL: determiner/pronoun, pre-qualifier\n",
      "    quite such rather\n",
      "ABN: determiner/pronoun, pre-quantifier\n",
      "    all half many nary\n",
      "ABX: determiner/pronoun, double conjunction or pre-quantifier\n",
      "    both\n",
      "AP: determiner/pronoun, post-determiner\n",
      "    many other next more last former little several enough most least only\n",
      "    very few fewer past same Last latter less single plenty 'nough lesser\n",
      "    certain various manye next-to-last particular final previous present\n",
      "    nuf\n",
      "AP$: determiner/pronoun, post-determiner, genitive\n",
      "    other's\n",
      "AP+AP: determiner/pronoun, post-determiner, hyphenated pair\n",
      "    many-much\n",
      "AT: article\n",
      "    the an no a every th' ever' ye\n",
      "BE: verb 'to be', infinitive or imperative\n",
      "    be\n",
      "BED: verb 'to be', past tense, 2nd person singular or all persons plural\n",
      "    were\n",
      "BED*: verb 'to be', past tense, 2nd person singular or all persons plural, negated\n",
      "    weren't\n",
      "BEDZ: verb 'to be', past tense, 1st and 3rd person singular\n",
      "    was\n",
      "BEDZ*: verb 'to be', past tense, 1st and 3rd person singular, negated\n",
      "    wasn't\n",
      "BEG: verb 'to be', present participle or gerund\n",
      "    being\n",
      "BEM: verb 'to be', present tense, 1st person singular\n",
      "    am\n",
      "BEM*: verb 'to be', present tense, 1st person singular, negated\n",
      "    ain't\n",
      "BEN: verb 'to be', past participle\n",
      "    been\n",
      "BER: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    are art\n",
      "BER*: verb 'to be', present tense, 2nd person singular or all persons plural, negated\n",
      "    aren't ain't\n",
      "BEZ: verb 'to be', present tense, 3rd person singular\n",
      "    is\n",
      "BEZ*: verb 'to be', present tense, 3rd person singular, negated\n",
      "    isn't ain't\n",
      "CC: conjunction, coordinating\n",
      "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
      "CD: numeral, cardinal\n",
      "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
      "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
      "    ...\n",
      "CD$: numeral, cardinal, genitive\n",
      "    1960's 1961's .404's\n",
      "CS: conjunction, subordinating\n",
      "    that as after whether before while like because if since for than altho\n",
      "    until so unless though providing once lest s'posin' till whereas\n",
      "    whereupon supposing tho' albeit then so's 'fore\n",
      "DO: verb 'to do', uninflected present tense, infinitive or imperative\n",
      "    do dost\n",
      "DO*: verb 'to do', uninflected present tense or imperative, negated\n",
      "    don't\n",
      "DO+PPSS: verb 'to do', past or present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    d'you\n",
      "DOD: verb 'to do', past tense\n",
      "    did done\n",
      "DOD*: verb 'to do', past tense, negated\n",
      "    didn't\n",
      "DOZ: verb 'to do', present tense, 3rd person singular\n",
      "    does\n",
      "DOZ*: verb 'to do', present tense, 3rd person singular, negated\n",
      "    doesn't don't\n",
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n",
      "DT$: determiner/pronoun, singular, genitive\n",
      "    another's\n",
      "DT+BEZ: determiner/pronoun + verb 'to be', present tense, 3rd person singular\n",
      "    that's\n",
      "DT+MD: determiner/pronoun + modal auxillary\n",
      "    that'll this'll\n",
      "DTI: determiner/pronoun, singular or plural\n",
      "    any some\n",
      "DTS: determiner/pronoun, plural\n",
      "    these those them\n",
      "DTS+BEZ: pronoun, plural + verb 'to be', present tense, 3rd person singular\n",
      "    them's\n",
      "DTX: determiner, pronoun or double conjunction\n",
      "    neither either one\n",
      "EX: existential there\n",
      "    there\n",
      "EX+BEZ: existential there + verb 'to be', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+HVD: existential there + verb 'to have', past tense\n",
      "    there'd\n",
      "EX+HVZ: existential there + verb 'to have', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+MD: existential there + modal auxillary\n",
      "    there'll there'd\n",
      "FW-*: foreign word: negator\n",
      "    pas non ne\n",
      "FW-AT: foreign word: article\n",
      "    la le el un die der ein keine eine das las les Il\n",
      "FW-AT+NN: foreign word: article + noun, singular, common\n",
      "    l'orchestre l'identite l'arcade l'ange l'assistance l'activite\n",
      "    L'Universite l'independance L'Union L'Unita l'osservatore\n",
      "FW-AT+NP: foreign word: article + noun, singular, proper\n",
      "    L'Astree L'Imperiale\n",
      "FW-BE: foreign word: verb 'to be', infinitive or imperative\n",
      "    sit\n",
      "FW-BER: foreign word: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    sind sunt etes\n",
      "FW-BEZ: foreign word: verb 'to be', present tense, 3rd person singular\n",
      "    ist est\n",
      "FW-CC: foreign word: conjunction, coordinating\n",
      "    et ma mais und aber och nec y\n",
      "FW-CD: foreign word: numeral, cardinal\n",
      "    une cinq deux sieben unam zwei\n",
      "FW-CS: foreign word: conjunction, subordinating\n",
      "    bevor quam ma\n",
      "FW-DT: foreign word: determiner/pronoun, singular\n",
      "    hoc\n",
      "FW-DT+BEZ: foreign word: determiner + verb 'to be', present tense, 3rd person singular\n",
      "    c'est\n",
      "FW-DTS: foreign word: determiner/pronoun, plural\n",
      "    haec\n",
      "FW-HV: foreign word: verb 'to have', present tense, not 3rd person singular\n",
      "    habe\n",
      "FW-IN: foreign word: preposition\n",
      "    ad de en a par con dans ex von auf super post sine sur sub avec per\n",
      "    inter sans pour pendant in di\n",
      "FW-IN+AT: foreign word: preposition + article\n",
      "    della des du aux zur d'un del dell'\n",
      "FW-IN+NN: foreign word: preposition + noun, singular, common\n",
      "    d'etat d'hotel d'argent d'identite d'art\n",
      "FW-IN+NP: foreign word: preposition + noun, singular, proper\n",
      "    d'Yquem d'Eiffel\n",
      "FW-JJ: foreign word: adjective\n",
      "    avant Espagnol sinfonica Siciliana Philharmonique grand publique haute\n",
      "    noire bouffe Douce meme humaine bel serieuses royaux anticus presto\n",
      "    Sovietskaya Bayerische comique schwarzen ...\n",
      "FW-JJR: foreign word: adjective, comparative\n",
      "    fortiori\n",
      "FW-JJT: foreign word: adjective, superlative\n",
      "    optimo\n",
      "FW-NN: foreign word: noun, singular, common\n",
      "    ballet esprit ersatz mano chatte goutte sang Fledermaus oud def kolkhoz\n",
      "    roi troika canto boite blutwurst carne muzyka bonheur monde piece force\n",
      "    ...\n",
      "FW-NN$: foreign word: noun, singular, common, genitive\n",
      "    corporis intellectus arte's dei aeternitatis senioritatis curiae\n",
      "    patronne's chambre's\n",
      "FW-NNS: foreign word: noun, plural, common\n",
      "    al culpas vopos boites haflis kolkhozes augen tyrannis alpha-beta-\n",
      "    gammas metis banditos rata phis negociants crus Einsatzkommandos\n",
      "    kamikaze wohaws sabinas zorrillas palazzi engages coureurs corroborees\n",
      "    yori Ubermenschen ...\n",
      "FW-NP: foreign word: noun, singular, proper\n",
      "    Karshilama Dieu Rundfunk Afrique Espanol Afrika Spagna Gott Carthago\n",
      "    deus\n",
      "FW-NPS: foreign word: noun, plural, proper\n",
      "    Svenskarna Atlantes Dieux\n",
      "FW-NR: foreign word: noun, singular, adverbial\n",
      "    heute morgen aujourd'hui hoy\n",
      "FW-OD: foreign word: numeral, ordinal\n",
      "    18e 17e quintus\n",
      "FW-PN: foreign word: pronoun, nominal\n",
      "    hoc\n",
      "FW-PP$: foreign word: determiner, possessive\n",
      "    mea mon deras vos\n",
      "FW-PPL: foreign word: pronoun, singular, reflexive\n",
      "    se\n",
      "FW-PPL+VBZ: foreign word: pronoun, singular, reflexive + verb, present tense, 3rd person singular\n",
      "    s'excuse s'accuse\n",
      "FW-PPO: pronoun, personal, accusative\n",
      "    lui me moi mi\n",
      "FW-PPO+IN: foreign word: pronoun, personal, accusative + preposition\n",
      "    mecum tecum\n",
      "FW-PPS: foreign word: pronoun, personal, nominative, 3rd person singular\n",
      "    il\n",
      "FW-PPSS: foreign word: pronoun, personal, nominative, not 3rd person singular\n",
      "    ich vous sie je\n",
      "FW-PPSS+HV: foreign word: pronoun, personal, nominative, not 3rd person singular + verb 'to have', present tense, not 3rd person singular\n",
      "    j'ai\n",
      "FW-QL: foreign word: qualifier\n",
      "    minus\n",
      "FW-RB: foreign word: adverb\n",
      "    bas assai deja um wiederum cito velociter vielleicht simpliciter non zu\n",
      "    domi nuper sic forsan olim oui semper tout despues hors\n",
      "FW-RB+CC: foreign word: adverb + conjunction, coordinating\n",
      "    forisque\n",
      "FW-TO+VB: foreign word: infinitival to + verb, infinitive\n",
      "    d'entretenir\n",
      "FW-UH: foreign word: interjection\n",
      "    sayonara bien adieu arigato bonjour adios bueno tchalo ciao o\n",
      "FW-VB: foreign word: verb, present tense, not 3rd person singular, imperative or infinitive\n",
      "    nolo contendere vive fermate faciunt esse vade noli tangere dites duces\n",
      "    meminisse iuvabit gosaimasu voulez habla ksu'u'peli'afo lacheln miuchi\n",
      "    say allons strafe portant\n",
      "FW-VBD: foreign word: verb, past tense\n",
      "    stabat peccavi audivi\n",
      "FW-VBG: foreign word: verb, present participle or gerund\n",
      "    nolens volens appellant seq. obliterans servanda dicendi delenda\n",
      "FW-VBN: foreign word: verb, past participle\n",
      "    vue verstrichen rasa verboten engages\n",
      "FW-VBZ: foreign word: verb, present tense, 3rd person singular\n",
      "    gouverne sinkt sigue diapiace\n",
      "FW-WDT: foreign word: WH-determiner\n",
      "    quo qua quod que quok\n",
      "FW-WPO: foreign word: WH-pronoun, accusative\n",
      "    quibusdam\n",
      "FW-WPS: foreign word: WH-pronoun, nominative\n",
      "    qui\n",
      "HV: verb 'to have', uninflected present tense, infinitive or imperative\n",
      "    have hast\n",
      "HV*: verb 'to have', uninflected present tense or imperative, negated\n",
      "    haven't ain't\n",
      "HV+TO: verb 'to have', uninflected present tense + infinitival to\n",
      "    hafta\n",
      "HVD: verb 'to have', past tense\n",
      "    had\n",
      "HVD*: verb 'to have', past tense, negated\n",
      "    hadn't\n",
      "HVG: verb 'to have', present participle or gerund\n",
      "    having\n",
      "HVN: verb 'to have', past participle\n",
      "    had\n",
      "HVZ: verb 'to have', present tense, 3rd person singular\n",
      "    has hath\n",
      "HVZ*: verb 'to have', present tense, 3rd person singular, negated\n",
      "    hasn't ain't\n",
      "IN: preposition\n",
      "    of in for by considering to on among at through with under into\n",
      "    regarding than since despite according per before toward against as\n",
      "    after during including between without except upon out over ...\n",
      "IN+IN: preposition, hyphenated pair\n",
      "    f'ovuh\n",
      "IN+PPO: preposition + pronoun, personal, accusative\n",
      "    t'hi-im\n",
      "JJ: adjective\n",
      "    ecent over-all possible hard-fought favorable hard meager fit such\n",
      "    widespread outmoded inadequate ambiguous grand clerical effective\n",
      "    orderly federal foster general proportionate ...\n",
      "JJ$: adjective, genitive\n",
      "    Great's\n",
      "JJ+JJ: adjective, hyphenated pair\n",
      "    big-large long-far\n",
      "JJR: adjective, comparative\n",
      "    greater older further earlier later freer franker wider better deeper\n",
      "    firmer tougher faster higher bigger worse younger lighter nicer slower\n",
      "    happier frothier Greater newer Elder ...\n",
      "JJR+CS: adjective + conjunction, coordinating\n",
      "    lighter'n\n",
      "JJS: adjective, semantically superlative\n",
      "    top chief principal northernmost master key head main tops utmost\n",
      "    innermost foremost uppermost paramount topmost\n",
      "JJT: adjective, superlative\n",
      "    best largest coolest calmest latest greatest earliest simplest\n",
      "    strongest newest fiercest unhappiest worst youngest worthiest fastest\n",
      "    hottest fittest lowest finest smallest staunchest ...\n",
      "MD: modal auxillary\n",
      "    should may might will would must can could shall ought need wilt\n",
      "MD*: modal auxillary, negated\n",
      "    cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't\n",
      "MD+HV: modal auxillary + verb 'to have', uninflected form\n",
      "    shouldda musta coulda must've woulda could've\n",
      "MD+PPSS: modal auxillary + pronoun, personal, nominative, not 3rd person singular\n",
      "    willya\n",
      "MD+TO: modal auxillary + infinitival to\n",
      "    oughta\n",
      "NN: noun, singular, common\n",
      "    failure burden court fire appointment awarding compensation Mayor\n",
      "    interim committee fact effect airport management surveillance jail\n",
      "    doctor intern extern night weekend duty legislation Tax Office ...\n",
      "NN$: noun, singular, common, genitive\n",
      "    season's world's player's night's chapter's golf's football's\n",
      "    baseball's club's U.'s coach's bride's bridegroom's board's county's\n",
      "    firm's company's superintendent's mob's Navy's ...\n",
      "NN+BEZ: noun, singular, common + verb 'to be', present tense, 3rd person singular\n",
      "    water's camera's sky's kid's Pa's heat's throat's father's money's\n",
      "    undersecretary's granite's level's wife's fat's Knife's fire's name's\n",
      "    hell's leg's sun's roulette's cane's guy's kind's baseball's ...\n",
      "NN+HVD: noun, singular, common + verb 'to have', past tense\n",
      "    Pa'd\n",
      "NN+HVZ: noun, singular, common + verb 'to have', present tense, 3rd person singular\n",
      "    guy's Knife's boat's summer's rain's company's\n",
      "NN+IN: noun, singular, common + preposition\n",
      "    buncha\n",
      "NN+MD: noun, singular, common + modal auxillary\n",
      "    cowhand'd sun'll\n",
      "NN+NN: noun, singular, common, hyphenated pair\n",
      "    stomach-belly\n",
      "NNS: noun, plural, common\n",
      "    irregularities presentments thanks reports voters laws legislators\n",
      "    years areas adjustments chambers $100 bonds courts sales details raises\n",
      "    sessions members congressmen votes polls calls ...\n",
      "NNS$: noun, plural, common, genitive\n",
      "    taxpayers' children's members' States' women's cutters' motorists'\n",
      "    steelmakers' hours' Nations' lawyers' prisoners' architects' tourists'\n",
      "    Employers' secretaries' Rogues' ...\n",
      "NNS+MD: noun, plural, common + modal auxillary\n",
      "    duds'd oystchers'll\n",
      "NP: noun, singular, proper\n",
      "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
      "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
      "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
      "NP$: noun, singular, proper, genitive\n",
      "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
      "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
      "    Shea's Palmer's Arnold's Broglio's ...\n",
      "NP+BEZ: noun, singular, proper + verb 'to be', present tense, 3rd person singular\n",
      "    W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's Seaton's\n",
      "    Buckhorn's Breed's Penny's Rob's Kitty's Blackwell's Myra's Wally's\n",
      "    Lucille's Springfield's Arlene's\n",
      "NP+HVZ: noun, singular, proper + verb 'to have', present tense, 3rd person singular\n",
      "    Bill's Guardino's Celie's Skolman's Crosson's Tim's Wally's\n",
      "NP+MD: noun, singular, proper + modal auxillary\n",
      "    Gyp'll John'll\n",
      "NPS: noun, plural, proper\n",
      "    Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen\n",
      "    Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British\n",
      "    Dixiecrats Mississippians Congresses ...\n",
      "NPS$: noun, plural, proper, genitive\n",
      "    Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees' Stevenses'\n",
      "    Geraghtys' Burkes' Wackers' Achaeans' Dresbachs' Russians' Democrats'\n",
      "    Gershwins' Adventists' Negroes' Catholics' ...\n",
      "NR: noun, singular, adverbial\n",
      "    Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow\n",
      "    tonight West East Saturday west left east downtown north northeast\n",
      "    southeast northwest North South right ...\n",
      "NR$: noun, singular, adverbial, genitive\n",
      "    Saturday's Monday's yesterday's tonight's tomorrow's Sunday's\n",
      "    Wednesday's Friday's today's Tuesday's West's Today's South's\n",
      "NR+MD: noun, singular, adverbial + modal auxillary\n",
      "    today'll\n",
      "NRS: noun, plural, adverbial\n",
      "    Sundays Mondays Saturdays Wednesdays Souths Fridays\n",
      "OD: numeral, ordinal\n",
      "    first 13th third nineteenth 2d 61st second sixth eighth ninth twenty-\n",
      "    first eleventh 50th eighteenth- Thirty-ninth 72nd 1/20th twentieth\n",
      "    mid-19th thousandth 350th sixteenth 701st ...\n",
      "PN: pronoun, nominal\n",
      "    none something everything one anyone nothing nobody everybody everyone\n",
      "    anybody anything someone no-one nothin\n",
      "PN$: pronoun, nominal, genitive\n",
      "    one's someone's anybody's nobody's everybody's anyone's everyone's\n",
      "PN+BEZ: pronoun, nominal + verb 'to be', present tense, 3rd person singular\n",
      "    nothing's everything's somebody's nobody's someone's\n",
      "PN+HVD: pronoun, nominal + verb 'to have', past tense\n",
      "    nobody'd\n",
      "PN+HVZ: pronoun, nominal + verb 'to have', present tense, 3rd person singular\n",
      "    nobody's somebody's one's\n",
      "PN+MD: pronoun, nominal + modal auxillary\n",
      "    someone'll somebody'll anybody'd\n",
      "PP$: determiner, possessive\n",
      "    our its his their my your her out thy mine thine\n",
      "PP$$: pronoun, possessive\n",
      "    ours mine his hers theirs yours\n",
      "PPL: pronoun, singular, reflexive\n",
      "    itself himself myself yourself herself oneself ownself\n",
      "PPLS: pronoun, plural, reflexive\n",
      "    themselves ourselves yourselves\n",
      "PPO: pronoun, personal, accusative\n",
      "    them it him me us you 'em her thee we'uns\n",
      "PPS: pronoun, personal, nominative, 3rd person singular\n",
      "    it he she thee\n",
      "PPS+BEZ: pronoun, personal, nominative, 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+HVD: pronoun, personal, nominative, 3rd person singular + verb 'to have', past tense\n",
      "    she'd he'd it'd\n",
      "PPS+HVZ: pronoun, personal, nominative, 3rd person singular + verb 'to have', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+MD: pronoun, personal, nominative, 3rd person singular + modal auxillary\n",
      "    he'll she'll it'll he'd it'd she'd\n",
      "PPSS: pronoun, personal, nominative, not 3rd person singular\n",
      "    they we I you ye thou you'uns\n",
      "PPSS+BEM: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 1st person singular\n",
      "    I'm Ahm\n",
      "PPSS+BER: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    we're you're they're\n",
      "PPSS+BEZ: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    you's\n",
      "PPSS+BEZ*: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular, negated\n",
      "    'tain't\n",
      "PPSS+HV: pronoun, personal, nominative, not 3rd person singular + verb 'to have', uninflected present tense\n",
      "    I've we've they've you've\n",
      "PPSS+HVD: pronoun, personal, nominative, not 3rd person singular + verb 'to have', past tense\n",
      "    I'd you'd we'd they'd\n",
      "PPSS+MD: pronoun, personal, nominative, not 3rd person singular + modal auxillary\n",
      "    you'll we'll I'll we'd I'd they'll they'd you'd\n",
      "PPSS+VB: pronoun, personal, nominative, not 3rd person singular + verb 'to verb', uninflected present tense\n",
      "    y'know\n",
      "QL: qualifier, pre\n",
      "    well less very most so real as highly fundamentally even how much\n",
      "    remarkably somewhat more completely too thus ill deeply little overly\n",
      "    halfway almost impossibly far severly such ...\n",
      "QLP: qualifier, post\n",
      "    indeed enough still 'nuff\n",
      "RB: adverb\n",
      "    only often generally also nevertheless upon together back newly no\n",
      "    likely meanwhile near then heavily there apparently yet outright fully\n",
      "    aside consistently specifically formally ever just ...\n",
      "RB$: adverb, genitive\n",
      "    else's\n",
      "RB+BEZ: adverb + verb 'to be', present tense, 3rd person singular\n",
      "    here's there's\n",
      "RB+CS: adverb + conjunction, coordinating\n",
      "    well's soon's\n",
      "RBR: adverb, comparative\n",
      "    further earlier better later higher tougher more harder longer sooner\n",
      "    less faster easier louder farther oftener nearer cheaper slower tighter\n",
      "    lower worse heavier quicker ...\n",
      "RBR+CS: adverb, comparative + conjunction, coordinating\n",
      "    more'n\n",
      "RBT: adverb, superlative\n",
      "    most best highest uppermost nearest brightest hardest fastest deepest\n",
      "    farthest loudest ...\n",
      "RN: adverb, nominal\n",
      "    here afar then\n",
      "RP: adverb, particle\n",
      "    up out off down over on in about through across after\n",
      "RP+IN: adverb, particle + preposition\n",
      "    out'n outta\n",
      "TO: infinitival to\n",
      "    to t'\n",
      "TO+VB: infinitival to + verb, infinitive\n",
      "    t'jawn t'lah\n",
      "UH: interjection\n",
      "    Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha crunch say\n",
      "    oh why see well hello lo alas tarantara rum-tum-tum gosh hell keerist\n",
      "    Jesus Keeeerist boy c'mon 'mon goddamn bah hoo-pig damn ...\n",
      "VB: verb, base: uninflected present, imperative or infinitive\n",
      "    investigate find act follow inure achieve reduce take remedy re-set\n",
      "    distribute realize disable feel receive continue place protect\n",
      "    eliminate elaborate work permit run enter force ...\n",
      "VB+AT: verb, base: uninflected present or infinitive + article\n",
      "    wanna\n",
      "VB+IN: verb, base: uninflected present, imperative or infinitive + preposition\n",
      "    lookit\n",
      "VB+JJ: verb, base: uninflected present, imperative or infinitive + adjective\n",
      "    die-dead\n",
      "VB+PPO: verb, uninflected present tense + pronoun, personal, accusative\n",
      "    let's lemme gimme\n",
      "VB+RP: verb, imperative + adverbial particle\n",
      "    g'ahn c'mon\n",
      "VB+TO: verb, base: uninflected present, imperative or infinitive + infinitival to\n",
      "    wanta wanna\n",
      "VB+VB: verb, base: uninflected present, imperative or infinitive; hypenated pair\n",
      "    say-speak\n",
      "VBD: verb, past tense\n",
      "    said produced took recommended commented urged found added praised\n",
      "    charged listed became announced brought attended wanted voted defeated\n",
      "    received got stood shot scheduled feared promised made ...\n",
      "VBG: verb, present participle or gerund\n",
      "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
      "    keeping getting picking entering voting warning making strengthening\n",
      "    setting neighboring attending participating moving ...\n",
      "VBG+TO: verb, present participle + infinitival to\n",
      "    gonna\n",
      "VBN: verb, past participle\n",
      "    conducted charged won received studied revised operated accepted\n",
      "    combined experienced recommended effected granted seen protected\n",
      "    adopted retarded notarized selected composed gotten printed ...\n",
      "VBN+TO: verb, past participle + infinitival to\n",
      "    gotta\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    deserves believes receives takes goes expires says opposes starts\n",
      "    permits expects thinks faces votes teaches holds calls fears spends\n",
      "    collects backs eliminates sets flies gives seeks reads ...\n",
      "WDT: WH-determiner\n",
      "    which what whatever whichever whichever-the-hell\n",
      "WDT+BER: WH-determiner + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    what're\n",
      "WDT+BER+PP: WH-determiner + verb 'to be', present, 2nd person singular or all persons plural + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+BEZ: WH-determiner + verb 'to be', present tense, 3rd person singular\n",
      "    what's\n",
      "WDT+DO+PPS: WH-determiner + verb 'to do', uninflected present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+DOD: WH-determiner + verb 'to do', past tense\n",
      "    what'd\n",
      "WDT+HVZ: WH-determiner + verb 'to have', present tense, 3rd person singular\n",
      "    what's\n",
      "WP$: WH-pronoun, genitive\n",
      "    whose whosever\n",
      "WPO: WH-pronoun, accusative\n",
      "    whom that who\n",
      "WPS: WH-pronoun, nominative\n",
      "    that who whoever whosoever what whatsoever\n",
      "WPS+BEZ: WH-pronoun, nominative + verb 'to be', present, 3rd person singular\n",
      "    that's who's\n",
      "WPS+HVD: WH-pronoun, nominative + verb 'to have', past tense\n",
      "    who'd\n",
      "WPS+HVZ: WH-pronoun, nominative + verb 'to have', present tense, 3rd person singular\n",
      "    who's that's\n",
      "WPS+MD: WH-pronoun, nominative + modal auxillary\n",
      "    who'll that'd who'd that'll\n",
      "WQL: WH-qualifier\n",
      "    however how\n",
      "WRB: WH-adverb\n",
      "    however when where why whereby wherever how whenever whereon wherein\n",
      "    wherewith wheare wherefore whereof howsabout\n",
      "WRB+BER: WH-adverb + verb 'to be', present, 2nd person singular or all persons plural\n",
      "    where're\n",
      "WRB+BEZ: WH-adverb + verb 'to be', present, 3rd person singular\n",
      "    how's where's\n",
      "WRB+DO: WH-adverb + verb 'to do', present, not 3rd person singular\n",
      "    howda\n",
      "WRB+DOD: WH-adverb + verb 'to do', past tense\n",
      "    where'd how'd\n",
      "WRB+DOD*: WH-adverb + verb 'to do', past tense, negated\n",
      "    whyn't\n",
      "WRB+DOZ: WH-adverb + verb 'to do', present tense, 3rd person singular\n",
      "    how's\n",
      "WRB+IN: WH-adverb + preposition\n",
      "    why'n\n",
      "WRB+MD: WH-adverb + modal auxillary\n",
      "    where'd\n"
     ]
    }
   ],
   "source": [
    "# Brown tagset\n",
    "nltk.help.brown_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jt9h5VacDO3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# Penn Treebank tagset\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj9JK_okDO3j"
   },
   "source": [
    "Taking the Brown Corpus as an example, we can explore the corpus further by counting how many times a tag appears in the corpus, and create a distribution of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O_eA3zroDO3q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 152470, 'IN': 120557, 'AT': 97959, 'JJ': 64028, '.': 60638, ',': 58156, 'NNS': 55110, 'CC': 37718, 'RB': 36464, 'NP': 34476, ...})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in nltk.corpus.brown.tagged_words())\n",
    "tag_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Hl5Pt4EDO32"
   },
   "source": [
    "We can find the most and least frequent POS tag by sorting the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ay1TuyjsDO34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 152470)\n",
      "('FW-UH-TL', 1)\n"
     ]
    }
   ],
   "source": [
    "tag_list = sorted(tag_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(tag_list[0])\n",
    "print(tag_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tCf4So3UDO4B"
   },
   "source": [
    "We can also plot the curve of the POS tag frequency distribution. For the simplicity of visualization, let's first map each tag name to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ytBRfCHDO4B"
   },
   "outputs": [],
   "source": [
    "tag_fd_index = dict(zip([tag for tag, freq in tag_list], range(len(tag_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQPgkz4oDO4M"
   },
   "source": [
    "Then let's create a new `FreqDict` of tag_index and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ER4TibGNDO4O"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fba26c24710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_index_fd = nltk.FreqDist({tag_fd_index[k]: v for k, v in tag_fd.items()})\n",
    "tag_index_fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arHQRd8hDO4W"
   },
   "source": [
    "Finally, let’s look for words that are highly ambiguous as to their POS tag. Understanding why such words are tagged as they are in each context can help us clarify the distinctions between the tags.\n",
    "\n",
    "Note that the items being counted in the frequency distribution are word-tag pairs, we can then treat the word as a condition and the tag as an event, and initialize a conditional frequency distribution with a list of condition-event\n",
    "pairs. This lets us see a frequency-ordered list of tags given a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X81wWfpLDO4X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'AT': 69013, 'AT-TL': 675, 'AT-HL': 253, 'AT-NC': 26, 'NIL': 3, 'AT-TL-HL': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in nltk.corpus.brown.tagged_words())\n",
    "print(data.conditions()[0])\n",
    "data['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQfIr67jDO4e"
   },
   "source": [
    "We can print first 20 words along with their POS tags when the word has more than 4 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGgJZspkDO4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the AT AT-TL AT-HL AT-NC NIL AT-TL-HL\n",
      "an AT AT-HL AT-TL NIL AT-NC CC\n",
      "of IN IN-TL IN-HL IN-TL-HL IN-NC NIL\n",
      "no AT RB AT-HL AT-TL QL RB-NC\n",
      "that CS WPS DT QL WPO CS-HL DT-TL WPS-TL DT-HL DT-NC NIL WPS-NC WPO-NC CS-NC WPS-HL\n",
      "any DTI QL DTI-HL DTI-TL RB\n",
      "place NN VB NN-TL NP NN-HL FW-NN-TL NN-NC\n",
      "in IN RP IN-HL IN-TL IN-NC RP-HL NN NIL RP-NC FW-IN\n",
      ", , ,-HL FW-RB-TL ,-TL ,-NC\n",
      "charge NN VB NN-HL NN-TL NN-NC\n",
      "and CC CC-TL CC-HL CC-TL-HL NIL CC-NC\n",
      "for IN IN-TL IN-HL CS RB NN IN-NC\n",
      "by IN IN-HL IN-TL RB NIL IN-NC\n",
      "to TO IN IN-HL TO-HL IN-TL TO-TL NPS NIL QL TO-NC IN-NC\n",
      "only RB AP JJ QL AP-HL\n",
      "a AT AT-HL NN AT-TL FW-IN NP-TL NN-TL AT-TL-HL NIL NP NP-HL AT-NC FW-IN-TL\n",
      "number NN NN-HL VB NN-NC NN-TL\n",
      "this DT DT-HL DT-TL QL DT-NC\n",
      "it PPS PPO PPO-TL PPO-HL PPS-HL PPS-TL PPS-NC PPO-NC UH\n",
      "many AP AP-HL ABN QL NIL AP-NC\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for word in data.conditions():\n",
    "    if len(data[word]) > 4:\n",
    "        tags = list(data[word].keys())\n",
    "        print (word, ' '.join(tags))\n",
    "        idx += 1\n",
    "        if idx >= 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iCcVirkrDO4k"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "For Penn Treebank Corpus, repeat the same process, i.e. plotting the distribution of the tagset, and finding the most and least frequent tag.\n",
    "\n",
    "For both corpora, what kinds of words occur in the noun, adjective, adverb category?\n",
    "\n",
    "Explore the POS ditribution of ambiguous words mentioned before, e.g. *content* and *present*. Do they support our observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHuFNJ_oDO4n"
   },
   "source": [
    "#### POS tag n-grams\n",
    "So far we have been working with a only single POS tag in text sequence, i.e. the unigram. \n",
    "However, we should be aware that POS each tag in text sequence often depends on its previous/later tags.\n",
    "For example, nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
    "Therefore, we can look at some n-gram sequences of POS tags such as tag bi-grams.\n",
    "\n",
    "Let's build a frequency dictionary of tag bi-grams for the Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9J-rbYO-DO4o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('AT', 'NN'): 48376, ('IN', 'AT'): 43271, ('NN', 'IN'): 42256, ('JJ', 'NN'): 28407, ('NN', '.'): 19873, ('AT', 'JJ'): 19488, ('NN', ','): 18282, ('IN', 'NN'): 17225, ('NNS', 'IN'): 14505, ('TO', 'VB'): 12291, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigram_list = list(nltk.bigrams(nltk.corpus.brown.tagged_words()))\n",
    "tag_bigram_fd = nltk.FreqDist((a[1], b[1]) for (a, b) in tag_bigram_list if b[1])\n",
    "tag_bigram_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piNIlFnxDO4u"
   },
   "source": [
    "We can see that compared to unigram tag frequency dictionary, bigram dictionary has many more entries. It can potentiall cause the data sparsity problmen as we will encounter later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bKICrzikDO4w"
   },
   "source": [
    "To validate our intuitions that nouns often occur after the determiners and adjectives, we can investigate the distribution of tag bi-gram **(X, NN)** where **X** refers to any possible tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SMQa50ZoDO4x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('AT', 'NN'), 48376), (('JJ', 'NN'), 28407), (('IN', 'NN'), 17225), (('NN', 'NN'), 11646), (('PP$', 'NN'), 8636), (('DT', 'NN'), 4367), (('CC', 'NN'), 4326), (('VBG', 'NN'), 2798), (('AP', 'NN'), 2671), ((',', 'NN'), 2394), (('VBN', 'NN'), 2301), (('CD', 'NN'), 1635), (('VB', 'NN'), 1463), (('NP', 'NN'), 1361), (('NP$', 'NN'), 1282), (('.', 'NN'), 1245), (('DTI', 'NN'), 1164), (('OD', 'NN'), 1139), (('CS', 'NN'), 992), (('NN$', 'NN'), 887)]\n"
     ]
    }
   ],
   "source": [
    "nn_bigram_fd = nltk.FreqDist({k: v for k, v in tag_bigram_fd.items() if k[1] == 'NN'})\n",
    "sorted_nn_bigram_list = sorted(nn_bigram_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_nn_bigram_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ49CtaaDO45"
   },
   "source": [
    "We can see that the two most frequent tags with **(X, NN)** are indeed **AT** (article) and **JJ** (adjective) according to the Brown Corpus tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A93RhIh2DO47"
   },
   "source": [
    "### Mapping of properties using Python dictionaries\n",
    "As we have seen, a tagged word of the form **(word, tag)** is an association between a word and a POS tag. \n",
    "Once we start doing POS tagging, we will be using such structures extensively.\n",
    "For example, we want to get the tag list of a given word **(word, tag list)**, or the frequency of a given tag **(tag, freq)**.\n",
    "We can think of this process as mapping from a certain property to another, and the most natural way to store mappings in Python uses the so-called `dictionary` data type, which we have encountered in module 1 and previous part of this module. \n",
    "\n",
    "We look at dictionaries and see how they can represent a variety of language information. \n",
    "For example, recall the frequency dictionary we used previously, which maps from a POS tag to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_QqJciyDO48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 152470, 'IN': 120557, 'AT': 97959, 'JJ': 64028, '.': 60638, ',': 58156, 'NNS': 55110, 'CC': 37718, 'RB': 36464, 'NP': 34476, ...})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tag_fd['NN'])\n",
    "tag_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjblRyhkDO5P"
   },
   "source": [
    "If we want to create a dictionary of the form **(word, its corresponding tags)**, what types would be for the key and value of the dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xPwzLATqDO5S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: V\n",
      "object: N\n"
     ]
    }
   ],
   "source": [
    "word2tags = {'present': 'V', 'object': 'N'}\n",
    "for k, v in word2tags.items():\n",
    "    print('{}: {}'.format(k, word2tags[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEZTe3sZDO5W"
   },
   "source": [
    "Here we use the `string` type for tag, which can be problematic, as both words *present* and *object* can be either noun and verb, so if we want to change a tag of *present*, the original tag will be overwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3OuZwO9DO5Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: V\n",
      "V\n",
      "present: N\n"
     ]
    }
   ],
   "source": [
    "word = 'present'\n",
    "print('{}: {}'.format(word, word2tags[word]))\n",
    "print(word2tags['present'])\n",
    "word2tags['present'] = 'N'\n",
    "print('{}: {}'.format(word, word2tags[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jngBzF-_DO5d"
   },
   "source": [
    "A better way is to use the mapping **(word (string), tags (list))** for the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RTSrYWuHDO5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: ['V', 'N']\n",
      "object: ['V', 'N']\n"
     ]
    }
   ],
   "source": [
    "word2tags = {'present': ['V', 'N'], 'object': ['V', 'N']}\n",
    "for k, v in word2tags.items():\n",
    "    print('{}: {}'.format(k, word2tags[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DL6PySYSDO5j"
   },
   "source": [
    "And if we want to further add a new tag to the word, we can simply `append` it to the tag list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Gqs2ahODO5l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: ['V', 'N']\n",
      "present: ['V', 'N', 'ADJ']\n"
     ]
    }
   ],
   "source": [
    "word = 'present'\n",
    "print('{}: {}'.format(word, word2tags[word]))\n",
    "word2tags[word].append('ADJ')\n",
    "print('{}: {}'.format(word, word2tags[word]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUZC5Nq6DO5p"
   },
   "source": [
    "#### Default dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEk5Sgs9DO5q"
   },
   "source": [
    "If we try to access a key that is not in a dictionary, we get an error. \n",
    "However, it’s often useful if a dictionary can automatically create an entry for this new key and give it a default value, such as zero or the empty list.\n",
    "Both **NLTK** and **Python** provide us with the `defaultdict`, a more advanced dictionary, where we can supply a parameter which can be used to create the default value, e.g., int, float, str, list, dict, tuple.\n",
    "For simplicity, we will stick to the `defaultdict` of NLTK, and for that of Python you can refer to the [link](https://docs.python.org/3/library/collections.html#collections.defaultdict).\n",
    "\n",
    "Here we assign 0 frequency to a new tag *UNK*, and a tag list with the most frequent tag **N** inside it to a new word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kaLPKVfDO5r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "tag_fd_def = nltk.defaultdict(int, tag_fd)\n",
    "print(tag_fd_def['UNK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8b8doyuIDO5w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N']\n"
     ]
    }
   ],
   "source": [
    "word2tags_def = nltk.defaultdict(lambda: ['N'], word2tags)\n",
    "print(word2tags_def['a_new_word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkPsNCLRDO5z"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "1. In NLP, one of the preprocessing steps is to replace low-frequency words with a special \"out of vocabulary\" token, *UNK*, with the help of a default dictionary. \n",
    "Preprocess `alice` corpus: keep the most frequent *n* words, and map the rest words to UNK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTx4UBk1DO50"
   },
   "outputs": [],
   "source": [
    "alice = nltk.corpus.gutenberg.words('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loHwiLMoDO55"
   },
   "source": [
    "2. Create a POS unigram for the Penn Treebank Corpus of the mapping **(word, tag_list)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T71k7AOBDO55"
   },
   "source": [
    "#### POS tag n-grams for tagging\n",
    "We can use default dictionaries with complex keys and values to build tag n-grams for tagging.\n",
    "\n",
    "Taking bi-gram as an example, we want to store the frequency of tag **t2**, which is dependent on the tag **t1** of its previous word, and the current word **w2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxM-LjDWDO56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'NN': 17, 'JJ': 2, 'RB': 1})\n"
     ]
    }
   ],
   "source": [
    "# create a nested default dictionary, where the value is another default dictionary whose default value is int (0)\n",
    "pos = nltk.defaultdict(lambda: nltk.defaultdict(int))\n",
    "# create bigrams of (word, tag) pair\n",
    "for ((w1, t1), (w2, t2)) in nltk.bigrams(nltk.corpus.treebank.tagged_words()):\n",
    "    pos[(t1, w2)][t2] += 1\n",
    "print(pos[('DT', 'right')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBUzcFILDO5_"
   },
   "source": [
    "Each time through the loop we updated our `pos` entry for **(t1, w2)**, a tag and its following word. \n",
    "When we look up an item in pos we must specify a compound key , and we get back a dictionary object. \n",
    "A POS tagger could use such information to decide that the word *right*, when preceded by a determiner, should be tagged as one of the tags in *NN*, *JJ*, and *RB*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5k41padDO6A"
   },
   "source": [
    "### Automatic Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdyqqkNcDO6B"
   },
   "source": [
    "We will explore various ways to automatically perform POS tagging. We will see that the tag of a word depends on the word and its context within a *sentence*. \n",
    "For this reason, we will be working with data at the level of (tagged) sentences rather than words. \n",
    "\n",
    "We’ll begin by loading the data we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONunhs7PDO6B"
   },
   "outputs": [],
   "source": [
    "penn_tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
    "penn_sents = nltk.corpus.treebank.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIlSUz_uDO6E"
   },
   "source": [
    "#### Data Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "znPR6DKVDO6F"
   },
   "source": [
    "As we mentioned earlier, for most machine learning POS taggers, a split of the original dataset into the *training* set and *test* set is necessary.\n",
    "As Training and testing a model on a single dataset can easily lead to the problem of [overfitting](https://en.wikipedia.org/wiki/Overfitting), as a tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score, but would be useless for tagging new text. \n",
    "Therefore, another held-out test set is used after training the model with training set to evaluate the generalizability of the model.\n",
    "\n",
    "In this section, we split the data, training on 90% and testing on the remaining 10%, and this proportion may vary depending on the task and data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq2hvscwDO6H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sents: 3522 3522\n",
      "Test sents: 392 392\n"
     ]
    }
   ],
   "source": [
    "train_prop = 0.9\n",
    "size = int(len(penn_tagged_sents) * train_prop)\n",
    "train_tagged_sents = penn_tagged_sents[:size]\n",
    "test_tagged_sents = penn_tagged_sents[size:]\n",
    "train_sents = penn_sents[:size]\n",
    "test_sents = penn_sents[size:]\n",
    "print('Training sents: {} {}'.format(len(train_sents), len(train_tagged_sents)))\n",
    "print('Test sents: {} {}'.format(len(test_sents), len(test_tagged_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAf_c5rfDO6Q"
   },
   "source": [
    "Now we split the original annotated dataset into a training and test set. we will evaluate ALL taggers on the test set, and use training data for taggers that require data to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zq1xHxy7DO6S"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "1. Create the tag distribution plot of both *training* and *test* set of Penn Treebank Corpus, and compare them with the plot of the whole corpus. What difference can you see?\n",
    "2. Measure the out-of-vocabulary (OOV) rate between the training and test sets:\n",
    "    - report the number of tokens and token types in the 90% training set;\n",
    "    - report the number of tokens and token types in the 10% test set;\n",
    "    - report the TEST token OOV rate and token type OOV rate, i.e. how many of test that are not present in training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBaoeH0nDO6U"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8DdBzBZDO6U"
   },
   "source": [
    "Evaluation is another core part of NLP.\n",
    "When evaluating our models on test set annotated by human experts, what numbers are we going to report? An intuitive one is the *accuracy* comparing the gold standard test set tags and the predicted tags produced by the model, i.e. the rate of correct tags the model has predicted.\n",
    "\n",
    "Recall the term *presicion*, *recall* and *F1-score* for 2-class classification in previous modules, it is still applicable in POS tagging, where we can have these number for EACH POS tag category (NN v.s. non NN). We can achieve this by building a *confusion matrix* of the outputs.\n",
    "\n",
    "Let's use the tagger in the very beginning of the section and create such confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avsdYKNeDO6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |                        -                                                                                                                                                                                          |\n",
      "       |                   -    N    -                                                                                                                                                                                     |\n",
      "       |                   L    O    R                                                                          N                        P                                                                                 |\n",
      "       |                   R    N    R                                                 J    J              N    N    N    P    P    P    R         R    R                   V    V    V    V    V    W         W    W      |\n",
      "       |         '         B    E    B              C    C    D    E    F    I    J    J    J    M    N    N    P    N    D    O    R    P    R    B    B    R    T    V    B    B    B    B    B    D    W    P    R    ` |\n",
      "       |    $    '    ,    -    -    -    .    :    C    D    T    X    W    N    J    R    S    D    N    P    S    S    T    S    P    $    B    R    S    P    O    B    D    G    N    P    Z    T    P    $    B    ` |\n",
      "-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "     $ |  <98>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    '' |    .  <42>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "     , |    .    . <434>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      " -LRB- |    .    .    .   <.>   .    .    .    .    .    .    .    .    .    .    2    .    .    .    5    4    .    .    .    .    .    .    .    .    .    .    .    .    1    .    .    .    .    .    .    .    .    . |\n",
      "-NONE- |    .    .    .    .   <.>   .    .    .    6  131    .    .    1    1  238    1    .    .  153   61    .   21    .    .    .    .   10    .    .    .    .    2    1    .    1   23    3    .    .    .    .    . |\n",
      " -RRB- |    .    .    .    .    .   <.>   .    .    .    .    .    .    .    .    1    .    .    .    4    7    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "     . |    .    .    .    .    .    . <384>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "     : |    .    .    .    .    .    .    .  <38>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    CC |    .    .    .    .    .    .    .    . <216>   .    .    .    1    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    CD |    .    .    .    .    .    .    .    .    . <380>   .    .    .    .    3    .    .    .    .    1    .    .    .    1    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    DT |    .    .    .    .    .    .    .    .    .    . <807>   .    .    2    .    .    .    .    1    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    EX |    .    .    .    .    .    .    .    .    .    .    .   <3>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    FW |    .    .    .    .    .    .    .    .    .    .    .    .   <.>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    IN |    .    .    .    .    .    .    .    .    .    .    .    .    . <935>   1    .    .    .    2    .    .    .    .    .    .    .   10    .    .   10    .    .    .    .    .    .    .    3    .    .    .    . |\n",
      "    JJ |    .    .    .    .    .    .    .    .    .    .    .    .    .    3 <512>   .    .    .   20    6    .    .    .    .    .    .    9    .    .    .    .    1    2    3    2    1    .    .    .    .    .    . |\n",
      "   JJR |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <26>   .    .    .    .    .    .    .    .    .    .    .    9    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   JJS |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <21>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    1    .    .    .    .    .    .    .    .    .    . |\n",
      "    MD |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . <101>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    NN |    .    .    .    .    .    .    .    .    .    1    1    .    1    .   47    .    .    1<1344>  20    .   17    .    .    .    .    4    .    .    .    .    2    .    5    .    .    1    .    .    .    .    . |\n",
      "   NNP |    .    .    .    .    .    .    .    .    .    .    .    .    .    .   10    .    .    .    4 <898>   1    3    .    .    .    .    .    .    .    .    .    .    .    5    .    .    .    .    .    .    .    . |\n",
      "  NNPS |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   10  <16>   1    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   NNS |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    3    1    . <587>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   PDT |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    1    .    .    .    .    .    .    .   <3>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   POS |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <94>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   PRP |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . <112>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "  PRP$ |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <57>   .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    RB |    .    .    .    .    .    .    .    .    .    .    1    .    .    4    7    .    .    .    2    1    .    1    .    .    .    . <221>   1    .    .    .    .    1    .    .    .    .    .    .    .    .    . |\n",
      "   RBR |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    4    .    .    .    .    .    .    .    .    .    .    .   <7>   .    .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "   RBS |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   <1>   .    .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    RP |    .    .    .    .    .    .    .    .    .    .    .    .    .    2    .    .    .    .    .    .    .    .    .    .    .    .    2    .    .  <18>   .    .    .    .    .    .    .    .    .    .    .    . |\n",
      "    TO |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    . <214>   .    .    .    .    .    .    .    .    .    .    . |\n",
      "    VB |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    4    .    .    .    .    .    .    .    .    .    .    .    . <239>   .    .    .    1    .    .    .    .    .    . |\n",
      "   VBD |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    2    .    .    .    2    1    .    .    .    .    .    .    .    .    .    .    .    . <328>   .    8    .    1    .    .    .    .    . |\n",
      "   VBG |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    3    .    .    .   14    .    .    .    .    .    .    .    .    .    .    .    .    .    . <135>   .    .    .    .    .    .    .    . |\n",
      "   VBN |    .    .    .    .    .    .    .    .    .    .    .    .    .    .   26    .    .    .    1    1    .    .    .    .    .    .    1    .    .    .    .    .   17    . <152>   .    .    .    .    .    .    . |\n",
      "   VBP |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    5    1    .    .    .    .    .    .    .    .    .    .    .    2    1    .    .  <89>   1    .    .    .    .    . |\n",
      "   VBZ |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    1    .    8    .    .    .    .    .    .    .    .    .    .    .    .    .    . <157>   .    .    .    .    . |\n",
      "   WDT |    .    .    .    .    .    .    .    .    .    .    .    .    .   19    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <24>   .    .    .    . |\n",
      "    WP |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <13>   .    .    . |\n",
      "   WP$ |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .   <4>   .    . |\n",
      "   WRB |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    1    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <17>   . |\n",
      "    `` |    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .    .  <43>|\n",
      "-------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold_test_tags = [tag for test_tagged_sent in test_tagged_sents for (word, tag) in test_tagged_sent]\n",
    "pred_test_tags = [tag for test_sent in test_sents for (word, tag) in nltk.pos_tag(test_sent)]\n",
    "print(nltk.ConfusionMatrix(gold_test_tags, pred_test_tags).pretty_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Andl53zVDO6X"
   },
   "source": [
    "As we can see, the row values indicate the POS tags given by reference (gold standard test set), and the column tags are from the model output.\n",
    "\n",
    "In order to calculate the metrics we mentioned earlier, let's first read the whole matrix in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_iXYufyDO6Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 98   0   0 ...   0   0   0]\n",
      " [  0  42   0 ...   0   0   0]\n",
      " [  0   0 434 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...   4   0   0]\n",
      " [  0   0   0 ...   0  17   0]\n",
      " [  0   0   0 ...   0   0  43]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "cm = nltk.ConfusionMatrix(gold_test_tags, pred_test_tags)\n",
    "cm_matrix = np.array(cm._confusion)\n",
    "print(cm_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ceg0Y4mQDO6f"
   },
   "source": [
    "The total number of tags are just the sum of the matrix value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5eF8yAI8DO6h"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9825"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tags = np.sum(cm_matrix)\n",
    "total_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RnA2NbNqDO6m"
   },
   "source": [
    "*Accuracy* is simply the ratial between the correct tags (sum of the diagnoal value called *trace*) and total tag number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WpT3mijBDO6m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8926208651399491"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = np.trace(cm_matrix) / total_tags\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLR3CjKJDO6q"
   },
   "source": [
    "Taking **NN** as an example, we want to calualte the *precision*, *recall* and *F1-score*.\n",
    "First we need to calculate true positive, false positive and false negative.\n",
    "True positive is simply the entry where both row and column are **NN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sERKpF1dDO6r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1344"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_idx = cm._indices['NN']\n",
    "tp = cm_matrix[nn_idx][nn_idx]\n",
    "tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYamHpGWDO6v"
   },
   "source": [
    "False positive is the sum of *NN* column values exluding true positive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HptD6yRpDO6v"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = sum(cm_matrix[:, nn_idx]) - tp\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-ixUhdRDO6y"
   },
   "source": [
    "Similarly, False negative is the sum of *NN* row values exluding true positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9anYdmXDO6y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = sum(cm_matrix[nn_idx, :]) - tp\n",
    "fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84GxTQeTDO62"
   },
   "source": [
    "The precision, recall and F1-score can be caclculated accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S56TbzMWDO63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8593350383631714 0.9307479224376731 0.8936170212765957\n"
     ]
    }
   ],
   "source": [
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xXq2EdxtDO68"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "1. Calculate the three metrics for each POS category with the same tagger, and complete the table below.\n",
    "\n",
    "| POS           | Precision     | Recall  | F1-score \n",
    "| ------------- |---------------|---------|----------\n",
    "| NN      | 0.86 | 0.93 | 0.89 \n",
    "| $       |      |      |      \n",
    "| ''      |      |      |      \n",
    "| Add tag here   |      |      |      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-fTGxzsDO69"
   },
   "source": [
    "#### Default tagger\n",
    "The simplest possible tagger assigns the same tag to each token. This may seem to be a rather banal step, but it establishes an important baseline for tagger performance. \n",
    "In order to get the best result, we tag each word with the most likely tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5foR3yhNDO69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(gold_test_tags).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25jgzTOaDO7B"
   },
   "source": [
    "Now we can create a tagger that tags everything as **NN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3N-ARB82DO7C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kalamazoo', 'NN'),\n",
       " (',', 'NN'),\n",
       " ('Mich.-based', 'NN'),\n",
       " ('First', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('America', 'NN'),\n",
       " ('said', 'NN'),\n",
       " ('0', 'NN'),\n",
       " ('it', 'NN'),\n",
       " ('will', 'NN'),\n",
       " ('eliminate', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('13', 'NN'),\n",
       " ('management', 'NN'),\n",
       " ('positions', 'NN'),\n",
       " ('of', 'NN'),\n",
       " ('the', 'NN'),\n",
       " ('former', 'NN'),\n",
       " ('Midwest', 'NN'),\n",
       " ('Financial', 'NN')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "default_preds = [(word, tag) for test_sent in test_sents for (word, tag) in default_tagger.tag(test_sent)]\n",
    "default_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1SjegFcDO7E"
   },
   "source": [
    "The accuracy of this tagger is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hsMtyVuDO7H"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14697201017811704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_pred_test_tags = [tag for (word, tag) in default_preds]\n",
    "default_cm = nltk.ConfusionMatrix(gold_test_tags, default_pred_test_tags)\n",
    "default_cm_matrix = np.array(default_cm._confusion)\n",
    "acc = np.trace(default_cm_matrix) / np.sum(default_cm_matrix)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p8yoUd18DO7M"
   },
   "source": [
    "Alternatively, we can directly calculate accuracy with `evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtTPYChjDO7O"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14697201017811704"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_sys = default_tagger.evaluate(test_tagged_sents)\n",
    "acc_sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-m9nU5CWDO7T"
   },
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "1. Calculate the three metrics for each POS category with the **default** tagger, and complete the table below.\n",
    "\n",
    "| POS           | Precision     | Recall  | F1-score \n",
    "| ------------- |---------------|---------|----------\n",
    "| NN            |  |  |  \n",
    "| $             |      |      |      \n",
    "| ''            |      |      |      \n",
    "| Add tag here  |      |      |      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fogy2wPBDO7V"
   },
   "source": [
    "#### The most frequent 100 unigram tagger\n",
    "A lot of high-frequency words do not have the **NN** tag. \n",
    "Let’s find the 100 most frequent words and store their most likely tag. We can then use this information as the\n",
    "model for a *lookup tagger*, which is an NLTK UnigramTagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvRyNfEGDO7V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pierre': 'NNP', 'Vinken': 'NNP', ',': ',', '61': 'CD', 'years': 'NNS', 'old': 'JJ', 'will': 'MD', 'join': 'VB', 'the': 'DT', 'board': 'NN', 'as': 'IN', 'a': 'DT', 'nonexecutive': 'JJ', 'director': 'NN', 'Nov.': 'NNP', '29': 'CD', '.': '.', 'Mr.': 'NNP', 'is': 'VBZ', 'chairman': 'NN', 'of': 'IN', 'Elsevier': 'NNP', 'N.V.': 'NNP', 'Dutch': 'JJ', 'publishing': 'NN', 'group': 'NN', 'Rudolph': 'NNP', 'Agnew': 'NNP', '55': 'CD', 'and': 'CC', 'former': 'JJ', 'Consolidated': 'NNP', 'Gold': 'NNP', 'Fields': 'NNP', 'PLC': 'NNP', 'was': 'VBD', 'named': 'VBN', '*-1': '-NONE-', 'this': 'DT', 'British': 'JJ', 'industrial': 'JJ', 'conglomerate': 'NN', 'A': 'DT', 'form': 'NN', 'asbestos': 'NN', 'once': 'RB', 'used': 'VBN', '*': '-NONE-', 'to': 'TO', 'make': 'VB', 'Kent': 'NNP', 'cigarette': 'NN', 'filters': 'NNS', 'has': 'VBZ', 'caused': 'VBN', 'high': 'JJ', 'percentage': 'NN', 'cancer': 'NN', 'deaths': 'NNS', 'among': 'IN', 'workers': 'NNS', 'exposed': 'VBN', 'it': 'PRP', 'more': 'JJR', 'than': 'IN', '30': 'CD', 'ago': 'IN', 'researchers': 'NNS', 'reported': 'VBD', '0': '-NONE-', '*T*-1': '-NONE-', 'The': 'DT', 'fiber': 'NN', 'crocidolite': 'NN', 'unusually': 'RB', 'resilient': 'JJ', 'enters': 'VBZ', 'lungs': 'NNS', 'with': 'IN', 'even': 'RB', 'brief': 'JJ', 'exposures': 'NNS', 'causing': 'VBG', 'symptoms': 'NNS', 'that': 'IN', 'show': 'NN', 'up': 'RP', 'decades': 'NNS', 'later': 'JJ', 'said': 'VBD', '*T*-2': '-NONE-', 'Lorillard': 'NNP', 'Inc.': 'NNP', 'unit': 'NN', 'New': 'NNP', 'York-based': 'JJ', 'Loews': 'NNP', 'Corp.': 'NNP', 'makes': 'VBZ', 'cigarettes': 'NNS'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32111959287531805"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a FreqDist of word unigram from training corpus\n",
    "fd = nltk.FreqDist([word for train_tagged_sent in train_tagged_sents for (word, tag) in train_tagged_sent])\n",
    "# get the 100 most frequent unigram word \n",
    "most_freq_words = list(fd.keys())[:100]\n",
    "# create a ConditionalFreqDist associating the word and POS tags\n",
    "cfd = nltk.ConditionalFreqDist([(word, tag) for train_tagged_sent in train_tagged_sents for (word, tag) in train_tagged_sent])\n",
    "# get the most likely tag for each word\n",
    "likely_tags = dict((word, cfd[word].max()) for word in most_freq_words)\n",
    "print(likely_tags)\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)\n",
    "baseline_tagger.evaluate(test_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to tag some sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Although', None),\n",
       " ('preliminary', None),\n",
       " ('findings', None),\n",
       " ('were', None),\n",
       " ('reported', 'VBD'),\n",
       " ('*-2', None),\n",
       " ('more', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('year', None),\n",
       " ('ago', 'IN'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('latest', None),\n",
       " ('results', None),\n",
       " ('appear', None),\n",
       " ('in', None),\n",
       " ('today', None),\n",
       " (\"'s\", None),\n",
       " ('New', 'NNP'),\n",
       " ('England', None),\n",
       " ('Journal', None),\n",
       " ('of', 'IN'),\n",
       " ('Medicine', None),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('forum', None),\n",
       " ('likely', None),\n",
       " ('*', '-NONE-'),\n",
       " ('to', 'TO'),\n",
       " ('bring', None),\n",
       " ('new', None),\n",
       " ('attention', None),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('problem', None),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(train_sents[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many words have been assigned a tag of **None**, because they were not among the 100 most frequent words. In these cases we would like to assign the default tag of **NN**. In other words, we want to use the lookup table first, and if it is unable to assign a tag, then use the default tagger, a process known as **backoff**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags, \n",
    "                                     backoff=nltk.DefaultTagger('NN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the lookup tagger will only store word-tag pairs for words other than nouns, and whenever it cannot assign a tag to a word, it will invoke the default tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46575063613231554"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(test_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And tagging the same sentence will give us different results now, where **None** is substituted by **NN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Although', 'NN'),\n",
       " ('preliminary', 'NN'),\n",
       " ('findings', 'NN'),\n",
       " ('were', 'NN'),\n",
       " ('reported', 'VBD'),\n",
       " ('*-2', 'NN'),\n",
       " ('more', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('ago', 'IN'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('latest', 'NN'),\n",
       " ('results', 'NN'),\n",
       " ('appear', 'NN'),\n",
       " ('in', 'NN'),\n",
       " ('today', 'NN'),\n",
       " (\"'s\", 'NN'),\n",
       " ('New', 'NNP'),\n",
       " ('England', 'NN'),\n",
       " ('Journal', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Medicine', 'NN'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('forum', 'NN'),\n",
       " ('likely', 'NN'),\n",
       " ('*', '-NONE-'),\n",
       " ('to', 'TO'),\n",
       " ('bring', 'NN'),\n",
       " ('new', 'NN'),\n",
       " ('attention', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('problem', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(train_sents[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "\n",
    "1. Calculate the three metrics for each POS category with the **lookup** tagger with **backoff**, and complete the table below.\n",
    "\n",
    "| POS           | Precision     | Recall  | F1-score \n",
    "| ------------- |---------------|---------|----------\n",
    "| NN            |  |  |  \n",
    "| $             |      |      |      \n",
    "| ''            |      |      |      \n",
    "| Add tag here  |      |      |      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous 100 most frequent lookup tagger is essentially a **unigram** tagger, where we consider only the current token, in isolation from any larger context. \n",
    "Given such a model, the best we can do is tag each word with\n",
    "its a *priori* most likely tag learned from the training data. \n",
    "This means we would tag a word such as *wind* with the same tag, regardless of whether it appears in the context *the wind* or *to wind*.\n",
    "\n",
    "An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the POS tags of the $n-1$ preceding tokens, as shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ngram_tagger](./ngram_tagger.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the tag to be chosen, $t_n$, is circled, and the context is shaded in grey. \n",
    "In the example of an n-gram tagger, we have $n=3$, i.e. a **tri-gram** tagger; that is, the model will pick the most likely $t_n$ given the tags of the 2 preceding words in addition to the current word.\n",
    "\n",
    "Recall again the unigram tagger, we have $n=1$, where it doesn't consider any previous word tags, but only the current word token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bi-gram tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, when we have larger $n$, the model will learn more specific situations which can be potentially helpful for our tagging.\n",
    "However, given the limited training data, the model will also learn either too specific or irrelevant information (noise) which cannot generalize well to the test data.\n",
    "We call this data sparsity problem.\n",
    "\n",
    "Consider the following example:\n",
    "\n",
    "I/PRP really/RB like/VBP to/TO **wind** ...\n",
    "\n",
    "I/PRP like/VBP to/TO **wind** ...\n",
    "\n",
    "Now if we want to decide the tag of the word **wind**, for a tri-gram tagger both sentences will give us the same information: (VPB, TO, wind).\n",
    "However, if we use a 4-gram tagger, the first sentence will give us (RB, VBP, TO, wind) whereas the second one will give us (PRP, VBP, TO, wind).\n",
    "It will\n",
    "1. reduce the signal of (VPB, TO, wind) which is useful for tagging, as (RB, VBP, TO, wind) and (PRP, VBP, TO, wind) will be counted separately once where in trigram tagger (VPB, TO, wind) will be counted twice, which is the main cause of data sparsity.\n",
    "2. learn irrelevant information from (RB, VBP, TO, wind) and (PRP, VBP, TO, wind) as the first tag really doesn't help us better in tagging.\n",
    "\n",
    "As a consequence, there is always a trade-off between the accuracy and the coverage of our results, which is closely related to the concept of overfitting, where for a certain amount of training data, we do not want our model to be too complex to perform well on the real-world unseen data.\n",
    "\n",
    "Generally, in NLP practice, a **bi-gram** tagger is sufficient to obtain a good test performance for the POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13455470737913486"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the tagger with training data\n",
    "t2 = nltk.BigramTagger(train_tagged_sents)\n",
    "# evaluate the tagger on the test data\n",
    "t2.evaluate(test_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Although', 'IN'),\n",
       " ('preliminary', 'JJ'),\n",
       " ('findings', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('reported', 'VBN'),\n",
       " ('*-2', '-NONE-'),\n",
       " ('more', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('ago', 'IN'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('latest', 'JJS'),\n",
       " ('results', 'NNS'),\n",
       " ('appear', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('today', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('New', 'NNP'),\n",
       " ('England', 'NNP'),\n",
       " ('Journal', 'NNP'),\n",
       " ('of', 'IN'),\n",
       " ('Medicine', 'NNP'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('forum', 'NN'),\n",
       " ('likely', 'JJ'),\n",
       " ('*', '-NONE-'),\n",
       " ('to', 'TO'),\n",
       " ('bring', 'VB'),\n",
       " ('new', 'JJ'),\n",
       " ('attention', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('problem', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.tag(train_sents[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('After', 'IN'),\n",
       " ('these', 'DT'),\n",
       " ('payments', 'NNS'),\n",
       " (',', ','),\n",
       " ('about', 'IN'),\n",
       " ('$', '$'),\n",
       " ('225,000', None),\n",
       " ('*U*', None),\n",
       " ('will', None),\n",
       " ('be', None),\n",
       " ('available', None),\n",
       " ('for', None),\n",
       " ('the', None),\n",
       " ('20', None),\n",
       " ('million', None),\n",
       " ('common', None),\n",
       " ('shares', None),\n",
       " ('outstanding', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.tag(test_sents[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the the bi-gram tagger can tag sentences in the training data pretty well, but fails to tag the test data sentence and yields a lot of **None** tags, simply because when the tagger encounters a new combination of (previous_tag, current_word), it will output **None** as the predicted tag, and it immediately propagate to the following predictions with (None, current_word) tuples, as the tagger has never seen the tag **None** in training data.\n",
    "\n",
    "There are two ways to remedy for that.\n",
    "- As seen previously, we can use **backoff** to combine bi-gram tagger with unigram and default tagger.\n",
    "- We reserve a high frequency word list, and convert low frequency word into *UNK* so that the tagger won't encounter any *new* word in both training and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining different taggers is rather easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8905852417302799"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_tagged_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_tagged_sents, backoff=t1)\n",
    "t2.evaluate(test_tagged_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **<h3>💻 Try it yourself!</h3>**\n",
    "1. Create new training and test data by keeping a fixed vocabulary (reserve words whose frequency is larger than 3) and casting other words into UNK. Retrain the bi-gram tagger and its combined version, report the accuracy respectively.\n",
    "2. Calculate the three metrics for each POS category with the **lookup** tagger with **backoff**, and complete the table below.\n",
    "\n",
    "| POS           | Precision     | Recall  | F1-score \n",
    "| ------------- |---------------|---------|----------\n",
    "| NN            |  |  |  \n",
    "| $             |      |      |      \n",
    "| ''            |      |      |      \n",
    "| Add tag here  |      |      |      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rE7Pd4VgDO7a"
   },
   "source": [
    "** A simple bi-gram tagger\n",
    "Emphasize that a real n-gram tagger would do decoding using a dynamic programming algorithm called Viterbi (the students will have heard of this in the lecture).  Today in the interests of time we are not going to do decoding, we’re just going to assign the most frequent POS sequence to each bigram that matches (if indeed a bigram does match).\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "Low accuracy because of data sparsity – most bigrams were never seen during training.\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** Combining a models with backoff (e.g. unigram plus bigram tagger)\n",
    "** Further insights from evaluation using a confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQUEHuepDO7b"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "module_2.3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
