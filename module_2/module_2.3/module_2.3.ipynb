{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='background-image: url(\"../../resources/section_header.jpg\") ; padding: 0px ; background-size: cover ; border-radius: 5px ; height: 250px'>\n",
    "    <div style=\"float: right ; margin: 50px ; padding: 20px ; background: rgba(255 , 255 , 255 , 0.7) ; width: 50% ; height: 150px\">\n",
    "        <div style=\"position: relative ; top: 50% ; transform: translatey(-50%)\">\n",
    "            <div style=\"font-size: x-large ; font-weight: 900 ; color: rgba(0 , 115 , 207 , 0.9) ; line-height: 100%\">Python for Computational Linguists</div>\n",
    "            <div style=\"font-size: x-large ; padding-top: 20px ; color: rgba(0 , 115 , 207, 0.7)\">2.3 Part-of-speech Tagging</div>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We agreed to be flexible on the content and just to include as much as could comfortably fit into 2 hours with the potential for extensional activities.\n",
    " material from http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf\n",
    " data from\n",
    "http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html\n",
    "\n",
    "* Summary of key objectives\n",
    "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
    "** understand that part of speech tagging is an important early application of sequence classification;\n",
    "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
    "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
    "\n",
    "* Quick introduction (with material taken from the NLTK book and the lecture notes):\n",
    "** What is part of speech tagging?\n",
    "** What is a tagged corpus?\n",
    "** Using spaCy’s part of speech tagger to process a sequence of words\n",
    "Quick reminder of how to use spaCy’s POS tagger and some of the issues we raised, e.g. about domain adaptation, data quantity and quality. E.g. get the students to POS tag two contrasting sentences, e.g. lexically ambiguous homophones - one with a common homonym sense and another with a rare homonym sense (e.g. ‘It is wrong to object to this object’ or ‘I must present the present on his birthday’, ‘The insurance for the invalid was invalid’), or one sentence with a neologism and one without a neologism or one with a nonsense word/unknown word that is not a common noun (e.g. ‘he was scrobbling’). Encourage the students to play with the tagger by submitting their own challenging sentences.  Get the students to notice the set of POS tags and their meanings.  Get the students to see what happens if you normalise the text to lower case. \n",
    "In Module 1 we saw how to call spaCy’s POS tagger using a function call.  Whilst spaCy’s POS tagger is very good, we’re going to be going hands on by building our own POS simple n-gram tagger using the NLTK library.\n",
    "* Today’s goal: building and testing a part of speech tagger with NLTK\n",
    "* Getting started\n",
    "** Exploring tagged corpora (section 5.2 in the NLTK book)\n",
    "Look at several POS tagged corpora, e.g. the Brown corpus and the GENIA corpus (see data at http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html). \n",
    "Try to get the students thinking about the POS tag set – what do the categories mean?  Get the students to explore the corpora and gain insights, e.g. most frequent POS tag, least frequent POS tag, plotting the curve of the POS tag frequency distribution.  What kinds of words occur in the noun, adjective, adverb category?  \n",
    "Look at some n-gram sequences of POS tags to get the students thinking about n-grams.\n",
    "Look at some words which are POS ambiguous.\n",
    "** Mapping words to properties using Python dictionaries (section 5.3 in NLTK)\n",
    "Get the students thinking about linguistic objects as data structures, e.g. a dictionary could map from a headword to a POS or to a specific sense, e.g. in WordNet or Wikipedia. \n",
    "Dictionaries in Python again. Defining a POS dictionary by hand.  Creating a POS dictionary using a tagged corpus. \n",
    "** Creating a bigram dictionary\n",
    "Page 196 in the NLTK book shows how to create a dictionary of words and tags to perform bigram tagging;\n",
    "If we try to create an n-gram tagger for larger values of n what will happen?  We’ll encounter the sparse data problem and also start to reach the limits of computer memory.\n",
    "* Automatic tagging (section 5.4 in the NLTK book)\n",
    "** Separating training and testing data\n",
    "From page 225 in the NLTK book – uses the Brown corpus to make a 90:10 split\n",
    "Get the students to fill in a LaTeX table that characterises the data: distribution of tags in the whole corpus, in the 90% training set and the 10% testing set.\n",
    "** A simple default tagger \n",
    "Assigns the NN to each word in the Brown training corpus\n",
    "Evaluate it using the Brown testing corpus with recall, precision and F1 for each category\n",
    "Get the students to measure the out of vocabulary rate between the training and test data sets\n",
    "-\tAsk the students to report the number of types and tokens in the 90% training set, the 10% testing set and the OOV rate.\n",
    "Get the students to look and learn from the evaluation data\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "** A regular expression tagger  cut out\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "Again encourage the students to look and learn from the evaluation data\n",
    "Get the students to construct new regular expression patterns and to evaluate them\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** The most frequent 100 unigram tagger\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "Again encourage the students to look and learn from the evaluation data\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** A simple bi-gram tagger\n",
    "Emphasize that a real n-gram tagger would do decoding using a dynamic programming algorithm called Viterbi (the students will have heard of this in the lecture).  Today in the interests of time we are not going to do decoding, we’re just going to assign the most frequent POS sequence to each bigram that matches (if indeed a bigram does match).\n",
    "Again, evaluate it using recall, precision and F1 for each category \n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "Low accuracy because of data sparsity – most bigrams were never seen during training.\n",
    "Extension task: get the students to test their tagger on the GENIA corpus. What is the accuracy? What does this tell us about the tagger, about the training data (representativeness) and how could the tagger be improved?\n",
    "-\tGet the students to report F1 scores for each category and for all categories in a LaTeX table (template is provided).\n",
    "** Combining a models with backoff (e.g. unigram plus bigram tagger)\n",
    "** Further insights from evaluation using a confusion matrix\n",
    "* Reminder of key objectives\n",
    "** understand that many language processing tasks, such as part of speech tagging, can be viewed as text classification;\n",
    "** understand what a part of speech tag set is;\n",
    "** see in practice how a simple n-gram model can be used to perform part of speech tagging;\n",
    "** see in practice how linguistic features found in corpora can help language models to understand linguistic patterns, and be used to make predictions about new language data;\n",
    "\n",
    "* Homework: \n",
    "** (1 point): Run your bigram tagger on this made-up sentence ‘@Will WOOOHOO Will the New jPhone bOut 2night? Soexcited :)‘. Does the tagger get the correct answer?  Write down two challenges the tagger has incorrectly resolved.  looking here for an OOV word (WOOOHOOO) and  failure to disambiguate homonyms (e.g. Will).  The tagger may also not handle slang (2night), neologisms (jPhone) and tokenization errors (Soexcited).\n",
    "** (2 points): Write a program to find out:\n",
    "a.\twhich word has the most POS tags in the GENIA corpus.\n",
    "b.\twhat percentage of words in the GENIA corpus are ambiguous, i.e. have more than one POS tag.\n",
    "** (2 points):  Write a program to replace low frequency words (words with a frequency of 5 or less) with UNK in the Brown corpus.  How much does this improve your backoff tagger’s F-score performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module is built based on the lecture note about part-of-speech tagging and Chapter 5 of the [book](http://www.datascienceassn.org/sites/default/files/Natural%20Language%20Processing%20with%20Python.pdf), and students are strongly recommended to go through them.\n",
    "\n",
    "In this module, we are going to\n",
    "- a\n",
    "- b\n",
    "- c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>Part-of-speech Tagging (POS Tagging)</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is POS tag?\n",
    "For each word in context, we can assign a **lexical category**, such as noun, verbs, adjectives, etc.\n",
    "These categories are often referred to as a word's part-of-speech tag or POS tag.\n",
    "The set of all POS tags is called a **tagset**, and the activity of assigning these tags to words is referred to as **part-of-speech tagging** or simply as **POS tagging**.\n",
    "\n",
    "The number of categories we require for our tagset often depends on both linguistic and practical considerations.\n",
    "Very commonly used tagsets include the 87-tag Brown set, 45-tag Penn Treebank set, the 61-tag CLAWS 5 (C5), and the 17-tag Universal POS tagset. \n",
    "[Here](https://universaldependencies.org/u/pos/) is the list of tags of Universal POS tagset.\n",
    "Please check lecture note for more detials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we want to do POS tagging?\n",
    "As the POS tag of a word will give a large amount of information about this word and its neighbours, POS tagging can help understanding better texts, beneficial to many dowstream NLP tasks such as:\n",
    "\n",
    "1. Disambiguating word senses. Consider the following example for the heterophone *content*:\n",
    "    1. There was very little *content* to the essay.\n",
    "    2. The sleepy pug puppy was very *content*.\n",
    "  \n",
    "  or homonyms:\n",
    "  - It is wrong to *object* to this *object*.\n",
    "  - I must *present* the *present* on his birthday.\n",
    "  - The insurance for the *invalid* was *invalid*.\n",
    "\n",
    "2. Mitigating the issue of data sparsity. \n",
    "    - Label named entities like people, places or organizations as part of information extraction systems. In the following example, both *Chase Manhattan* and *J.P. Morgan* should be detected as proper nouns:\n",
    "        - Chase Manhattan and its merger partner J.P. Morgan.\n",
    "    \n",
    "    - Deal with out-of-vocabulary (OOV) words such as neologism words or acronyms in the social media: \n",
    "        - @username its #awesome u gonna ♥ it Chk out our cooool project on some_url + RT it.\n",
    "\n",
    "\n",
    "More generally, being able to automatically perform POS tagging will help reduce the laborious human effort required to parse a sentence, and it will be the main goal of this module, which is to build such an automatic model/tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: any quiz more interactive????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>Getting Started</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building our own tagger, we are going to use the NLTK library to load some existing tagger to perform POS tagging, and some tagged corpora will be introduced for further training and evaluations of our models.\n",
    "First let's import the [NLTK](https://www.nltk.org/) library and download some necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yz568/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/yz568/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the input text, we first tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'like', 'this', 'module', 'and', 'NLTK', 'library', 'very', 'very', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "text = 'I like this module and NLTK library very very much!'\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the tokenized text is a list where each element is a token. \n",
    "\n",
    "Now let's tag the tokenize text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('like', 'VBP'), ('this', 'DT'), ('module', 'NN'), ('and', 'CC'), ('NLTK', 'NNP'), ('library', 'JJ'), ('very', 'RB'), ('very', 'RB'), ('much', 'JJ'), ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_text = nltk.pos_tag(tokenized_text)\n",
    "print(tagged_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned tagged text is a list, where each element is a tuple. For each tuple, the first element is the previous token, and the second element is its corresponding tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag, e.g., `nltk.help.upenn_tagset('PRP')`, or a regular expression, e.g., `nltk.help.upenn_brown_tagset('NN.*')`. Some corpora have README files with tagset documentation; see `nltk.name.readme()`, substituting in the name of the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Use the current tagger to tag the previous text examples to see if the key words (heterophone, homonym, named entity and neologism) are correctly tagged.\n",
    "What are possible explanations to the wrong words?\n",
    "What happens if we normalise the text to lower case?\n",
    "Play with the tagger by submitting your own challenging sentences.\n",
    "\n",
    "- There was very little *content* to the essay.\n",
    "- The sleepy pug puppy was very *content*.\n",
    "- It is wrong to *object* to this *object*.\n",
    "- I must *present* the *present* on his birthday.\n",
    "- The insurance for the *invalid* was *invalid*.\n",
    "- They *refuse* to permit us to obtain the *refuse* permit\n",
    "- Chase Manhattan and its merger partner J.P. Morgan.\n",
    "- @username its #awesome u gonna ♥ it Chk out our cooool project on some_url + RT it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: have answers ready and ask students to identify any wrong ones, and ask them why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connection with Text Classification\n",
    "\n",
    "As we have seen in previous modules, text classification is the task where we assign a label to the whole sequence of text, e.g. document. POS tagging can be also viewd similarly. Instead of assigning a label on the document level, we assign a label to each word or token. This type of task is usually called **sequence labeling** or **sequence classification**, and POS tagging is one of the most important tasks of sequence classification in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagged corpora\n",
    "POS taggers using machine learning techniques generally require annotated data to train on.\n",
    "For all tagging models, including those that do not require any training data like regex- or rule-based models, an annotated evaluation dataset is also required to measure the performance of these models by comparing the model outputs with the gold standard annotations.\n",
    "\n",
    "Tagged corpora, generally tagged by an human expert, have the form similar to the following:\n",
    "\n",
    "```\n",
    "The/AT grand/JJ jury/NN commented/VBD on/IN a/AT number/NN of/IN other/AP topics/NNS ,/, AMONG/IN them/PPO the/AT Atlanta/NP and/CC Fulton/NP-tl County/NN-tl purchasing/VBG departments/NNS which/WDT it/PPS said/VBD ``/`` ARE/BER well/QL operated/VBN and/CC follow/VB generally/RB accepted/VBN practices/NNS which/WDT inure/VB to/IN the/AT best/JJT interest/NN of/IN both/ABX governments/NNS ''/'' ./.\n",
    "```\n",
    "Different datasets might have different forms, but each word (token) in sentences will be generally accompanied by a POS tag to form the pair **word/tag**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus) and the [Penn Treebank Corpus](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf) annotated with POS tags.\n",
    "Both corpora are already included in NLTK with POS tags, and we can get the full text by calling `tagged_words()` or `tagged_sents()` when the corpus is also segmented into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]\n",
      "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# read Brown Corpus\n",
    "print(nltk.corpus.brown.tagged_words())\n",
    "print(nltk.corpus.brown.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]\n",
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "# read Penn Treebank Corpus\n",
    "print(nltk.corpus.treebank.tagged_words())\n",
    "print(nltk.corpus.treebank.tagged_sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that the tags used in the two corpora seem different? They are indeed using different tagsets, and to investigate what each tag category means, we can check both tagsets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(: opening parenthesis\n",
      "    (\n",
      "): closing parenthesis\n",
      "    )\n",
      "*: negator\n",
      "    not n't\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ? ; ! :\n",
      ":: colon\n",
      "    :\n",
      "ABL: determiner/pronoun, pre-qualifier\n",
      "    quite such rather\n",
      "ABN: determiner/pronoun, pre-quantifier\n",
      "    all half many nary\n",
      "ABX: determiner/pronoun, double conjunction or pre-quantifier\n",
      "    both\n",
      "AP: determiner/pronoun, post-determiner\n",
      "    many other next more last former little several enough most least only\n",
      "    very few fewer past same Last latter less single plenty 'nough lesser\n",
      "    certain various manye next-to-last particular final previous present\n",
      "    nuf\n",
      "AP$: determiner/pronoun, post-determiner, genitive\n",
      "    other's\n",
      "AP+AP: determiner/pronoun, post-determiner, hyphenated pair\n",
      "    many-much\n",
      "AT: article\n",
      "    the an no a every th' ever' ye\n",
      "BE: verb 'to be', infinitive or imperative\n",
      "    be\n",
      "BED: verb 'to be', past tense, 2nd person singular or all persons plural\n",
      "    were\n",
      "BED*: verb 'to be', past tense, 2nd person singular or all persons plural, negated\n",
      "    weren't\n",
      "BEDZ: verb 'to be', past tense, 1st and 3rd person singular\n",
      "    was\n",
      "BEDZ*: verb 'to be', past tense, 1st and 3rd person singular, negated\n",
      "    wasn't\n",
      "BEG: verb 'to be', present participle or gerund\n",
      "    being\n",
      "BEM: verb 'to be', present tense, 1st person singular\n",
      "    am\n",
      "BEM*: verb 'to be', present tense, 1st person singular, negated\n",
      "    ain't\n",
      "BEN: verb 'to be', past participle\n",
      "    been\n",
      "BER: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    are art\n",
      "BER*: verb 'to be', present tense, 2nd person singular or all persons plural, negated\n",
      "    aren't ain't\n",
      "BEZ: verb 'to be', present tense, 3rd person singular\n",
      "    is\n",
      "BEZ*: verb 'to be', present tense, 3rd person singular, negated\n",
      "    isn't ain't\n",
      "CC: conjunction, coordinating\n",
      "    and or but plus & either neither nor yet 'n' and/or minus an'\n",
      "CD: numeral, cardinal\n",
      "    two one 1 four 2 1913 71 74 637 1937 8 five three million 87-31 29-5\n",
      "    seven 1,119 fifty-three 7.5 billion hundred 125,000 1,700 60 100 six\n",
      "    ...\n",
      "CD$: numeral, cardinal, genitive\n",
      "    1960's 1961's .404's\n",
      "CS: conjunction, subordinating\n",
      "    that as after whether before while like because if since for than altho\n",
      "    until so unless though providing once lest s'posin' till whereas\n",
      "    whereupon supposing tho' albeit then so's 'fore\n",
      "DO: verb 'to do', uninflected present tense, infinitive or imperative\n",
      "    do dost\n",
      "DO*: verb 'to do', uninflected present tense or imperative, negated\n",
      "    don't\n",
      "DO+PPSS: verb 'to do', past or present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    d'you\n",
      "DOD: verb 'to do', past tense\n",
      "    did done\n",
      "DOD*: verb 'to do', past tense, negated\n",
      "    didn't\n",
      "DOZ: verb 'to do', present tense, 3rd person singular\n",
      "    does\n",
      "DOZ*: verb 'to do', present tense, 3rd person singular, negated\n",
      "    doesn't don't\n",
      "DT: determiner/pronoun, singular\n",
      "    this each another that 'nother\n",
      "DT$: determiner/pronoun, singular, genitive\n",
      "    another's\n",
      "DT+BEZ: determiner/pronoun + verb 'to be', present tense, 3rd person singular\n",
      "    that's\n",
      "DT+MD: determiner/pronoun + modal auxillary\n",
      "    that'll this'll\n",
      "DTI: determiner/pronoun, singular or plural\n",
      "    any some\n",
      "DTS: determiner/pronoun, plural\n",
      "    these those them\n",
      "DTS+BEZ: pronoun, plural + verb 'to be', present tense, 3rd person singular\n",
      "    them's\n",
      "DTX: determiner, pronoun or double conjunction\n",
      "    neither either one\n",
      "EX: existential there\n",
      "    there\n",
      "EX+BEZ: existential there + verb 'to be', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+HVD: existential there + verb 'to have', past tense\n",
      "    there'd\n",
      "EX+HVZ: existential there + verb 'to have', present tense, 3rd person singular\n",
      "    there's\n",
      "EX+MD: existential there + modal auxillary\n",
      "    there'll there'd\n",
      "FW-*: foreign word: negator\n",
      "    pas non ne\n",
      "FW-AT: foreign word: article\n",
      "    la le el un die der ein keine eine das las les Il\n",
      "FW-AT+NN: foreign word: article + noun, singular, common\n",
      "    l'orchestre l'identite l'arcade l'ange l'assistance l'activite\n",
      "    L'Universite l'independance L'Union L'Unita l'osservatore\n",
      "FW-AT+NP: foreign word: article + noun, singular, proper\n",
      "    L'Astree L'Imperiale\n",
      "FW-BE: foreign word: verb 'to be', infinitive or imperative\n",
      "    sit\n",
      "FW-BER: foreign word: verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    sind sunt etes\n",
      "FW-BEZ: foreign word: verb 'to be', present tense, 3rd person singular\n",
      "    ist est\n",
      "FW-CC: foreign word: conjunction, coordinating\n",
      "    et ma mais und aber och nec y\n",
      "FW-CD: foreign word: numeral, cardinal\n",
      "    une cinq deux sieben unam zwei\n",
      "FW-CS: foreign word: conjunction, subordinating\n",
      "    bevor quam ma\n",
      "FW-DT: foreign word: determiner/pronoun, singular\n",
      "    hoc\n",
      "FW-DT+BEZ: foreign word: determiner + verb 'to be', present tense, 3rd person singular\n",
      "    c'est\n",
      "FW-DTS: foreign word: determiner/pronoun, plural\n",
      "    haec\n",
      "FW-HV: foreign word: verb 'to have', present tense, not 3rd person singular\n",
      "    habe\n",
      "FW-IN: foreign word: preposition\n",
      "    ad de en a par con dans ex von auf super post sine sur sub avec per\n",
      "    inter sans pour pendant in di\n",
      "FW-IN+AT: foreign word: preposition + article\n",
      "    della des du aux zur d'un del dell'\n",
      "FW-IN+NN: foreign word: preposition + noun, singular, common\n",
      "    d'etat d'hotel d'argent d'identite d'art\n",
      "FW-IN+NP: foreign word: preposition + noun, singular, proper\n",
      "    d'Yquem d'Eiffel\n",
      "FW-JJ: foreign word: adjective\n",
      "    avant Espagnol sinfonica Siciliana Philharmonique grand publique haute\n",
      "    noire bouffe Douce meme humaine bel serieuses royaux anticus presto\n",
      "    Sovietskaya Bayerische comique schwarzen ...\n",
      "FW-JJR: foreign word: adjective, comparative\n",
      "    fortiori\n",
      "FW-JJT: foreign word: adjective, superlative\n",
      "    optimo\n",
      "FW-NN: foreign word: noun, singular, common\n",
      "    ballet esprit ersatz mano chatte goutte sang Fledermaus oud def kolkhoz\n",
      "    roi troika canto boite blutwurst carne muzyka bonheur monde piece force\n",
      "    ...\n",
      "FW-NN$: foreign word: noun, singular, common, genitive\n",
      "    corporis intellectus arte's dei aeternitatis senioritatis curiae\n",
      "    patronne's chambre's\n",
      "FW-NNS: foreign word: noun, plural, common\n",
      "    al culpas vopos boites haflis kolkhozes augen tyrannis alpha-beta-\n",
      "    gammas metis banditos rata phis negociants crus Einsatzkommandos\n",
      "    kamikaze wohaws sabinas zorrillas palazzi engages coureurs corroborees\n",
      "    yori Ubermenschen ...\n",
      "FW-NP: foreign word: noun, singular, proper\n",
      "    Karshilama Dieu Rundfunk Afrique Espanol Afrika Spagna Gott Carthago\n",
      "    deus\n",
      "FW-NPS: foreign word: noun, plural, proper\n",
      "    Svenskarna Atlantes Dieux\n",
      "FW-NR: foreign word: noun, singular, adverbial\n",
      "    heute morgen aujourd'hui hoy\n",
      "FW-OD: foreign word: numeral, ordinal\n",
      "    18e 17e quintus\n",
      "FW-PN: foreign word: pronoun, nominal\n",
      "    hoc\n",
      "FW-PP$: foreign word: determiner, possessive\n",
      "    mea mon deras vos\n",
      "FW-PPL: foreign word: pronoun, singular, reflexive\n",
      "    se\n",
      "FW-PPL+VBZ: foreign word: pronoun, singular, reflexive + verb, present tense, 3rd person singular\n",
      "    s'excuse s'accuse\n",
      "FW-PPO: pronoun, personal, accusative\n",
      "    lui me moi mi\n",
      "FW-PPO+IN: foreign word: pronoun, personal, accusative + preposition\n",
      "    mecum tecum\n",
      "FW-PPS: foreign word: pronoun, personal, nominative, 3rd person singular\n",
      "    il\n",
      "FW-PPSS: foreign word: pronoun, personal, nominative, not 3rd person singular\n",
      "    ich vous sie je\n",
      "FW-PPSS+HV: foreign word: pronoun, personal, nominative, not 3rd person singular + verb 'to have', present tense, not 3rd person singular\n",
      "    j'ai\n",
      "FW-QL: foreign word: qualifier\n",
      "    minus\n",
      "FW-RB: foreign word: adverb\n",
      "    bas assai deja um wiederum cito velociter vielleicht simpliciter non zu\n",
      "    domi nuper sic forsan olim oui semper tout despues hors\n",
      "FW-RB+CC: foreign word: adverb + conjunction, coordinating\n",
      "    forisque\n",
      "FW-TO+VB: foreign word: infinitival to + verb, infinitive\n",
      "    d'entretenir\n",
      "FW-UH: foreign word: interjection\n",
      "    sayonara bien adieu arigato bonjour adios bueno tchalo ciao o\n",
      "FW-VB: foreign word: verb, present tense, not 3rd person singular, imperative or infinitive\n",
      "    nolo contendere vive fermate faciunt esse vade noli tangere dites duces\n",
      "    meminisse iuvabit gosaimasu voulez habla ksu'u'peli'afo lacheln miuchi\n",
      "    say allons strafe portant\n",
      "FW-VBD: foreign word: verb, past tense\n",
      "    stabat peccavi audivi\n",
      "FW-VBG: foreign word: verb, present participle or gerund\n",
      "    nolens volens appellant seq. obliterans servanda dicendi delenda\n",
      "FW-VBN: foreign word: verb, past participle\n",
      "    vue verstrichen rasa verboten engages\n",
      "FW-VBZ: foreign word: verb, present tense, 3rd person singular\n",
      "    gouverne sinkt sigue diapiace\n",
      "FW-WDT: foreign word: WH-determiner\n",
      "    quo qua quod que quok\n",
      "FW-WPO: foreign word: WH-pronoun, accusative\n",
      "    quibusdam\n",
      "FW-WPS: foreign word: WH-pronoun, nominative\n",
      "    qui\n",
      "HV: verb 'to have', uninflected present tense, infinitive or imperative\n",
      "    have hast\n",
      "HV*: verb 'to have', uninflected present tense or imperative, negated\n",
      "    haven't ain't\n",
      "HV+TO: verb 'to have', uninflected present tense + infinitival to\n",
      "    hafta\n",
      "HVD: verb 'to have', past tense\n",
      "    had\n",
      "HVD*: verb 'to have', past tense, negated\n",
      "    hadn't\n",
      "HVG: verb 'to have', present participle or gerund\n",
      "    having\n",
      "HVN: verb 'to have', past participle\n",
      "    had\n",
      "HVZ: verb 'to have', present tense, 3rd person singular\n",
      "    has hath\n",
      "HVZ*: verb 'to have', present tense, 3rd person singular, negated\n",
      "    hasn't ain't\n",
      "IN: preposition\n",
      "    of in for by considering to on among at through with under into\n",
      "    regarding than since despite according per before toward against as\n",
      "    after during including between without except upon out over ...\n",
      "IN+IN: preposition, hyphenated pair\n",
      "    f'ovuh\n",
      "IN+PPO: preposition + pronoun, personal, accusative\n",
      "    t'hi-im\n",
      "JJ: adjective\n",
      "    ecent over-all possible hard-fought favorable hard meager fit such\n",
      "    widespread outmoded inadequate ambiguous grand clerical effective\n",
      "    orderly federal foster general proportionate ...\n",
      "JJ$: adjective, genitive\n",
      "    Great's\n",
      "JJ+JJ: adjective, hyphenated pair\n",
      "    big-large long-far\n",
      "JJR: adjective, comparative\n",
      "    greater older further earlier later freer franker wider better deeper\n",
      "    firmer tougher faster higher bigger worse younger lighter nicer slower\n",
      "    happier frothier Greater newer Elder ...\n",
      "JJR+CS: adjective + conjunction, coordinating\n",
      "    lighter'n\n",
      "JJS: adjective, semantically superlative\n",
      "    top chief principal northernmost master key head main tops utmost\n",
      "    innermost foremost uppermost paramount topmost\n",
      "JJT: adjective, superlative\n",
      "    best largest coolest calmest latest greatest earliest simplest\n",
      "    strongest newest fiercest unhappiest worst youngest worthiest fastest\n",
      "    hottest fittest lowest finest smallest staunchest ...\n",
      "MD: modal auxillary\n",
      "    should may might will would must can could shall ought need wilt\n",
      "MD*: modal auxillary, negated\n",
      "    cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't\n",
      "MD+HV: modal auxillary + verb 'to have', uninflected form\n",
      "    shouldda musta coulda must've woulda could've\n",
      "MD+PPSS: modal auxillary + pronoun, personal, nominative, not 3rd person singular\n",
      "    willya\n",
      "MD+TO: modal auxillary + infinitival to\n",
      "    oughta\n",
      "NN: noun, singular, common\n",
      "    failure burden court fire appointment awarding compensation Mayor\n",
      "    interim committee fact effect airport management surveillance jail\n",
      "    doctor intern extern night weekend duty legislation Tax Office ...\n",
      "NN$: noun, singular, common, genitive\n",
      "    season's world's player's night's chapter's golf's football's\n",
      "    baseball's club's U.'s coach's bride's bridegroom's board's county's\n",
      "    firm's company's superintendent's mob's Navy's ...\n",
      "NN+BEZ: noun, singular, common + verb 'to be', present tense, 3rd person singular\n",
      "    water's camera's sky's kid's Pa's heat's throat's father's money's\n",
      "    undersecretary's granite's level's wife's fat's Knife's fire's name's\n",
      "    hell's leg's sun's roulette's cane's guy's kind's baseball's ...\n",
      "NN+HVD: noun, singular, common + verb 'to have', past tense\n",
      "    Pa'd\n",
      "NN+HVZ: noun, singular, common + verb 'to have', present tense, 3rd person singular\n",
      "    guy's Knife's boat's summer's rain's company's\n",
      "NN+IN: noun, singular, common + preposition\n",
      "    buncha\n",
      "NN+MD: noun, singular, common + modal auxillary\n",
      "    cowhand'd sun'll\n",
      "NN+NN: noun, singular, common, hyphenated pair\n",
      "    stomach-belly\n",
      "NNS: noun, plural, common\n",
      "    irregularities presentments thanks reports voters laws legislators\n",
      "    years areas adjustments chambers $100 bonds courts sales details raises\n",
      "    sessions members congressmen votes polls calls ...\n",
      "NNS$: noun, plural, common, genitive\n",
      "    taxpayers' children's members' States' women's cutters' motorists'\n",
      "    steelmakers' hours' Nations' lawyers' prisoners' architects' tourists'\n",
      "    Employers' secretaries' Rogues' ...\n",
      "NNS+MD: noun, plural, common + modal auxillary\n",
      "    duds'd oystchers'll\n",
      "NP: noun, singular, proper\n",
      "    Fulton Atlanta September-October Durwood Pye Ivan Allen Jr. Jan.\n",
      "    Alpharetta Grady William B. Hartsfield Pearl Williams Aug. Berry J. M.\n",
      "    Cheshire Griffin Opelika Ala. E. Pelham Snodgrass ...\n",
      "NP$: noun, singular, proper, genitive\n",
      "    Green's Landis' Smith's Carreon's Allison's Boston's Spahn's Willie's\n",
      "    Mickey's Milwaukee's Mays' Howsam's Mantle's Shaw's Wagner's Rickey's\n",
      "    Shea's Palmer's Arnold's Broglio's ...\n",
      "NP+BEZ: noun, singular, proper + verb 'to be', present tense, 3rd person singular\n",
      "    W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's Seaton's\n",
      "    Buckhorn's Breed's Penny's Rob's Kitty's Blackwell's Myra's Wally's\n",
      "    Lucille's Springfield's Arlene's\n",
      "NP+HVZ: noun, singular, proper + verb 'to have', present tense, 3rd person singular\n",
      "    Bill's Guardino's Celie's Skolman's Crosson's Tim's Wally's\n",
      "NP+MD: noun, singular, proper + modal auxillary\n",
      "    Gyp'll John'll\n",
      "NPS: noun, plural, proper\n",
      "    Chases Aderholds Chapelles Armisteads Lockies Carbones French Marskmen\n",
      "    Toppers Franciscans Romans Cadillacs Masons Blacks Catholics British\n",
      "    Dixiecrats Mississippians Congresses ...\n",
      "NPS$: noun, plural, proper, genitive\n",
      "    Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees' Stevenses'\n",
      "    Geraghtys' Burkes' Wackers' Achaeans' Dresbachs' Russians' Democrats'\n",
      "    Gershwins' Adventists' Negroes' Catholics' ...\n",
      "NR: noun, singular, adverbial\n",
      "    Friday home Wednesday Tuesday Monday Sunday Thursday yesterday tomorrow\n",
      "    tonight West East Saturday west left east downtown north northeast\n",
      "    southeast northwest North South right ...\n",
      "NR$: noun, singular, adverbial, genitive\n",
      "    Saturday's Monday's yesterday's tonight's tomorrow's Sunday's\n",
      "    Wednesday's Friday's today's Tuesday's West's Today's South's\n",
      "NR+MD: noun, singular, adverbial + modal auxillary\n",
      "    today'll\n",
      "NRS: noun, plural, adverbial\n",
      "    Sundays Mondays Saturdays Wednesdays Souths Fridays\n",
      "OD: numeral, ordinal\n",
      "    first 13th third nineteenth 2d 61st second sixth eighth ninth twenty-\n",
      "    first eleventh 50th eighteenth- Thirty-ninth 72nd 1/20th twentieth\n",
      "    mid-19th thousandth 350th sixteenth 701st ...\n",
      "PN: pronoun, nominal\n",
      "    none something everything one anyone nothing nobody everybody everyone\n",
      "    anybody anything someone no-one nothin\n",
      "PN$: pronoun, nominal, genitive\n",
      "    one's someone's anybody's nobody's everybody's anyone's everyone's\n",
      "PN+BEZ: pronoun, nominal + verb 'to be', present tense, 3rd person singular\n",
      "    nothing's everything's somebody's nobody's someone's\n",
      "PN+HVD: pronoun, nominal + verb 'to have', past tense\n",
      "    nobody'd\n",
      "PN+HVZ: pronoun, nominal + verb 'to have', present tense, 3rd person singular\n",
      "    nobody's somebody's one's\n",
      "PN+MD: pronoun, nominal + modal auxillary\n",
      "    someone'll somebody'll anybody'd\n",
      "PP$: determiner, possessive\n",
      "    our its his their my your her out thy mine thine\n",
      "PP$$: pronoun, possessive\n",
      "    ours mine his hers theirs yours\n",
      "PPL: pronoun, singular, reflexive\n",
      "    itself himself myself yourself herself oneself ownself\n",
      "PPLS: pronoun, plural, reflexive\n",
      "    themselves ourselves yourselves\n",
      "PPO: pronoun, personal, accusative\n",
      "    them it him me us you 'em her thee we'uns\n",
      "PPS: pronoun, personal, nominative, 3rd person singular\n",
      "    it he she thee\n",
      "PPS+BEZ: pronoun, personal, nominative, 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+HVD: pronoun, personal, nominative, 3rd person singular + verb 'to have', past tense\n",
      "    she'd he'd it'd\n",
      "PPS+HVZ: pronoun, personal, nominative, 3rd person singular + verb 'to have', present tense, 3rd person singular\n",
      "    it's he's she's\n",
      "PPS+MD: pronoun, personal, nominative, 3rd person singular + modal auxillary\n",
      "    he'll she'll it'll he'd it'd she'd\n",
      "PPSS: pronoun, personal, nominative, not 3rd person singular\n",
      "    they we I you ye thou you'uns\n",
      "PPSS+BEM: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 1st person singular\n",
      "    I'm Ahm\n",
      "PPSS+BER: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    we're you're they're\n",
      "PPSS+BEZ: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular\n",
      "    you's\n",
      "PPSS+BEZ*: pronoun, personal, nominative, not 3rd person singular + verb 'to be', present tense, 3rd person singular, negated\n",
      "    'tain't\n",
      "PPSS+HV: pronoun, personal, nominative, not 3rd person singular + verb 'to have', uninflected present tense\n",
      "    I've we've they've you've\n",
      "PPSS+HVD: pronoun, personal, nominative, not 3rd person singular + verb 'to have', past tense\n",
      "    I'd you'd we'd they'd\n",
      "PPSS+MD: pronoun, personal, nominative, not 3rd person singular + modal auxillary\n",
      "    you'll we'll I'll we'd I'd they'll they'd you'd\n",
      "PPSS+VB: pronoun, personal, nominative, not 3rd person singular + verb 'to verb', uninflected present tense\n",
      "    y'know\n",
      "QL: qualifier, pre\n",
      "    well less very most so real as highly fundamentally even how much\n",
      "    remarkably somewhat more completely too thus ill deeply little overly\n",
      "    halfway almost impossibly far severly such ...\n",
      "QLP: qualifier, post\n",
      "    indeed enough still 'nuff\n",
      "RB: adverb\n",
      "    only often generally also nevertheless upon together back newly no\n",
      "    likely meanwhile near then heavily there apparently yet outright fully\n",
      "    aside consistently specifically formally ever just ...\n",
      "RB$: adverb, genitive\n",
      "    else's\n",
      "RB+BEZ: adverb + verb 'to be', present tense, 3rd person singular\n",
      "    here's there's\n",
      "RB+CS: adverb + conjunction, coordinating\n",
      "    well's soon's\n",
      "RBR: adverb, comparative\n",
      "    further earlier better later higher tougher more harder longer sooner\n",
      "    less faster easier louder farther oftener nearer cheaper slower tighter\n",
      "    lower worse heavier quicker ...\n",
      "RBR+CS: adverb, comparative + conjunction, coordinating\n",
      "    more'n\n",
      "RBT: adverb, superlative\n",
      "    most best highest uppermost nearest brightest hardest fastest deepest\n",
      "    farthest loudest ...\n",
      "RN: adverb, nominal\n",
      "    here afar then\n",
      "RP: adverb, particle\n",
      "    up out off down over on in about through across after\n",
      "RP+IN: adverb, particle + preposition\n",
      "    out'n outta\n",
      "TO: infinitival to\n",
      "    to t'\n",
      "TO+VB: infinitival to + verb, infinitive\n",
      "    t'jawn t'lah\n",
      "UH: interjection\n",
      "    Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha crunch say\n",
      "    oh why see well hello lo alas tarantara rum-tum-tum gosh hell keerist\n",
      "    Jesus Keeeerist boy c'mon 'mon goddamn bah hoo-pig damn ...\n",
      "VB: verb, base: uninflected present, imperative or infinitive\n",
      "    investigate find act follow inure achieve reduce take remedy re-set\n",
      "    distribute realize disable feel receive continue place protect\n",
      "    eliminate elaborate work permit run enter force ...\n",
      "VB+AT: verb, base: uninflected present or infinitive + article\n",
      "    wanna\n",
      "VB+IN: verb, base: uninflected present, imperative or infinitive + preposition\n",
      "    lookit\n",
      "VB+JJ: verb, base: uninflected present, imperative or infinitive + adjective\n",
      "    die-dead\n",
      "VB+PPO: verb, uninflected present tense + pronoun, personal, accusative\n",
      "    let's lemme gimme\n",
      "VB+RP: verb, imperative + adverbial particle\n",
      "    g'ahn c'mon\n",
      "VB+TO: verb, base: uninflected present, imperative or infinitive + infinitival to\n",
      "    wanta wanna\n",
      "VB+VB: verb, base: uninflected present, imperative or infinitive; hypenated pair\n",
      "    say-speak\n",
      "VBD: verb, past tense\n",
      "    said produced took recommended commented urged found added praised\n",
      "    charged listed became announced brought attended wanted voted defeated\n",
      "    received got stood shot scheduled feared promised made ...\n",
      "VBG: verb, present participle or gerund\n",
      "    modernizing improving purchasing Purchasing lacking enabling pricing\n",
      "    keeping getting picking entering voting warning making strengthening\n",
      "    setting neighboring attending participating moving ...\n",
      "VBG+TO: verb, present participle + infinitival to\n",
      "    gonna\n",
      "VBN: verb, past participle\n",
      "    conducted charged won received studied revised operated accepted\n",
      "    combined experienced recommended effected granted seen protected\n",
      "    adopted retarded notarized selected composed gotten printed ...\n",
      "VBN+TO: verb, past participle + infinitival to\n",
      "    gotta\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    deserves believes receives takes goes expires says opposes starts\n",
      "    permits expects thinks faces votes teaches holds calls fears spends\n",
      "    collects backs eliminates sets flies gives seeks reads ...\n",
      "WDT: WH-determiner\n",
      "    which what whatever whichever whichever-the-hell\n",
      "WDT+BER: WH-determiner + verb 'to be', present tense, 2nd person singular or all persons plural\n",
      "    what're\n",
      "WDT+BER+PP: WH-determiner + verb 'to be', present, 2nd person singular or all persons plural + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+BEZ: WH-determiner + verb 'to be', present tense, 3rd person singular\n",
      "    what's\n",
      "WDT+DO+PPS: WH-determiner + verb 'to do', uninflected present tense + pronoun, personal, nominative, not 3rd person singular\n",
      "    whaddya\n",
      "WDT+DOD: WH-determiner + verb 'to do', past tense\n",
      "    what'd\n",
      "WDT+HVZ: WH-determiner + verb 'to have', present tense, 3rd person singular\n",
      "    what's\n",
      "WP$: WH-pronoun, genitive\n",
      "    whose whosever\n",
      "WPO: WH-pronoun, accusative\n",
      "    whom that who\n",
      "WPS: WH-pronoun, nominative\n",
      "    that who whoever whosoever what whatsoever\n",
      "WPS+BEZ: WH-pronoun, nominative + verb 'to be', present, 3rd person singular\n",
      "    that's who's\n",
      "WPS+HVD: WH-pronoun, nominative + verb 'to have', past tense\n",
      "    who'd\n",
      "WPS+HVZ: WH-pronoun, nominative + verb 'to have', present tense, 3rd person singular\n",
      "    who's that's\n",
      "WPS+MD: WH-pronoun, nominative + modal auxillary\n",
      "    who'll that'd who'd that'll\n",
      "WQL: WH-qualifier\n",
      "    however how\n",
      "WRB: WH-adverb\n",
      "    however when where why whereby wherever how whenever whereon wherein\n",
      "    wherewith wheare wherefore whereof howsabout\n",
      "WRB+BER: WH-adverb + verb 'to be', present, 2nd person singular or all persons plural\n",
      "    where're\n",
      "WRB+BEZ: WH-adverb + verb 'to be', present, 3rd person singular\n",
      "    how's where's\n",
      "WRB+DO: WH-adverb + verb 'to do', present, not 3rd person singular\n",
      "    howda\n",
      "WRB+DOD: WH-adverb + verb 'to do', past tense\n",
      "    where'd how'd\n",
      "WRB+DOD*: WH-adverb + verb 'to do', past tense, negated\n",
      "    whyn't\n",
      "WRB+DOZ: WH-adverb + verb 'to do', present tense, 3rd person singular\n",
      "    how's\n",
      "WRB+IN: WH-adverb + preposition\n",
      "    why'n\n",
      "WRB+MD: WH-adverb + modal auxillary\n",
      "    where'd\n"
     ]
    }
   ],
   "source": [
    "# Brown tagset\n",
    "nltk.help.brown_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# Penn Treebank tagset\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the Brown Corpus as an example, we can explore the corpus further by counting how many times a tag appears in the corpus, and create a distribution of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 152470, 'IN': 120557, 'AT': 97959, 'JJ': 64028, '.': 60638, ',': 58156, 'NNS': 55110, 'CC': 37718, 'RB': 36464, 'NP': 34476, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_fd = nltk.FreqDist(tag for (word, tag) in nltk.corpus.brown.tagged_words())\n",
    "tag_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the most and least frequent POS tag by sorting the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('NN', 152470)\n",
      "('FW-UH-TL', 1)\n"
     ]
    }
   ],
   "source": [
    "tag_list = sorted(tag_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(tag_list[0])\n",
    "print(tag_list[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the curve of the POS tag frequency distribution. For the simplicity of visualization, let's first map each tag name to an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 0,\n",
       " 'IN': 1,\n",
       " 'AT': 2,\n",
       " 'JJ': 3,\n",
       " '.': 4,\n",
       " ',': 5,\n",
       " 'NNS': 6,\n",
       " 'CC': 7,\n",
       " 'RB': 8,\n",
       " 'NP': 9,\n",
       " 'VB': 10,\n",
       " 'VBN': 11,\n",
       " 'VBD': 12,\n",
       " 'CS': 13,\n",
       " 'PPS': 14,\n",
       " 'VBG': 15,\n",
       " 'PP$': 16,\n",
       " 'TO': 17,\n",
       " 'PPSS': 18,\n",
       " 'CD': 19,\n",
       " 'NN-TL': 20,\n",
       " 'MD': 21,\n",
       " 'PPO': 22,\n",
       " 'BEZ': 23,\n",
       " 'BEDZ': 24,\n",
       " 'AP': 25,\n",
       " 'DT': 26,\n",
       " '``': 27,\n",
       " \"''\": 28,\n",
       " 'QL': 29,\n",
       " 'VBZ': 30,\n",
       " 'BE': 31,\n",
       " 'RP': 32,\n",
       " 'WDT': 33,\n",
       " 'HVD': 34,\n",
       " '*': 35,\n",
       " 'WRB': 36,\n",
       " 'BER': 37,\n",
       " 'JJ-TL': 38,\n",
       " 'NP-TL': 39,\n",
       " 'HV': 40,\n",
       " 'WPS': 41,\n",
       " '--': 42,\n",
       " 'BED': 43,\n",
       " 'ABN': 44,\n",
       " 'DTI': 45,\n",
       " 'PN': 46,\n",
       " 'NP$': 47,\n",
       " 'BEN': 48,\n",
       " 'DTS': 49,\n",
       " 'HVZ': 50,\n",
       " ')': 51,\n",
       " '(': 52,\n",
       " 'NNS-TL': 53,\n",
       " 'EX': 54,\n",
       " 'JJR': 55,\n",
       " 'OD': 56,\n",
       " 'NR': 57,\n",
       " ':': 58,\n",
       " 'NN$': 59,\n",
       " 'IN-TL': 60,\n",
       " 'NN-HL': 61,\n",
       " 'DO': 62,\n",
       " 'NPS': 63,\n",
       " 'PPL': 64,\n",
       " 'RBR': 65,\n",
       " 'DOD': 66,\n",
       " 'JJT': 67,\n",
       " 'CD-TL': 68,\n",
       " 'MD*': 69,\n",
       " 'AT-TL': 70,\n",
       " 'ABX': 71,\n",
       " 'BEG': 72,\n",
       " 'NNS-HL': 73,\n",
       " 'UH': 74,\n",
       " '.-HL': 75,\n",
       " 'VBN-TL': 76,\n",
       " 'NP-HL': 77,\n",
       " 'IN-HL': 78,\n",
       " 'DO*': 79,\n",
       " 'PPSS+MD': 80,\n",
       " 'DOZ': 81,\n",
       " 'CD-HL': 82,\n",
       " 'PPS+BEZ': 83,\n",
       " 'DOD*': 84,\n",
       " 'JJ-HL': 85,\n",
       " 'NN$-TL': 86,\n",
       " 'JJS': 87,\n",
       " 'ABL': 88,\n",
       " 'PPLS': 89,\n",
       " 'AT-HL': 90,\n",
       " \"'\": 91,\n",
       " 'NR-TL': 92,\n",
       " 'CC-TL': 93,\n",
       " 'FW-NN': 94,\n",
       " 'HVG': 95,\n",
       " 'WPO': 96,\n",
       " 'PPSS+BER': 97,\n",
       " 'PPSS+BEM': 98,\n",
       " 'QLP': 99,\n",
       " 'NNS$': 100,\n",
       " 'WP$': 101,\n",
       " 'PPSS+HV': 102,\n",
       " 'HVN': 103,\n",
       " 'BEM': 104,\n",
       " 'OD-TL': 105,\n",
       " ')-HL': 106,\n",
       " 'DT+BEZ': 107,\n",
       " 'WQL': 108,\n",
       " ',-HL': 109,\n",
       " 'FW-NN-TL': 110,\n",
       " 'PP$$': 111,\n",
       " '(-HL': 112,\n",
       " 'NIL': 113,\n",
       " 'BEDZ*': 114,\n",
       " 'VBG-HL': 115,\n",
       " 'PPS+MD': 116,\n",
       " 'NP$-TL': 117,\n",
       " ':-HL': 118,\n",
       " 'VBN-HL': 119,\n",
       " 'VBG-TL': 120,\n",
       " 'NN-TL-HL': 121,\n",
       " 'VB-HL': 122,\n",
       " 'CC-HL': 123,\n",
       " 'NN-NC': 124,\n",
       " 'BEZ*': 125,\n",
       " 'EX+BEZ': 126,\n",
       " 'DTX': 127,\n",
       " 'RBT': 128,\n",
       " 'HVD*': 129,\n",
       " 'VB-TL': 130,\n",
       " 'DOZ*': 131,\n",
       " 'PN$': 132,\n",
       " 'FW-IN': 133,\n",
       " 'PPSS+HVD': 134,\n",
       " 'FW-NNS': 135,\n",
       " 'PPS+HVD': 136,\n",
       " 'NNS$-TL': 137,\n",
       " 'FW-JJ-TL': 138,\n",
       " 'VBZ-HL': 139,\n",
       " 'VB+PPO': 140,\n",
       " 'NPS-TL': 141,\n",
       " 'NR$': 142,\n",
       " 'TO-HL': 143,\n",
       " 'FW-JJ': 144,\n",
       " 'RB-HL': 145,\n",
       " 'BER*': 146,\n",
       " 'WDT+BEZ': 147,\n",
       " 'FW-AT-TL': 148,\n",
       " 'PPS+HVZ': 149,\n",
       " 'HV*': 150,\n",
       " 'JJ-NC': 151,\n",
       " 'IN-NC': 152,\n",
       " 'VB-NC': 153,\n",
       " 'AP-HL': 154,\n",
       " 'RB-TL': 155,\n",
       " 'FW-IN-TL': 156,\n",
       " 'NPS$': 157,\n",
       " 'WRB-HL': 158,\n",
       " 'FW-NNS-TL': 159,\n",
       " 'PP$-TL': 160,\n",
       " 'AT-NC': 161,\n",
       " 'NN+BEZ': 162,\n",
       " 'FW-RB': 163,\n",
       " 'PPSS-NC': 164,\n",
       " 'BEZ-HL': 165,\n",
       " 'WDT-HL': 166,\n",
       " 'MD-HL': 167,\n",
       " 'FW-CC': 168,\n",
       " 'FW-VB': 169,\n",
       " 'JJ-TL-HL': 170,\n",
       " 'RB-NC': 171,\n",
       " '---HL': 172,\n",
       " 'NNS-NC': 173,\n",
       " 'CS-HL': 174,\n",
       " 'NP+BEZ': 175,\n",
       " 'PPSS-HL': 176,\n",
       " 'FW-AT': 177,\n",
       " 'BED*': 178,\n",
       " 'HVZ*': 179,\n",
       " ':-TL': 180,\n",
       " 'WPS+BEZ': 181,\n",
       " 'JJS-TL': 182,\n",
       " 'NN$-HL': 183,\n",
       " 'PPS-HL': 184,\n",
       " 'AP-TL': 185,\n",
       " 'FW-IN+AT-TL': 186,\n",
       " 'JJR-HL': 187,\n",
       " 'VBZ-TL': 188,\n",
       " 'CD-TL-HL': 189,\n",
       " 'VBG+TO': 190,\n",
       " 'FW-WDT': 191,\n",
       " 'DOZ-HL': 192,\n",
       " 'NRS': 193,\n",
       " '.-NC': 194,\n",
       " 'VBG-NC': 195,\n",
       " 'UH-TL': 196,\n",
       " 'JJR-TL': 197,\n",
       " 'NP-NC': 198,\n",
       " 'RP-HL': 199,\n",
       " 'NNS-TL-HL': 200,\n",
       " 'FW-CC-TL': 201,\n",
       " 'BE-HL': 202,\n",
       " 'PPO-TL': 203,\n",
       " 'FW-AT+NN-TL': 204,\n",
       " 'TO-NC': 205,\n",
       " 'PP$-NC': 206,\n",
       " 'WPS-TL': 207,\n",
       " 'FW-VBN': 208,\n",
       " 'BER-HL': 209,\n",
       " 'RB+BEZ': 210,\n",
       " 'NR$-TL': 211,\n",
       " 'WRB+BEZ': 212,\n",
       " 'HV-NC': 213,\n",
       " 'VBD-NC': 214,\n",
       " 'NR-HL': 215,\n",
       " 'TO-TL': 216,\n",
       " 'PP$-HL': 217,\n",
       " 'AP$': 218,\n",
       " 'RB$': 219,\n",
       " 'RN': 220,\n",
       " 'FW-PPL': 221,\n",
       " 'PPSS-TL': 222,\n",
       " 'DT-TL': 223,\n",
       " 'WRB-TL': 224,\n",
       " 'FW-NN$': 225,\n",
       " 'PPS-NC': 226,\n",
       " 'VBN-NC': 227,\n",
       " 'PPO-NC': 228,\n",
       " 'BEM*': 229,\n",
       " 'VBD-HL': 230,\n",
       " 'NPS-HL': 231,\n",
       " 'OD-HL': 232,\n",
       " 'MD-TL': 233,\n",
       " '*-HL': 234,\n",
       " 'NP$-HL': 235,\n",
       " 'BEZ-TL': 236,\n",
       " 'WPS+MD': 237,\n",
       " 'FW-UH': 238,\n",
       " 'BEDZ-NC': 239,\n",
       " 'NP-TL-HL': 240,\n",
       " 'MD+HV': 241,\n",
       " 'FW-CD': 242,\n",
       " 'ABN-TL': 243,\n",
       " 'FW-NP': 244,\n",
       " 'FW-VBG': 245,\n",
       " 'DT-NC': 246,\n",
       " 'WRB-NC': 247,\n",
       " 'WDT-NC': 248,\n",
       " 'VBZ-NC': 249,\n",
       " 'PN+BEZ': 250,\n",
       " 'VBN-TL-HL': 251,\n",
       " 'DT-HL': 252,\n",
       " 'JJT-HL': 253,\n",
       " 'VBD-TL': 254,\n",
       " 'DTI-HL': 255,\n",
       " 'BER-TL': 256,\n",
       " 'QL-TL': 257,\n",
       " 'FW-*': 258,\n",
       " 'IN-TL-HL': 259,\n",
       " 'PPS-TL': 260,\n",
       " 'FW-NN-NC': 261,\n",
       " 'FW-PPSS': 262,\n",
       " 'WPS+HVD': 263,\n",
       " 'NP+HVZ': 264,\n",
       " 'WRB+DOD': 265,\n",
       " 'DT$': 266,\n",
       " 'FW-IN+NN': 267,\n",
       " 'CD$': 268,\n",
       " 'JJR-NC': 269,\n",
       " 'PPO-HL': 270,\n",
       " 'DO-TL': 271,\n",
       " 'WQL-TL': 272,\n",
       " 'PN-TL': 273,\n",
       " 'NR-TL-HL': 274,\n",
       " 'AT-TL-HL': 275,\n",
       " 'NN+HVZ': 276,\n",
       " 'VBN+TO': 277,\n",
       " 'BER-NC': 278,\n",
       " 'UH-NC': 279,\n",
       " ',-NC': 280,\n",
       " 'CD-NC': 281,\n",
       " 'RP-NC': 282,\n",
       " 'CC-NC': 283,\n",
       " 'CS-NC': 284,\n",
       " 'BEZ-NC': 285,\n",
       " 'ABN-HL': 286,\n",
       " 'DO-HL': 287,\n",
       " 'NNS$-HL': 288,\n",
       " 'WPO-TL': 289,\n",
       " 'FW-VBZ': 290,\n",
       " 'FW-PPO': 291,\n",
       " 'QL-HL': 292,\n",
       " 'FW-OD-TL': 293,\n",
       " 'HVZ-TL': 294,\n",
       " 'RP-TL': 295,\n",
       " ',-TL': 296,\n",
       " 'FW-NN$-TL': 297,\n",
       " 'FW-BEZ': 298,\n",
       " 'FW-NP-TL': 299,\n",
       " 'JJT-TL': 300,\n",
       " 'EX+MD': 301,\n",
       " 'FW-IN+AT': 302,\n",
       " 'NR-NC': 303,\n",
       " 'VB+TO': 304,\n",
       " 'RP+IN': 305,\n",
       " 'PN+HVZ': 306,\n",
       " 'FW-VB-NC': 307,\n",
       " 'NPS$-TL': 308,\n",
       " 'WRB+BEZ-TL': 309,\n",
       " 'FW-IN+AT-T': 310,\n",
       " 'FW-RB-TL': 311,\n",
       " 'HV-HL': 312,\n",
       " 'VB+IN': 313,\n",
       " 'DO*-HL': 314,\n",
       " 'FW-PP$': 315,\n",
       " 'FW-BER': 316,\n",
       " 'FW-NR-TL': 317,\n",
       " 'FW-CS': 318,\n",
       " 'HV-TL': 319,\n",
       " 'FW-PPO+IN': 320,\n",
       " 'VBN-TL-NC': 321,\n",
       " 'NNS-TL-NC': 322,\n",
       " 'NN-TL-NC': 323,\n",
       " 'BED-NC': 324,\n",
       " 'PPS+BEZ-NC': 325,\n",
       " 'NP+BEZ-NC': 326,\n",
       " 'WPS-NC': 327,\n",
       " 'EX+HVD': 328,\n",
       " 'PN+MD': 329,\n",
       " 'DT+MD': 330,\n",
       " 'HV+TO': 331,\n",
       " 'RB+CS': 332,\n",
       " 'FW-DT': 333,\n",
       " 'PN-HL': 334,\n",
       " 'FW-IN+NN-TL': 335,\n",
       " 'FW-AT+NP-TL': 336,\n",
       " 'BEN-TL': 337,\n",
       " 'CS-TL': 338,\n",
       " 'CC-TL-HL': 339,\n",
       " 'FW-JJ-NC': 340,\n",
       " 'FW-VBD': 341,\n",
       " 'FW-*-TL': 342,\n",
       " 'DTS-HL': 343,\n",
       " 'PN-NC': 344,\n",
       " 'WDT+HVZ': 345,\n",
       " 'FW-IN+NP-TL': 346,\n",
       " 'NN+MD': 347,\n",
       " 'FW-NNS-NC': 348,\n",
       " 'VB+RP': 349,\n",
       " 'FW-PP$-TL': 350,\n",
       " 'DTI-TL': 351,\n",
       " '.-TL': 352,\n",
       " 'FW-NPS': 353,\n",
       " 'FW-CD-TL': 354,\n",
       " 'FW-PPL+VBZ': 355,\n",
       " 'DOZ-TL': 356,\n",
       " 'WDT+BEZ-NC': 357,\n",
       " 'HVZ-NC': 358,\n",
       " 'QL-NC': 359,\n",
       " 'WPS+BEZ-NC': 360,\n",
       " 'PPSS+MD-NC': 361,\n",
       " 'AP-NC': 362,\n",
       " 'DO-NC': 363,\n",
       " 'MD-NC': 364,\n",
       " 'NNS$-NC': 365,\n",
       " 'PPL-NC': 366,\n",
       " 'BEM-NC': 367,\n",
       " 'NPS-NC': 368,\n",
       " 'JJ+JJ-NC': 369,\n",
       " 'WPS-HL': 370,\n",
       " 'FW-DT+BEZ': 371,\n",
       " 'WPS+HVZ': 372,\n",
       " 'MD+TO': 373,\n",
       " 'NN+BEZ-TL': 374,\n",
       " 'EX+HVZ': 375,\n",
       " 'PPSS+VB': 376,\n",
       " 'NNS+MD': 377,\n",
       " 'NP+MD': 378,\n",
       " 'TO+VB': 379,\n",
       " 'VB+AT': 380,\n",
       " 'DTS+BEZ': 381,\n",
       " 'MD*-HL': 382,\n",
       " 'BEDZ-HL': 383,\n",
       " 'PPS+BEZ-HL': 384,\n",
       " 'HVD-HL': 385,\n",
       " 'FW-AT-HL': 386,\n",
       " 'FW-PP$-NC': 387,\n",
       " 'NPS$-HL': 388,\n",
       " 'UH-HL': 389,\n",
       " 'WDT+BEZ-HL': 390,\n",
       " 'PPL-HL': 391,\n",
       " 'FW-VBD-TL': 392,\n",
       " 'PPSS+BER-TL': 393,\n",
       " 'BE-TL': 394,\n",
       " 'PPSS+HV-TL': 395,\n",
       " 'DOD*-TL': 396,\n",
       " 'WDT+BEZ-TL': 397,\n",
       " 'FW-JJR': 398,\n",
       " 'WDT+BER+PP': 399,\n",
       " 'FW-UH-NC': 400,\n",
       " 'RB+BEZ-HL': 401,\n",
       " 'JJS-HL': 402,\n",
       " 'PPL-TL': 403,\n",
       " 'JJR+CS': 404,\n",
       " 'NRS-TL': 405,\n",
       " 'FW-HV': 406,\n",
       " 'DOZ*-TL': 407,\n",
       " 'FW-NPS-TL': 408,\n",
       " '*-TL': 409,\n",
       " 'FW-PN': 410,\n",
       " 'FW-BE': 411,\n",
       " 'FW-PPS': 412,\n",
       " 'FW-NR': 413,\n",
       " 'FW-TO+VB': 414,\n",
       " 'JJ$-TL': 415,\n",
       " 'FW-VB-TL': 416,\n",
       " 'FW-RB+CC': 417,\n",
       " 'FW-WPO': 418,\n",
       " 'FW-NN-TL-NC': 419,\n",
       " 'FW-WPS': 420,\n",
       " 'FW-DTS': 421,\n",
       " 'NNS$-TL-HL': 422,\n",
       " 'FW-VBG-TL': 423,\n",
       " 'EX-HL': 424,\n",
       " 'PPSS+BER-N': 425,\n",
       " 'NP+HVZ-NC': 426,\n",
       " 'DT+BEZ-NC': 427,\n",
       " 'RB+BEZ-NC': 428,\n",
       " '*-NC': 429,\n",
       " 'EX-NC': 430,\n",
       " 'BER*-NC': 431,\n",
       " 'PPSS+BER-NC': 432,\n",
       " 'RBR-NC': 433,\n",
       " 'OD-NC': 434,\n",
       " 'ABN-NC': 435,\n",
       " 'JJT-NC': 436,\n",
       " 'DOD-NC': 437,\n",
       " 'WPO-NC': 438,\n",
       " 'NN+NN-NC': 439,\n",
       " 'AP+AP-NC': 440,\n",
       " 'VB+JJ-NC': 441,\n",
       " 'VB+VB-NC': 442,\n",
       " 'FW-QL': 443,\n",
       " 'JJ-TL-NC': 444,\n",
       " 'FW-JJT': 445,\n",
       " 'WPS+BEZ-TL': 446,\n",
       " 'HVG-HL': 447,\n",
       " 'MD+PPSS': 448,\n",
       " 'NR+MD': 449,\n",
       " 'NN+IN': 450,\n",
       " 'NN+HVD-TL': 451,\n",
       " 'WDT+DOD': 452,\n",
       " 'WRB+DO': 453,\n",
       " 'WRB+IN': 454,\n",
       " 'WRB+MD': 455,\n",
       " 'NN+HVZ-TL': 456,\n",
       " 'WRB+BER': 457,\n",
       " 'PPSS+BEZ': 458,\n",
       " 'PPSS+BEZ*': 459,\n",
       " 'RBR+CS': 460,\n",
       " 'IN+PPO': 461,\n",
       " 'IN+IN': 462,\n",
       " 'DO+PPSS': 463,\n",
       " 'WRB+DOZ': 464,\n",
       " 'WDT+DO+PPS': 465,\n",
       " 'WRB+DOD*': 466,\n",
       " 'WDT+BER': 467,\n",
       " 'FW-OD-NC': 468,\n",
       " 'FW-PPSS+HV': 469,\n",
       " 'PN+HVD': 470,\n",
       " 'FW-UH-TL': 471}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_fd_index = dict(zip([tag for tag, freq in tag_list], range(len(tag_list))))\n",
    "tag_fd_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create a new `FreqDict` of tag_index and its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fad84659f60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_index_fd = nltk.FreqDist({tag_fd_index[k]: v for k, v in tag_fd.items()})\n",
    "tag_index_fd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s look for words that are highly ambiguous as to their POS tag. Understanding why such words are tagged as they are in each context can help us clarify the distinctions between the tags.\n",
    "\n",
    "Note that the items being counted in the frequency distribution are word-tag pairs, we can then treat the word as a condition and the tag as an event, and initialize a conditional frequency distribution with a list of condition-event\n",
    "pairs. This lets us see a frequency-ordered list of tags given a word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'AT': 69013, 'AT-TL': 675, 'AT-HL': 253, 'AT-NC': 26, 'NIL': 3, 'AT-TL-HL': 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = nltk.ConditionalFreqDist((word.lower(), tag) for (word, tag) in nltk.corpus.brown.tagged_words())\n",
    "print(data.conditions()[0])\n",
    "data['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print words along with their POS tags when the word has more than 4 tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the AT AT-TL AT-HL AT-NC NIL AT-TL-HL\n",
      "an AT AT-HL AT-TL NIL AT-NC CC\n",
      "of IN IN-TL IN-HL IN-TL-HL IN-NC NIL\n",
      "no AT RB AT-HL AT-TL QL RB-NC\n",
      "that CS WPS DT QL WPO CS-HL DT-TL WPS-TL DT-HL DT-NC NIL WPS-NC WPO-NC CS-NC WPS-HL\n",
      "any DTI QL DTI-HL DTI-TL RB\n",
      "place NN VB NN-TL NP NN-HL FW-NN-TL NN-NC\n",
      "in IN RP IN-HL IN-TL IN-NC RP-HL NN NIL RP-NC FW-IN\n",
      ", , ,-HL FW-RB-TL ,-TL ,-NC\n",
      "charge NN VB NN-HL NN-TL NN-NC\n",
      "and CC CC-TL CC-HL CC-TL-HL NIL CC-NC\n",
      "for IN IN-TL IN-HL CS RB NN IN-NC\n",
      "by IN IN-HL IN-TL RB NIL IN-NC\n",
      "to TO IN IN-HL TO-HL IN-TL TO-TL NPS NIL QL TO-NC IN-NC\n",
      "only RB AP JJ QL AP-HL\n",
      "a AT AT-HL NN AT-TL FW-IN NP-TL NN-TL AT-TL-HL NIL NP NP-HL AT-NC FW-IN-TL\n",
      "number NN NN-HL VB NN-NC NN-TL\n",
      "this DT DT-HL DT-TL QL DT-NC\n",
      "it PPS PPO PPO-TL PPO-HL PPS-HL PPS-TL PPS-NC PPO-NC UH\n",
      "many AP AP-HL ABN QL NIL AP-NC\n",
      "or CC CC-TL NIL CC-HL CC-NC\n",
      "on IN IN-TL RP IN-HL RP-TL IN-NC NIL\n",
      "purchasing VBG VBG-TL NN NN-TL VBG-HL\n",
      "well QL RB JJ NN UH RB-TL JJ-TL QL-HL VB\n",
      "best JJT RBT JJT-HL VB NP-TL QL NP\n",
      "be BE BE-HL BE-TL NIL UH\n",
      "cost NN NN-HL VB VBD VBN NN-TL\n",
      "is BEZ BEZ-HL BEZ-TL NIL BEZ-NC\n",
      "as CS IN QL CS-HL CS-TL RB NIL\n",
      "next AP IN QL RB AP-HL AP-TL\n",
      "so CS QL RB QL-TL UH\n",
      "may MD NP NP-HL NP-TL MD-HL NIL MD-TL\n",
      "state NN-TL NN VB NN-HL NN-TL-HL NN-TL-NC\n",
      "one CD PN CD-TL CD-HL CD-NC PN-NC DTX\n",
      "major JJ NN-TL NP NN VB JJ-HL JJ-TL\n",
      "general JJ NN JJ-TL NN-TL JJ-HL\n",
      "program NN VB NN-TL NN-HL NIL\n",
      "but CC IN RB CC-HL CC-NC\n",
      "fit JJ VB VBN NN VB-HL NN-HL VBD\n",
      "all ABN QL ABN-HL ABN-TL RB ABN-NC\n",
      "with IN IN-HL IN-TL RB IN-NC\n",
      "future NN JJ NN-HL JJ-TL JJ-HL NN-TL\n",
      "some DTI DTI-HL RB DTI-TL QL\n",
      "do DO DO-HL NN-TL DO-TL NP DO-NC\n",
      "will MD MD-HL NN NP NN-TL VB\n",
      "under IN RB JJ-TL IN-TL IN-HL JJ\n",
      "fire NN NN-TL VB VB-TL NN-HL\n",
      "bar NN-TL VB-HL NN NN-HL VB\n",
      "1 CD CD-HL CD-TL CD-TL-HL NN\n",
      "there EX RB RB-HL EX-HL EX-NC\n",
      ": : :-HL NP :-TL . .-HL , IN NIL\n",
      "fair JJ NN JJ-HL NN-TL JJ-TL RB\n",
      "plan NN VB NN-HL NN-TL VB-HL\n",
      "his PP$ PP$$ NIL PP$-NC PP$-TL\n",
      "man NN NN-TL NN-HL VB NN-NC UH\n",
      "more AP AP-HL QL RBR NP NIL RBR-NC AP-TL\n",
      "than IN CS CS-HL CS-NC IN-NC\n",
      "home NR NN NN-HL NN-TL NR-HL NP NR-TL NR-NC NN-NC VB\n",
      "back RB VB NN RB-HL RB-TL JJ-TL JJ NN-HL\n",
      "present JJ RB NN VB NN-TL AP AP-HL\n",
      "he PPS PPS-TL PPS-NC NIL PPS-HL\n",
      "after CS IN CS-TL RB IN-TL RP\n",
      "run VB NN VBN VBD NN-TL\n",
      "top JJS NN VB JJS-TL JJS-HL\n",
      "w. NP JJ-HL NP-HL JJ-TL NP-TL\n",
      "up RP RP-HL IN RP-TL RP-NC IN-NC\n",
      "john NP NP-HL NP-TL NN-TL NN\n",
      "force VB NN-TL NN FW-NN-TL NN-HL FW-NN\n",
      "out RP IN PP$ RP-HL IN-HL RB-TL RB-NC RP-NC RB\n",
      "before CS IN RB CS-HL IN-TL NIL\n",
      "first OD RB OD-HL RB-HL OD-TL OD-NC\n",
      "must MD MD-HL MD-TL NN NIL\n",
      "head VB NN JJS NN-TL-HL NN-TL NNS\n",
      "study NN VB VB-TL NN-TL VB-HL\n",
      "what WDT WDT-HL WPS-TL WPO-TL WDT-NC WPS\n",
      "made VBN VBD VBN-HL NIL VBN-NC\n",
      "issue NN VB VB-HL NN-HL NN-TL\n",
      "near RB RB-HL JJ IN QL JJ-TL VB\n",
      "go VB NN VB-TL VB-HL VB-NC\n",
      "test NN VB NN-HL NN-TL NN-TL-HL\n",
      "then RB JJ RB-HL NIL CS RB-NC RN\n",
      "let VBN VB NN VBD VB-HL\n",
      "most QL AP RBT QL-TL RB AP-HL\n",
      "about IN RB RP IN-TL IN-HL NIL\n",
      "house NN-TL NN NN-HL NP-TL-HL NP NN-TL-HL VB\n",
      "can MD VB NN MD-HL MD-TL NIL MD-NC\n",
      "set VB VBN VBD VBN-HL NN NN-TL NN-HL VBN-TL\n",
      "last AP AP-TL VB NN AP-HL RB\n",
      "pay NN VB VB-HL NN-HL VB-TL\n",
      "past NN JJ AP IN NN-TL RB\n",
      "like VB CS IN JJ VB-HL IN-TL\n",
      "congress NP NP-HL NP-TL NP-TL-HL NN\n",
      "board NN-TL NN NN-HL VB-NC VB\n",
      "long JJ RB JJ-TL QL NP-HL NP RB-HL VB\n",
      "put VBD VB VBN VB-HL VBD-NC\n",
      "i PPSS NN PPSS-NC NN-TL NP NIL PPSS-HL\n",
      "church NN NN-HL NN-TL NP-HL NP NN-TL-HL\n",
      "quiet JJ NN JJ-TL JJ-HL VB\n",
      "p. NP NN-TL NN NP-HL NP-TL JJ\n",
      "march NP NN NP-HL NN-TL VB\n",
      "post NN VB NP NN-TL FW-IN IN\n",
      "too RB QL RB-HL QL-HL QL-TL RB-NC\n",
      "local JJ NN NN-TL JJ-TL JJ-HL\n",
      "good JJ NN JJ-TL RB JJ-HL NN-HL\n",
      "price NP NN NN-TL VB NP-HL NN-HL\n",
      "down RP IN RP-HL NP-TL RB-TL VB JJ NN\n",
      "questions NNS NNS-TL VBZ-HL VBZ NNS-HL\n",
      "left VBD VBN JJ NR NN NN-TL\n",
      "little AP JJ QL NP JJ-TL JJ-HL AP-HL RB\n",
      "help VB NN NN-HL VB-HL UH\n",
      "means NNS VBZ NNS-TL NN VBZ-TL NN-HL NP\n",
      "over RP IN IN-HL RP-HL IN-TL JJ\n",
      "banks NNS NNS-TL VBZ NP NP-TL\n",
      "report VB NN VB-HL NN-TL NN-HL\n",
      "you PPSS PPO PPO-TL PPSS-TL PPO-HL PPSS-HL PPSS-NC PPO-NC\n",
      "special JJ JJ-HL JJ-TL NN NN-TL\n",
      "here RB RB-HL RB-TL RN RB-NC\n",
      "red NP JJ-TL NN-TL JJ NN JJ-TL-HL\n",
      "still RB QL VB JJ QLP NP NN\n",
      "pro JJ NN IN FW-IN-TL IN-HL\n",
      "people NNS NNS-TL NNS-HL VB NNS-NC\n",
      "better RBR VB JJR JJR-TL QL JJR-HL\n",
      "planning VBG VBG-TL NN VBG-HL NN-TL\n",
      "a. NP NP-HL NNS JJ NN\n",
      "fine NN JJ JJ-TL RB VB\n",
      "right NN JJ RB QL NN-HL NR NN-TL JJ-HL JJ-TL\n",
      "3 CD-TL CD CD-HL OD OD-TL\n",
      "need NN VB MD VB-TL NN-HL\n",
      "west JJ-TL NR-TL NR NR-TL-HL NR-HL NP JJ\n",
      "final JJ NN JJ-HL JJ-TL AP\n",
      "water NN NN-TL VB NN-TL-HL NN-HL NN-NC\n",
      "fort NN-TL NN-TL-HL NN NP VB\n",
      "junior JJ JJ-TL NN NP NN-TL\n",
      "high JJ NN JJ-TL JJ-HL RB\n",
      "english NP JJ NP-TL NPS JJ-TL\n",
      "close NN JJ RB VB QL\n",
      "master NN-TL JJS NN VB JJ NN-HL\n",
      "range NN NN-HL VB NN-NC NN-TL\n",
      "case NN NP NN-HL NP-TL NN-TL VB\n",
      "deal VB VB-HL NN NN-TL VB-NC\n",
      "grant VB NN NP VB-HL NN-TL\n",
      "? . .-HL .-TL .-NC NIL\n",
      "honor NN VB VB-HL NN-HL NP NN-TL\n",
      "true JJ RB JJ-TL NN JJ-NC\n",
      "white JJ-TL NN-TL NP JJ NN NP-TL\n",
      "american JJ JJ-TL NP JJ-HL NP-TL\n",
      "much AP QL RB AP-HL AP-TL QL-HL\n",
      "way NN QL NN-TL NN-HL NP RB\n",
      "press NN NN-TL VB NN-TL-HL NN-HL\n",
      "say VB UH VB-TL NN VB-NC FW-VB\n",
      "even QL RB JJ VB NIL\n",
      "outside IN JJ NN RB NN-HL\n",
      "east JJ-TL NR-TL NR JJ-TL-HL JJ\n",
      "claims VBZ-HL NNS VBZ NNS-TL NNS-HL\n",
      "reading VBG NN NN-HL NP NN-NC VBG-NC\n",
      "full JJ RB QL NN JJ-TL\n",
      "part NN VB NIL NN-TL NN-HL\n",
      "service NN NN-TL VB NN-HL NN-TL-HL FW-NN-TL\n",
      "states NNS NNS-TL NNS-TL-HL VBZ NNS-HL NNS-TL-NC\n",
      "cut VBN VB VBD NN-HL NN VB-HL\n",
      "young JJ JJ-TL NN-TL NP NN JJ-NC\n",
      "field NN NN-TL VB NP NP-TL NN-HL\n",
      "national JJ JJ-TL JJ-TL-HL NN FW-JJ-TL\n",
      "how QL WRB WRB-HL WQL-TL WQL WRB-NC\n",
      "direct JJ VB RB JJ-TL JJ-HL\n",
      "sound JJ VB NN NN-TL JJ-HL QL\n",
      "north JJ-TL NR NR-TL NP NN\n",
      "atlantic NP-TL JJ-TL JJ JJ-HL NP\n",
      "united VBN-TL VBN-TL-HL VBN VBD VBN-TL-NC\n",
      "scale NN VB NN-HL NN-TL NIL\n",
      "inside IN NN IN-TL RB JJ NN-HL\n",
      "forces NNS VBZ NNS-TL FW-NNS-TL NNS-TL-HL NNS-HL\n",
      "come VB VBN VB-TL VB-HL VB-NC\n",
      "ground NN VBN NN-TL VBD VB\n",
      "international JJ JJ-TL NN JJ-TL-HL JJ-HL\n",
      "communist NN-TL NN JJ-TL JJ JJ-HL NP\n",
      "south JJ-TL NR NR-TL NR-TL-HL NR-HL JJ RB\n",
      "free JJ JJ-TL RB JJ-TL-HL VB\n",
      "power NN NN-HL NN-TL VB NP\n",
      "early RB JJ JJ-HL JJ-TL NP\n",
      "far RB QL JJ JJ-TL RB-TL\n",
      "show VB NN NN-HL NN-TL-HL NN-TL\n",
      "southeast JJ-TL NR JJ-TL-HL NR-TL JJ\n",
      "half ABN RB NN QL NN-TL\n",
      "form NN VB NN-TL NN-HL NN-NC\n",
      "miss NP VB NN-TL NP-TL NN\n",
      "times NNS NNS-TL IN NNS-TL-HL VBZ\n",
      "chance NN NP NN-HL JJ VB\n",
      "small JJ NP JJ-HL JJ-TL NN\n",
      "rotary JJ-TL NP NP-TL JJ JJ-HL\n",
      "gross JJ NN NP NP-TL JJ-HL JJ-TL\n",
      "hand NN NP VB NN-TL NN-HL\n",
      "hope VB NN NP NN-HL NN-TL\n",
      "model JJ NN VB NN-TL NN-HL\n",
      "park NN-TL NN NN-TL-HL NP NN-HL VB\n",
      "love VB VB-HL NN NN-TL VB-TL\n",
      "fall NN VB-TL NN-HL VB NN-TL\n",
      "common JJ JJ-TL NN JJ-HL NN-TL\n",
      "dead JJ JJ-TL JJ-HL NN NN-TL QL RB\n",
      "wrong JJ JJ-HL RB NN JJ-NC\n",
      "open JJ RB VB NN-TL NN JJ-HL VB-HL JJ-TL\n",
      "paper NN VB NN-TL NN-HL NN-NC\n",
      "private JJ NN JJ-HL NN-TL JJ-TL\n",
      "cross NN-TL NN VB NP JJ\n",
      "bid NN VBD VB NN-TL VBN\n",
      "looking VBG JJ NN VBG-TL VBG-NC\n",
      "keep VB NN VB-HL VB-TL VB-NC\n",
      "clean JJ VB VB-HL NN-TL RB\n",
      "green JJ-TL JJ NP NN JJ-HL NN-TL\n",
      "land NN VB NN-TL NN-HL NP\n",
      "7 CD CD-TL CD-HL CD-TL-HL NN\n",
      "file NN VB VB-HL VB-TL NN-TL\n",
      "rival JJ NN-HL VB JJ-TL NN\n",
      "key NN JJS NN-HL NP NN-TL VB\n",
      "low JJ RB NN NP JJ-TL JJ-HL\n",
      "s. NP-TL NP JJ-TL NIL NP-HL\n",
      "due JJ-HL RB JJ NN JJ-TL\n",
      "round NN VB JJ NN-TL RB JJ-HL JJ-TL IN\n",
      "mass VB NN JJ NN-TL NN-NC\n",
      "square NN-HL NN-TL NN VB JJ RB\n",
      "trust NN NN-TL VB VB-HL VB-NC\n",
      "iron VB NN JJ-TL NN-TL JJ\n",
      "single JJ AP NN AP-HL VB\n",
      "flat JJ NN JJ-TL RB QL\n",
      "second OD NN RB QL OD-TL OD-HL NN-TL\n",
      "bridges NNS NP VBZ NNS-HL NNS-TL\n",
      "boy NN NN-HL NN-TL NN-NC UH\n",
      "al NP FW-NNS FW-AT-HL NN NP-TL\n",
      "god NP-TL NP NN-HL NN NP-HL UH\n",
      "born VBN VBN-TL VBN-HL JJ VBN-NC\n",
      "light NN JJ JJ-HL VB NN-TL JJ-TL\n",
      "double NN JJ JJ-HL VB RB JJ-TL\n",
      "deep RB JJ NN-TL JJ-TL JJ-HL\n",
      "short JJ RB JJ-HL JJ-TL NP\n",
      "hole NN NP NN-HL VB NN-TL\n",
      "beat VBD VB NN-TL-HL NN NNS JJ-TL VBN JJ\n",
      "hit NN VBN VB VBD NN-HL NN-TL\n",
      "fast JJ RB RB-HL NN QL\n",
      "news NN NN-HL NN-TL NN-NC NP\n",
      "camp NN VB NN-TL NP NN-NC\n",
      "brown NP NP-TL JJ NN JJ-TL\n",
      "art NP NN-TL NN BER NP-TL NN-HL\n",
      "bright JJ JJ-TL-HL JJ-TL NP-TL NP\n",
      "lay VBD JJ NP VB JJ-TL\n",
      "buffalo NP NN NP-TL NNS NN-TL\n",
      "dark JJ NP JJ-TL-HL NN JJ-TL\n",
      "standard JJ-TL JJ NN NN-TL JJ-HL\n",
      "guy NN NN-TL NP NP-TL VB\n",
      "c NP-TL NN NN-TL NP NP-HL\n",
      "spread VBN VB NN NN-HL VBD\n",
      "ain't BEZ* BER* BEM* HV* HVZ*\n",
      "tour NN NN-HL FW-NN FW-NN-TL VB\n",
      "swim NN VB NP-HL NP VB-TL\n",
      "chase NP NN VB NP-TL NN-HL NN-TL\n",
      "black JJ NP NN JJ-TL VB\n",
      "welcome NN VB JJ UH-TL JJ-NC NN-TL\n",
      "rose NP VBD NN NN-TL JJ\n",
      "snow NN NN-HL NP NN-TL VB\n",
      "ring NN NP VB NN-TL NN-TL-HL VB-TL\n",
      "german NP JJ NP-TL NP-HL JJ-TL\n",
      "electric JJ-TL JJ NN-TL NN NN-HL JJ-HL\n",
      "bound VBN VB VBN-HL NN VBD\n",
      "northeast NR JJ-TL JJ NN-TL NR-TL-HL NR-TL\n",
      "rock NN NN-TL VB-TL VB NP\n",
      "northwest JJ-TL NN-TL NR JJ NR-TL\n",
      "data NN NNS NN-HL NNS-TL NNS-HL\n",
      "french JJ NP JJ-HL JJ-TL NPS\n",
      "shore NN-TL VB NN RB NP\n",
      "lower JJR VB JJR-TL JJR-HL RBR NP-TL\n",
      "japanese JJ NPS NP JJ-TL JJ-NC\n",
      "b NN NN-TL NP-TL NN-TL-HL NP NP-HL NN-HL\n",
      "o NN UH NP NP-TL FW-UH-TL\n",
      "feed NN VB NN-HL VB-HL VB-TL\n",
      "prime JJ JJ-TL NN JJ-HL NP\n",
      "du FW-IN+AT-TL NP-TL NP FW-IN+AT NN\n",
      "ballet FW-NN-TL NN-TL NN FW-NN NN-TL-HL\n",
      "die VB FW-AT-TL NN FW-AT FW-NN\n",
      "renaissance NN-TL NN-HL NN NP NP-TL\n",
      "hurt VBN VB NN JJ VBD\n",
      "plain JJ QL JJ-TL NN RB\n",
      "chinese JJ JJ-HL NP NPS JJ-TL JJ-NC NP-NC\n",
      "cool JJ VB RB NP NN\n",
      "purple JJ NN JJ-TL-HL JJ-HL JJ-TL\n",
      "pleasant NN NP-TL JJ JJ-HL NP\n",
      "elder JJR-TL JJR NP NN-TL NN\n",
      "what's WDT+BEZ WDT+BEZ-HL WDT+BEZ-TL WDT+HVZ WDT+BEZ-NC\n",
      "moon NN VB NN-TL NN-HL NP\n",
      "brace VB-HL NN NP NP-TL VB\n",
      "pope NN-TL NP-HL NP NN NP-TL\n",
      "e NN-TL NN NP NP-HL NP-TL\n",
      "abstract JJ JJ-HL VB JJ-TL NN\n",
      "boulder NN-TL NN-HL NN NP-TL NP\n",
      "n NN NP NN-TL NN-HL NP-TL\n",
      "damn VB QL JJ UH NN\n"
     ]
    }
   ],
   "source": [
    "for word in data.conditions():\n",
    "    if len(data[word]) > 4:\n",
    "        tags = data[word].keys()\n",
    "        print (word, ' '.join(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "For Penn Treebank Corpus, repeat the same process, i.e. plotting the distribution of the tagset, and finding the most and least frequent tag.\n",
    "\n",
    "For both corpora, what kinds of words occur in the noun, adjective, adverb category?\n",
    "\n",
    "Explore the POS ditribution of ambiguous words mentioned before, e.g. *content* and *present*. Do they support our observations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tag n-grams\n",
    "So far we have been working with a only single POS tag in text sequence, i.e. the unigram. \n",
    "However, we should be aware that POS each tag in text sequence often depends on its previous/later tags.\n",
    "For example, nouns can appear after determiners and adjectives, and can be the subject or object of the verb.\n",
    "Therefore, we can look at some n-gram sequences of POS tags such as tag bi-grams.\n",
    "\n",
    "Let's build a frequency dictionary of tag bi-grams for the Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('AT', 'NN'): 48376, ('IN', 'AT'): 43271, ('NN', 'IN'): 42256, ('JJ', 'NN'): 28407, ('NN', '.'): 19873, ('AT', 'JJ'): 19488, ('NN', ','): 18282, ('IN', 'NN'): 17225, ('NNS', 'IN'): 14505, ('TO', 'VB'): 12291, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_bigram_list = list(nltk.bigrams(nltk.corpus.brown.tagged_words()))\n",
    "tag_bigram_fd = nltk.FreqDist((a[1], b[1]) for (a, b) in tag_bigram_list if b[1])\n",
    "tag_bigram_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that compared to unigram tag frequency dictionary, bigram dictionary has many more entries. It can potentiall cause the data sparsity problmen as we will encounter later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our intuitions that nouns often occur after the determiners and adjectives, we can investigate the distribution of tag bi-gram **(X, NN)** where **X** refers to any possible tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('AT', 'NN'), 48376), (('JJ', 'NN'), 28407), (('IN', 'NN'), 17225), (('NN', 'NN'), 11646), (('PP$', 'NN'), 8636), (('DT', 'NN'), 4367), (('CC', 'NN'), 4326), (('VBG', 'NN'), 2798), (('AP', 'NN'), 2671), ((',', 'NN'), 2394), (('VBN', 'NN'), 2301), (('CD', 'NN'), 1635), (('VB', 'NN'), 1463), (('NP', 'NN'), 1361), (('NP$', 'NN'), 1282), (('.', 'NN'), 1245), (('DTI', 'NN'), 1164), (('OD', 'NN'), 1139), (('CS', 'NN'), 992), (('NN$', 'NN'), 887), (('``', 'NN'), 770), (('JJR', 'NN'), 680), (('VBD', 'NN'), 531), (('NN-TL', 'NN'), 521), (('JJT', 'NN'), 422), (('NNS', 'NN'), 364), (('RB', 'NN'), 326), (('VBZ', 'NN'), 325), (('JJ-TL', 'NN'), 292), (('WDT', 'NN'), 265), ((\"''\", 'NN'), 231), (('ABN', 'NN'), 219), (('NR', 'NN'), 214), (('JJS', 'NN'), 202), (('BEZ', 'NN'), 184), (('BEDZ', 'NN'), 156), (('(', 'NN'), 152), (('WRB', 'NN'), 141), (('NN$-TL', 'NN'), 136), (('PPO', 'NN'), 131), (('WP$', 'NN'), 125), (('NP-TL', 'NN'), 124), (('--', 'NN'), 119), (('NNS$', 'NN'), 116), (('HV', 'NN'), 108), ((')', 'NN'), 106), ((':', 'NN'), 98), (('HVD', 'NN'), 73), (('RP', 'NN'), 68), (('NNS-TL', 'NN'), 65), (('BE', 'NN'), 65), (('PN$', 'NN'), 49), (('*', 'NN'), 48), (('ABX', 'NN'), 46), (('DTX', 'NN'), 41), (('DTS', 'NN'), 41), (('.-HL', 'NN'), 38), (('BER', 'NN'), 37), ((\"'\", 'NN'), 35), (('HVZ', 'NN'), 33), (('BEN', 'NN'), 25), (('NR$', 'NN'), 24), (('NN-HL', 'NN'), 24), (('MD', 'NN'), 22), (('BED', 'NN'), 21), (('CD-TL', 'NN'), 20), (('HVG', 'NN'), 20), ((':-HL', 'NN'), 17), (('NPS$', 'NN'), 16), (('QL', 'NN'), 16), (('PPSS', 'NN'), 16), (('FW-NN', 'NN'), 14), (('DO', 'NN'), 12), (('NP$-TL', 'NN'), 12), ((')-HL', 'NN'), 12), (('VBN-TL', 'NN'), 10), (('WPO', 'NN'), 10), (('NNS-HL', 'NN'), 9), (('HVN', 'NN'), 9), (('EX+BEZ', 'NN'), 8), (('BEG', 'NN'), 7), (('NNS$-TL', 'NN'), 6), (('PPS+BEZ', 'NN'), 6), (('TO', 'NN'), 6), (('DOZ', 'NN'), 5), (('DOD', 'NN'), 5), (('AT-TL', 'NN'), 4), (('RB$', 'NN'), 4), (('OD-TL', 'NN'), 4), (('NR$-TL', 'NN'), 4), (('RBR', 'NN'), 4), (('PPS', 'NN'), 4), (('NR-TL', 'NN'), 4), (('EX', 'NN'), 4), (('PN', 'NN'), 4), (('UH', 'NN'), 4), (('QLP', 'NN'), 4), (('AP$', 'NN'), 3), (('NP-HL', 'NN'), 3), (('NR-HL', 'NN'), 2), (('VBN-HL', 'NN'), 2), (('FW-NN-TL', 'NN'), 2), (('NPS', 'NN'), 2), (('CD-HL', 'NN'), 2), (('PPO-HL', 'NN'), 2), (('PPLS', 'NN'), 2), (('FW-VBN', 'NN'), 2), (('FW-JJ', 'NN'), 2), (('WPS', 'NN'), 2), (('DT+BEZ', 'NN'), 2), (('WPS+BEZ', 'NN'), 2), (('FW-NN$', 'NN'), 2), (('BEDZ*', 'NN'), 2), (('BEZ*', 'NN'), 2), (('VB+AT', 'NN'), 2), (('FW-DT', 'NN'), 1), (('RB-HL', 'NN'), 1), (('CD$', 'NN'), 1), (('NPS$-TL', 'NN'), 1), (('RB-NC', 'NN'), 1), (('IN-NC', 'NN'), 1), (('FW-NN$-TL', 'NN'), 1), (('NR-TL-HL', 'NN'), 1), (('PPL', 'NN'), 1), (('FW-WDT', 'NN'), 1), (('JJ$-TL', 'NN'), 1), (('BEM', 'NN'), 1), (('VB-HL', 'NN'), 1), (('AP-TL', 'NN'), 1), (('PPSS+BEM', 'NN'), 1), (('PP$$', 'NN'), 1), (('WRB+IN', 'NN'), 1), (('HVD*', 'NN'), 1), (('DO*', 'NN'), 1), (('PPSS+BER', 'NN'), 1), (('VB+PPO', 'NN'), 1), (('PPSS+MD', 'NN'), 1), (('RB-TL', 'NN'), 1), (('MD*', 'NN'), 1)]\n"
     ]
    }
   ],
   "source": [
    "nn_bigram_fd = nltk.FreqDist({k: v for k, v in tag_bigram_fd.items() if k[1] == 'NN'})\n",
    "sorted_nn_bigram_list = sorted(nn_bigram_fd.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(sorted_nn_bigram_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the two most frequent tags with **(X, NN)** are indeed **AT** (article) and **JJ** (adjective) according to the Brown Corpus tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping of properties using Python dictionaries\n",
    "As we have seen, a tagged word of the form **(word, tag)** is an association between a word and a POS tag. \n",
    "Once we start doing POS tagging, we will be using such structures extensively.\n",
    "For example, we want to get the tag list of a given word **(word, tag list)**, or the frequency of a given tag **(tag, freq)**.\n",
    "We can think of this process as mapping from a certain property to another, and the most natural way to store mappings in Python uses the so-called `dictionary` data type, which we have encountered in module 1 and previous part of this module. \n",
    "\n",
    "we look at dictionaries and see how they can represent a variety of language information, including POS, shown below.\n",
    "\n",
    "A dictionary in Python can be used for mapping between arbitrary types.\n",
    "Recall the frequency dictionary we used previously, which maps from a POS tag to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'NN': 152470, 'IN': 120557, 'AT': 97959, 'JJ': 64028, '.': 60638, ',': 58156, 'NNS': 55110, 'CC': 37718, 'RB': 36464, 'NP': 34476, ...})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tag_fd['NN'])\n",
    "tag_fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to create a dictionary of the form **(word, its corresponding tags)**, what types would be for the key and value of the dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present: V\n",
      "object: N\n"
     ]
    }
   ],
   "source": [
    "word2tags = {'present': 'V', 'object': 'N'}\n",
    "for k, v in word2tags.items():\n",
    "    print('{}: {}'.format(k, word2tags[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Mapping words to properties using Python dictionaries (section 5.3 in NLTK)\n",
    "Get the students thinking about linguistic objects as data structures, e.g. a dictionary could map from a headword to a POS or to a specific sense, e.g. in WordNet or Wikipedia. \n",
    "Dictionaries in Python again. Defining a POS dictionary by hand.  Creating a POS dictionary using a tagged corpus. \n",
    "** Creating a bigram dictionary\n",
    "Page 196 in the NLTK book shows how to create a dictionary of words and tags to perform bigram tagging;\n",
    "If we try to create an n-gram tagger for larger values of n what will happen?  We’ll encounter the sparse data problem and also start to reach the limits of computer memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print('hello world!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int(3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another similar concept is `method`. Check [here](https://stackoverflow.com/questions/155609/whats-the-difference-between-a-method-and-a-function) for their difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's learn how to define and call functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is defined by using the `def` keyword, followed by \n",
    "1. a **function name** of your choosing;\n",
    "2. a set of parentheses which hold any **parameters** the function will take (they can be empty);\n",
    "3. an ending **colon**;\n",
    "4. function **content code**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function `hello()`that prints **hello world**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hello():\n",
    "    print('hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \n",
    "- We do not have any parameter for this function, and we will discuss on it in more details later;\n",
    "- To start our real function content code, python requires a 4-space indent, as shown above.\n",
    "\n",
    "Now we have defined our first function. In order to let the program run this function, we still need to call the function, just like what we did to the python built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions can be more complicated. For example, we can use `for` loops, conditional statements, and more within our function block.\n",
    "\n",
    "For example, the function defined below utilizes a conditional statement to check if the input for the name variable contains a vowel, then uses a for loop to iterate over the letters in the name string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define function names()\n",
    "def names():\n",
    "    # Set up name variable with input\n",
    "    name = str(input('Enter your name: '))\n",
    "    # Check whether name has a vowel\n",
    "    if set('aeiou').intersection(name.lower()):\n",
    "        print('Your name contains a vowel.')\n",
    "    else:\n",
    "        print('Your name does not contain a vowel.')\n",
    "\n",
    "    # Iterate over name\n",
    "    for letter in name:\n",
    "        print(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `names()` function sets up a conditional statement and a for loop, showing how code can be organized within a function definition.\n",
    "\n",
    "> Defining functions within a program makes our code modular and reusable so that we can call the same functions without rewriting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have looked at functions with empty parentheses, but we can define parameters in function definitions within their parentheses.\n",
    "\n",
    "> A parameter is a variable in the definition of a function that the function can accept.\n",
    "\n",
    "Let’s create a function that takes trhee parameters `x`, `y`, `z`, and adds them in different configurations. The sums of these will be printed by the function. Then we’ll call the function and pass numbers into the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_numbers(x, y, z):\n",
    "    a = x + y\n",
    "    b = x + z\n",
    "    c = y + z\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass the `arguments` we want into the function to call it. Check [here](https://www.quora.com/What-is-the-difference-between-argument-and-parameters-in-C) for the difference between parameter and argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_numbers(1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed the number `1` in for the `x` parameter, `2` in for the `y` parameter, and `3` in for the `z` parameter. These values correspond with each parameter in the order they are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try different arguments\n",
    "add_numbers(4, 6, 8)\n",
    "add_numbers('a', 'b', 'c') # how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Write a function that, given an input list of integers, prints their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p><code>\n",
    "  def get_average_list(l):\n",
    "      print( sum(l) / len(l) )\n",
    "  </code></p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Write a function that, given an input string, computes and prints the following statistics:\n",
    "- Total number of characters.\n",
    "- Total number of tokens.\n",
    "- Total number of word types (i.e., distinct tokens) *Hint: there is a datatype that can help you with this!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_statistics(s):\n",
    "    tot_characters = #TODO\n",
    "    tot_tokens = #TODO\n",
    "    tot_types = #TODO\n",
    "    \n",
    "    print(\"The given input string contains\", tot_characters, \"characters,\",\\\n",
    "         tot_tokens, \"tokens and\", tot_types, \"types\")\n",
    "\n",
    "# Test your function here!\n",
    "test_string = \"To be or not to be, not to be or to be!\"\n",
    "string_statistics(test_string) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use keyword arguments in a function call to identify the arguments by the parameter name.\n",
    "\n",
    "When using keyword arguments, we can use parameters out of order because the Python interpreter will use the keywords provided to match the values to the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_numbers(x = 4, y = 6, z = 8)\n",
    "add_numbers(y = 6, x = 4, z = 8) # out of order, but same arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Argument Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide default values for one or more parameters, so the parameters will be set to the default values\n",
    "if we do not mention them when calling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_numbers_default_value(x, y = 6, z = 10):\n",
    "    a = x + y\n",
    "    b = x + z\n",
    "    c = y + z\n",
    "    print(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_numbers_default_value(4)\n",
    "add_numbers_default_value(x = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explicitly pass the arguments to the parameters with the default value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_numbers_default_value(4, y = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning a Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass parameters into a function, and a function can also return a value to us in the end. When a function exits, it can *optionally* pass an expression back to the caller, with the `return` statement. If you use a `return` statement with no arguments, the function will return `None`.\n",
    "\n",
    "So far, we have used the `print()` statement instead of the return statement in our functions. Let’s create a program that instead of printing will return a variable. The function `square` will take a parameter `x`, and returns the variable `y` representing the square of `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    y = x ** 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = square(3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated previously, we can use `return` with no arguments, so that the function will return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we return no arguments\n",
    "def square_return_noarg(x):\n",
    "    y = x ** 2\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = square_return_noarg(3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, without using the `return` statement here, the function cannot return a value so the value defaults to `None` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we do no use return\n",
    "def square_noreturn(x):\n",
    "    y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = square_noreturn(3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous function `add_numbers`, instead of printing the results, we can `return` more than one value wrapped in data structures such as **tuples**, **lists** or **dictionaries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_numbers_return_dict(x, y, z):\n",
    "    a = x + y\n",
    "    b = x + z\n",
    "    c = y + z\n",
    "    return {'a': a, 'b': b, 'c': c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = add_numbers_return_dict(1, 2, 3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever the program hits a `return` statement, the function will exit immediately, whether or not they are returning a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop_five():\n",
    "    for x in range(0, 25):\n",
    "        print(x)\n",
    "        if x == 5:\n",
    "            # Stop function at x == 5\n",
    "            return\n",
    "    print(\"This line will not execute.\")\n",
    "\n",
    "loop_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function hits `return` before the `for` loop ends, so the line that is outside of the loop will not run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Write a function that:\n",
    "- Takes as input a shopping list in the form of `items : quantity` pairs;\n",
    "- For each item to buy, prints a reminder in the form: *Don't forget to buy* + `quantity` + `items`;\n",
    "- Ends by wishing you a nice shopping time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_shopping_list(shopping_list):\n",
    "    # write here your function\n",
    "    pass\n",
    "\n",
    "# Test your function here!\n",
    "shopping_list = {\"pears\": 5, \"apples\": 7, \"bananas\": 4}\n",
    "print_shopping_list(shopping_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p><code>\n",
    "    def print_shopping_list(shopping_list):\n",
    "        for item, quantity in shopping_list.items():\n",
    "            print(\"Don't forget to buy\", quantity, item)\n",
    "        print(\"Have a nice shopping time!\")\n",
    "  </code></p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make our function a bit nicer.\n",
    "What would happen, for example, if someone would enter an incorrect input?\n",
    "Our function would crash!\n",
    "\n",
    "Let's improve our `print_shopping_list` function in the cell above in this way:\n",
    "- At the beginning of the function, check that the given input is of type `dict`;\n",
    "- If the input doesn't pass this test, return a `print` statement that asks the user for a correct input.\n",
    "\n",
    "Now, test our improved funtion with the test inputs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your improved function here!\n",
    "print_shopping_list({\"pears\": 5, \"apples\": 7, \"bananas\": 4})\n",
    "print_shopping_list([\"pears\", \"apples\", \"bananas\"])\n",
    "print_shopping_list({\"tomatoes\": 9, \"cat_litter\": 1, \"thuna_chunks\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p><code>\n",
    "    def print_shopping_list(shopping_list):\n",
    "        if type(shopping_list) != dict:\n",
    "            return(\"Please, give me a valid shopping list!\")\n",
    "        for item, quantity in shopping_list.items():\n",
    "            print(\"Don't forget to buy\", quantity, item)\n",
    "        print(\"Have a nice shopping time!\")\n",
    "  </code></p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Now, let's write a function that returns the total cost of a shopping list given as inputs two dictionaries: a `shopping_list` dictionary composed of `item : quantity` pairs (as in the excercise above) and an `inventory` dictionary composed of `item : cost` pairs.\n",
    "\n",
    "\n",
    "Complete the function below with the missing statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_shopping_bill(shopping_list, inventory):\n",
    "    shopping_cost = 0\n",
    "    for item, quantity in shopping_list.items():\n",
    "        # TODO: get the total price by summing up the prices of single items\n",
    "\n",
    "    return shopping_cost\n",
    "        \n",
    "    \n",
    "# Test your function here!\n",
    "our_shopping = {\"pears\": 5, \"apples\": 7, \"bananas\": 4}\n",
    "current_inventory = {\"pears\": 0.5, \"apples\": 0.7, \"bananas\": 1.3}\n",
    "\n",
    "cost = calculate_shopping_bill(our_shopping, current_inventory)\n",
    "print(\"You'll spend\", cost, \"GBP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p><code>\n",
    "    def calculate_shopping_bill(shopping_list, inventory):\n",
    "        shopping_cost = 0\n",
    "        for item, quantity in shopping_list.items():\n",
    "            single_item_cost = inventory[item]\n",
    "            total_item_cost = single_item_cost * quantity\n",
    "            shopping_cost += total_item_cost\n",
    "        return shopping_cost\n",
    "  </code></p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../resources/quiz.png\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function:\n",
    "```\n",
    "def add_numbers(x = 3, y = 4, z = 5):\n",
    "    x = x + y\n",
    "    y = x + z\n",
    "    z = y + z\n",
    "    print(x, y, z)\n",
    "    \n",
    "o, p, q = 5, 6, 7\n",
    "L = [5,6,7]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What does the following code print?\n",
    "\n",
    "```\n",
    "d = add_numbers(5)\n",
    "```\n",
    "\n",
    "A. 7, 8, 9\n",
    "\n",
    "B. 7, 8, 9\n",
    "\n",
    "C. 9, 14, 19\n",
    "\n",
    "D. 9, 14, 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>C</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the value of `d`?\n",
    "\n",
    "A. 7\n",
    "\n",
    "B. 9\n",
    "\n",
    "C. 10\n",
    "\n",
    "D. None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>D</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the value of `o p q` after running the following code?\n",
    "\n",
    "```\n",
    "add_numbers(o, p, q)\n",
    "```\n",
    "\n",
    "A. 7, 12, 17\n",
    "\n",
    "B. 9, 14, 19\n",
    "\n",
    "C. 11, 18, 25\n",
    "\n",
    "D. 5, 6, 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>D</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How to pass the element of `L` as the parameters of the function?\n",
    "\n",
    "\n",
    "A. ```add_numbers(L)```\n",
    "\n",
    "\n",
    "B. ```add_numbers(L*)```\n",
    "\n",
    "C. ```add_numbers(*L)```\n",
    "\n",
    "D. ```add_numbers(**L)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>C</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the value of `L` after running the following code?\n",
    "\n",
    "```\n",
    "add_numbers(L, L, L)\n",
    "```\n",
    "\n",
    "A. [5, 6, 7]\n",
    "\n",
    "B. [5, 6, 7, 5, 6, 7]\n",
    "\n",
    "C. [5, 6, 7, 5, 6, 7, 5, 6, 7]\n",
    "\n",
    "D. [5, 6, 7, 5, 6, 7, 5, 6, 7, 5, 6, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>A</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>Regular Expressions</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section is heavily built on this [tutorial](https://docs.python.org/3.6/howto/regex.html#regex-howto), and students are strongly recommended to go through it. A more comprehensive documentation of python regular expressions can be found [here](https://docs.python.org/3.6/library/re.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions (called REs, or regexes, or regex patterns) are essentially a tiny, highly specialized programming language embedded inside Python and made available through the re module. Using this little language, you specify the rules for the set of possible strings that you want to match; this set might contain English sentences, or e-mail addresses, or TeX commands, or anything you like. You can then ask questions such as “Does this string match the pattern?”, or “Is there a match for the pattern anywhere in this string?”. You can also use REs to modify a string or to split it apart in various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python uses the **raw string** notation for RE patterns, and backslashes `'\\'` are not handled in any special way in a string literal prefixed with `r`. So `r'\\n'` is a two-character string containing `'\\'` and `'n'`, while `'\\n'` is a one-character string containing a newline. \n",
    "\n",
    "We’ll use the raw string notation for the rest of the section. We will also write REs in highlight style , i.e. `r'\\n'` is equivalent to `\\n`, usually without quotes, and strings to be matched 'in single quotes'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REs can contain both special and ordinary characters. Most ordinary characters, like `A`, `a`, or `0`, are the simplest REs; they simply match themselves. You can concatenate ordinary characters to match more complex sequence. For example, `test` will match the string 'test' exactly.\n",
    "\n",
    "Some characters are special metacharacters, and don’t match themselves. Instead, they signal that some out-of-the-ordinary thing should be matched, or they affect other portions of the RE by repeating them or changing their meaning.\n",
    "\n",
    "Here’s a complete list of the metacharacters and their explanations:\n",
    "\n",
    "- `.`             Matches any character except a newline.\n",
    "- `^`             Matches the start of the string.\n",
    "- `$`             Matches the end of the string or just before the newline at the end of the string.\n",
    "- `*`             Matches 0 or more (greedy) repetitions of the preceding RE. Greedy means that it will match as many repetitions as possible.\n",
    "- `+`             Matches 1 or more (greedy) repetitions of the preceding RE.\n",
    "- `?`             Matches 0 or 1 (greedy) of the preceding RE.\n",
    "- `*?, +?, ??`    Non-greedy versions of the previous three special characters.\n",
    "- `{m,n}`         Matches from m to n repetitions of the preceding RE.\n",
    "- `{m,n}?`        Non-greedy version of the above.\n",
    "- `\\\\`            Either escapes special characters or signals a special sequence.\n",
    "- `[]`            Indicates a set of characters. A \"^\" as the first character indicates a complementing set.\n",
    "- `|`             A|B, creates an RE that will match either A or B.\n",
    "- `(...)`         Matches the RE inside the parentheses. The contents can be retrieved or matched later in the string.\n",
    "\n",
    "\n",
    "The special sequences consist of \"\\\\\" and a character from the list below:\n",
    "- `\\number`  Matches the contents of the group of the same number.\n",
    "- `\\A`       Matches only at the start of the string.\n",
    "- `\\Z`       Matches only at the end of the string.\n",
    "- `\\b`       Matches the empty string, but only at the start or end of a word.\n",
    "- `\\B`       Matches the empty string, but not at the start or end of a word.\n",
    "- `\\d`       Matches any decimal digit; equivalent to the set `[0-9]`.\n",
    "- `\\D`       Matches any non-digit character; equivalent to `[^\\d]`.\n",
    "- `\\s`       Matches any whitespace character; equivalent to `[ \\t\\n\\r\\f\\v]`.\n",
    "- `\\S`       Matches any non-whitespace character; equivalent to `[^\\s]`.\n",
    "- `\\w`       Matches any alphanumeric character; equivalent to `[a-zA-Z0-9_]`.\n",
    "- `\\W`       Matches the complement of `\\w`.\n",
    "- `\\\\`       Matches a literal backslash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import built-in regular expression module in python\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = (\"\"\"                2\n",
    "  When forty winters shall besiege thy brow,\"\"\"\n",
    "  \"And dig deep trenches in thy beauty's field, \"\n",
    "  \"Thy youth's proud livery so gazed on now, \"\n",
    "  \"Will be a tattered weed of small worth held: \"\n",
    "  \"Then being asked, where all thy beauty lies, \"\n",
    "  \"Where all the treasure of thy lusty days; \"\n",
    "  \"To say within thine own deep sunken eyes, \"\n",
    "  \"Were an all-eating shame, and thriftless praise. \"\n",
    "  \"How much more praise deserved thy beauty's use, \"\n",
    "  \"If thou couldst answer 'This fair child of mine \"\n",
    "  \"Shall sum my count, and make my old excuse' \"\n",
    "  \"Proving his beauty by succession thine. \"\n",
    "  \"This were to be new made when thou art old, \"\n",
    "  \"And see thy blood warm when thou feel'st it cold.\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mainly concentrate two essential usage of regular expressions: **searching** and **substituting** text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first find word '**where**', in regardless of case.\n",
    "\n",
    "The **re** module provides an interface to the regular expression engine, allowing you to compile REs into objects and then perform matches with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'\\b[Ww]here\\b')\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we obtained compiled pattern object, it has several functions and attributes. For **searching** text, there are mainly four functions available:\n",
    "- *match()*: determine if the RE matches at the beginning of the string.\n",
    "- *search()*: scan through a string, looking for any location where this RE matches.\n",
    "- *findall()*: find all substrings where the RE matches, and returns them as a list.\n",
    "- *finditer()*: find all substrings where the RE matches, and returns them as an iterator.\n",
    "\n",
    "Please consult the **re** documentation for a complete listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = p.match(text)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = p.search(text)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions *match()* and *search()* return None if no match can be found. If they’re successful, a match object instance is returned, containing information about the match: where it starts and ends, the substring it matched, and more.\n",
    "\n",
    "In this example, as the text doesn't start with the word 'where', so *match()* won't find any match, whereas *search()* will look for any location where RE matches.\n",
    "\n",
    "We can further query the match object for information about the matching string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.start(), m.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.span()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what if we want to find all matches, i.e. all words 'where'? We can use *findall()* and *finditer()*.\n",
    "\n",
    "The *findall()* returns a list of matching strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = p.findall(text)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *finditer()* returns a sequence of match object instances as an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterator = p.finditer(text)\n",
    "for match in iterator:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don’t have to create a pattern object and call its functions; the **re** module also provides the same top-level functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(re.search(r'\\b[Ww]here\\b', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(re.findall(r'\\b[Ww]here\\b', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when we compile the RE, there is a **re.UNICODE** item when we print the object. It is a compliation flag which let you modify some aspects of how RE works. The full flag list is available in the module documentation. For example, we can use **re.IGNORECASE** or **re.I** to do case-insensitive matches, so that we do not have to take care of capital letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'\\bwhere\\b', re.I)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will still have the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = p.findall(text)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to come up with your regular expressions and search in the text to see if they will work.\n",
    "\n",
    "You can use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently you need to obtain more information than just whether the RE matched or not. REs are often used to dissect strings by writing a RE divided into several subgroups which match different components of interest.\n",
    "\n",
    "Groups are marked by the `'('`, `')'` metacharacters. They group together the expressions contained inside them, and you can repeat the contents of a group with a repeating qualifier, such as `*`, `+`, `?`, or `{m,n}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile('((a(b)c)d)*')\n",
    "m = p.match('abcdabcd')\n",
    "print(m.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups indicated with `'('`, `')'` also capture the starting and ending index of the text that they match; this can be retrieved by passing an argument to `group()`, `start()`, `end()`, and `span()`. \n",
    "\n",
    "Groups are numbered starting with 0. Group 0 is always present; it’s the whole RE, so previous methods all have group 0 as their default argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(m.start(0), m.end(0))\n",
    "print(m.group(0,1,2,3))\n",
    "print(m.span(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groups()` method returns a tuple containing the strings for all the subgroups, from 1 up to however many there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(m.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substituting text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following functions for substitution:\n",
    "- *sub()*: find all substrings where the RE matches, and replace them with a different string.\n",
    "- *subn()*: does the same thing as sub(), but returns the new string and the number of replacements.\n",
    "\n",
    "Again, you are encouraged to refer to the docs for more functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***sub*** *(replacement, string[, count=0])*\n",
    "\n",
    "Returns the string obtained by replacing the leftmost non-overlapping occurrences of the RE in *string* by the replacement *replacement*. If the pattern isn’t found, string is returned unchanged.\n",
    "\n",
    "The optional argument *count* is the maximum number of pattern occurrences to be replaced; *count* must be a non-negative integer. The default value of 0 means to replace all occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the same example, let's substitute all `'where'` words to `'-------'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'\\b[Ww]here\\b')\n",
    "print(p.sub('------', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(p.sub('------', text, count = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subn()` method does the same work, but returns a 2-tuple containing the new string value and the number of replacements that were performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(p.subn('------', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = re.compile('section{ (?P<name> [^}]* ) }', re.VERBOSE)\n",
    "p.sub(r'subsection{\\g<1>}','section{First} section{second}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:1.5em; font-weight: bold\">\n",
    "<img src=\"../../resources/exercise.png\" style=\"height:36pt; display:inline; vertical-align:bottom; margin-right: 4pt\" /> \n",
    "Try it yourself! <hr>\n",
    "</p>\n",
    "\n",
    "Now it's your turn! Please write a regular expression to match sentences that start with the word “This” (case insensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a regex that matches words which are emoticons (like *:)*). At least 5 emoticons must be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a regex that can match words which contains at least 3 *o*s in a row (so that *cooooool* would be a match, but *book* would not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, write a regex that substitutes any word-ending in *a* or *as* into an *o* or *os*.\n",
    "\n",
    "Test your regex by converting the Spanish sentence *Maria es ecuatoriana y tiene dos hijas rubias y muy altas* (Mary is Ecuadorian and has two blonde and very tall daughters) into its masculine equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a simple Eliza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having learnt about Eliza in the lecture, which is meant to emulate a Rogerian psychologist, let us now build a simple version of Eliza with RE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstl, let us build a dictionary `grefs` that is used to map first-person pronouns to second-person pronouns and vice-versa. It is used to “reflect” a statement back against the user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grefs = {\n",
    "    \"am\":          \"are\",\n",
    "    \"was\":         \"were\",\n",
    "    \"i\":           \"you\",\n",
    "    \"i'd\":         \"you would\",\n",
    "    \"i've\":        \"you have\",\n",
    "    \"i'll\":        \"you will\",\n",
    "    \"my\":          \"your\",\n",
    "    \"are\":         \"am\",\n",
    "    \"you are\":     \"I am\",\n",
    "    \"you've\":      \"I have\",\n",
    "    \"you'll\":      \"I will\",\n",
    "    \"your\":        \"my\",\n",
    "    \"yours\":       \"mine\",\n",
    "    \"you\":         \"me\",\n",
    "    \"me\":          \"you\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need another table `gpats` that is made up of a list of lists, where the first element is a RE that matches the user’s statements and the second element is a list of potential responses. \n",
    "\n",
    "Many of the potential responses contain placeholders that can be filled in with fragments to echo the user’s statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpats = [\n",
    "  [r'I need (.*)',\n",
    "  [  \"Why do you need {0}?\",\n",
    "    \"Would it really help you to get {0}?\",\n",
    "    \"Are you sure you need {0}?\"]],\n",
    "\n",
    "  [r'I can\\'?t (.*)',\n",
    "  [  \"How do you know you can't {0}?\",\n",
    "    \"Perhaps you could {0} if you tried.\",\n",
    "    \"What would it take for you to {0}?\"]],\n",
    "\n",
    "  [r'I am (.*)',\n",
    "  [  \"Did you come to me because you are {0}?\",\n",
    "    \"How long have you been {0}?\",\n",
    "    \"How do you feel about being {0}?\"]],\n",
    "\n",
    "  [r'I\\'?m (.*)',\n",
    "  [  \"How does being {0} make you feel?\",\n",
    "    \"Do you enjoy being {0}?\",\n",
    "    \"Why do you tell me you're {0}?\",\n",
    "    \"Why do you think you're {0}?\"]],\n",
    "\n",
    "  [r'Are you ([^\\?]*)\\??',\n",
    "  [  \"Why does it matter whether I am {0}?\",\n",
    "    \"Would you prefer it if I were not {0}?\",\n",
    "    \"Perhaps you believe I am {0}.\",\n",
    "    \"I may be {0} -- what do you think?\"]],\n",
    "\n",
    "  [r'What (.*)',\n",
    "  [  \"Why do you ask?\",\n",
    "    \"How would an answer to that help you?\",\n",
    "    \"What do you think?\"]],\n",
    "\n",
    "  [r'How (.*)',\n",
    "  [  \"How do you suppose?\",\n",
    "    \"Perhaps you can answer your own question.\",\n",
    "    \"What is it you're really asking?\"]],\n",
    "\n",
    "  [r'Because (.*)',\n",
    "  [  \"Is that the real reason?\",\n",
    "    \"What other reasons come to mind?\",\n",
    "    \"Does that reason apply to anything else?\",\n",
    "    \"If {0}, what else must be true?\"]],\n",
    "\n",
    "  [r'Hello(.*)',\n",
    "  [  \"Hello... I'm glad you could drop by today.\",\n",
    "    \"Hi there... how are you today?\",\n",
    "    \"Hello, how are you feeling today?\"]],\n",
    "\n",
    "  [r'quit',\n",
    "  [  \"Thank you for talking with me.\",\n",
    "    \"Good-bye.\",\n",
    "    \"Thank you, that will be $150.  Have a good day!\"]],\n",
    "  \n",
    "  [r'(.*)',\n",
    "  [  \"Please tell me more.\",\n",
    "    \"Let's change focus a bit... Tell me about your family.\",\n",
    "    \"Can you elaborate on that?\",\n",
    "    \"Why do you say that {0}?\",\n",
    "    \"I see.\",\n",
    "    \"Very interesting.\",\n",
    "    \"{0}.\",\n",
    "    \"I see.  And what does that tell you?\",\n",
    "    \"How does that make you feel?\",\n",
    "    \"How do you feel when you say that?\"]] \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us write the function `respond` that reads the user input and generates the response accordingly.\n",
    "\n",
    "We iterate through the regular expressions in `gpats`, trying to match each one with the user’s input `s`. If we find a match, we choose a response template randomly from the list of possible responses associated with the matching pattern. Then we interpolate the match groups from the RE into the response string, calling the `transalte` function on each match group first.\n",
    "\n",
    "When we use the list comprehension to generate a list of reflected match groups, we explode the list with the asterisk `\\*` character before passing it to the string’s `format` method. Format expects a series of positional arguments corresponding to the number of format placeholders – `{0}`, `{1}`, etc. – in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "def respond(s):\n",
    "    for pattern, responses in gpats:\n",
    "        # find a match for s\n",
    "        match = re.match(pattern, s)\n",
    "        if match:\n",
    "            # chosen randomly from among the available options\n",
    "            response = random.choice(responses)\n",
    "            return response.format(*[translate(g) for g in match.groups()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we make the input lowercase, then we tokenize it by splitting on whitespace characters. We iterate through the list of tokens and, if the token exists in our reflections dictionary, we replace it with the value from the dictionary. So “I” becomes “you”, “your” becomes “my”, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def translate(fragment):\n",
    "    tokens = fragment.lower().split()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in grefs:\n",
    "            tokens[i] = grefs[token]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can wrap everything into an interface where our Eliza will wait for the user input and generate corresponding response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def start_eliza():\n",
    "    print('Therapist\\n---------')\n",
    "    print('Talk to the program by typing in plain English, using normal upper-')\n",
    "    print('and lower-case letters and punctuation.  Enter \"quit\" when done.')\n",
    "    print('='*72)\n",
    "    print('Hello.  How are you feeling today?')\n",
    "\n",
    "    s = ''\n",
    "    while s != 'quit':\n",
    "        try:\n",
    "            s = input('> ')\n",
    "            while s[-1] in '!.':\n",
    "                s = s[:-1]\n",
    "        except:\n",
    "            s = 'quit'\n",
    "            print('Invalid Input, exit ...')\n",
    "        print(respond(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_eliza()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to add your own [RE, responses] pairs to the `gpats` to capture more questions from the user. Feel free to refer to [this](https://github.com/jezhiggins/eliza.py/blob/master/eliza.py) for more REs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../resources/quiz.png\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the previous text, which RE can find all words with 3, 4 and 5 characters?\n",
    "\n",
    "A. `r'.{3,5}'`\n",
    "\n",
    "B. `r'\\b.{3,5}+\\b'`\n",
    "\n",
    "C. `r'\\b\\w{3,5}\\b'`\n",
    "\n",
    "D. `r'\\w{3,5}'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>C</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Which function can you use to find all the words that the RE in 1. matches?\n",
    "\n",
    "A. `match()`\n",
    "\n",
    "B. `search()`\n",
    "\n",
    "C. `finditer()`\n",
    "\n",
    "D. `findall()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>C, D</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How many words the RE in 1 mathes are there?\n",
    "\n",
    "A. 128\n",
    "\n",
    "B. 105\n",
    "\n",
    "C. 96\n",
    "\n",
    "D. 77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>D</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How many words are there with 5 letters?\n",
    "\n",
    "A. 10\n",
    "\n",
    "B. 20\n",
    "\n",
    "C. 30\n",
    "\n",
    "D. 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>B</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What's the span of the first word with 4 letters?\n",
    "\n",
    "A. (16, 20)\n",
    "\n",
    "B. (18, 22)\n",
    "\n",
    "C. (20, 24)\n",
    "\n",
    "D. (22, 26) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>    <!-- please remember this! -->\n",
    "<details>\n",
    "  <summary>Click <b>here</b> to see the answer.</summary>\n",
    "  <p>C</p>\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center' style='margin-top:2em'>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "    <u>SpaCy</u>\n",
    "    <img src=\"../../resources/section_header.png\" \n",
    "         style=\"height:36pt; display:inline; vertical-align:center; margin-top:0em\" />\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "\n",
    "spaCy is designed specifically for production use and helps you build applications that process and “understand” large volumes of text. It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning.\n",
    "\n",
    "There are many useful resources online for spaCy, among which the official [tutorial](https://spacy.io/usage/spacy-101) and the [course](https://course.spacy.io/) are extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the link [here](https://spacy.io/usage) to install spaCy with pip. Then download and install the English model by running:\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is enables many different features introduced [here](https://spacy.io/usage/spacy-101#features), and we will focus on the simplest and direct features:\n",
    "- tokenization;\n",
    "- stemming; (not supported by spaCy)\n",
    "- lemmatization;\n",
    "- part-of-speech (POS) tagging;\n",
    "- sentence boundary detection and sentence segmentation;\n",
    "- dependency parsing;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import spacy module\n",
    "import spacy\n",
    "\n",
    "# load english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# read the text and process\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# get all separated tokens in doc\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the list *tokens* that there each tokenized token is in the entry of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you notice that almost all pronouns are subsituted with `-PRON-`?\n",
    "\n",
    "Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "poss = [token.pos_ for token in doc]\n",
    "print(poss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the outputs of the previous three lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move to sentence level features, i.e. sentence boundary detection and sentence segmentation. Unlike other libraries, spaCy uses the dependency parse to determine sentence boundaries. This is usually more accurate than a rule-based approach, but it also means you’ll need a statistical model and accurate predictions.\n",
    "If your texts are closer to general-purpose news or web text, this should work well out-of-the-box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "sent_texts = [sentence.text for sentence in sentences]\n",
    "print(sent_texts)\n",
    "print(len(sent_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for other texts like social media texts, your application may benefit from a custom rule-based implementation. You can either use the built-in Sentencizer or plug an entirely custom rule-based function into your processing pipeline.\n",
    "\n",
    "Let's build a rule-based sentence segmentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp_rule = English()  # just the language with no model\n",
    "sentencizer = nlp_rule.create_pipe(\"sentencizer\")\n",
    "nlp_rule.add_pipe(sentencizer)\n",
    "doc_rule = nlp_rule(text)\n",
    "sent_rule_texts = [sent.text for sent in doc_rule.sents]\n",
    "print(sent_rule_texts)\n",
    "print(len(sent_rule_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see clearly the difference in both methods, where the statistical dependency parse sentence segmentor generate 12 sentencs, but the rule-based sentence segmentor only yields 4 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the dependencies, i.e. the dependency relations between tokens, which is also used to segment sentences for the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "deps = [token.dep_ for token in doc]\n",
    "print(deps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spaCy’s built-in [displaCy visualizer](https://spacy.io/usage/visualizers), here’s what our example sentence and its dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(sentences, style = \"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../resources/quiz.png\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Quiz</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../resources/assessment.png\" style=\"height:36pt; float:left; margin-right: 4pt\" /> <h2>Assessment</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function to calculate the $N_{th}$ Finbonacci number without `for` loop using the folloing formula:\n",
    "$$\n",
    "f_n = \\frac{a^n - b^n}{a - b} = \\frac{a^n - b^n}{\\sqrt{5}}\n",
    "$$\n",
    "where $a = \\frac{1 + \\sqrt{5}}{2}$ and $b = \\frac{1- \\sqrt{5}}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "SQRT_5 = math.sqrt(5)\n",
    "def calc_fibonacci(n):\n",
    "    a = (1 + SQRT_5) / 2\n",
    "    b = (1 - SQRT_5) / 2\n",
    "    fn = int((a**n - b**n) / SQRT_5)\n",
    "    return fn\n",
    "\n",
    "print(calc_fibonacci(5))\n",
    "print(calc_fibonacci(10))\n",
    "print(calc_fibonacci(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenize the following sentences with RE:\n",
    "```\n",
    "sents = \"\"\"He asked: \"When will you graduate?\"\\n\"I will get my Ph.D. degree(Doctor of Philosophy) in a few years, hehe. And get $123,45675.45.\" I answered. END\"\"\"\n",
    "```\n",
    "How many token types are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PUNCTS = ['\"', \"'\",':', ';', '?', '(',')', '[', ']', '!', '$', '&', '*', '#', '-']\n",
    "PUNCTS_REGEX =  {',':['[^ ](,)(\\s+)'], '.':['[^ ]\\.(\\s+)([^a-z])']}\n",
    "\n",
    "def my_tokenizer(sents):\n",
    "    print('Original...')\n",
    "    print(sents)\n",
    "\n",
    "    for punt in PUNCTS:\n",
    "        sents = sents.replace(punt, (' ' + punt + ' '))\n",
    "\n",
    "    for punct, regs in PUNCTS_REGEX.items():\n",
    "        for reg in regs:\n",
    "            p = re.compile(reg)\n",
    "            m = p.search(sents)\n",
    "            while m:\n",
    "                sents = sents[:m.start() + 1] + ' ' + punct + ' ' + sents[m.start() + 2:]\n",
    "                m = p.search(sents)\n",
    "    sents = re.sub(r' +', ' ', sents)        \n",
    "\n",
    "    print('After Tokenization...')\n",
    "    print(sents)\n",
    "    return sents\n",
    "\n",
    "sents = \"\"\"He asked: \"When will you graduate?\"\\n\"I will get my Ph.D. degree(Doctor of Philosophy) in a few years, hehe. And get $123,45675.45.\" I answered. END\"\"\"\n",
    "tok_sents = my_tokenizer(sents)\n",
    "print(set(tok_sents.split()))\n",
    "print(len(set(tok_sents.split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Tokenize the previous sentences with SpaCy. How many token types are there in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import spacy module\n",
    "import spacy\n",
    "\n",
    "# load english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# read the text and process\n",
    "doc = nlp(sents)\n",
    "print(doc)\n",
    "tokens = [token.text for token in doc]\n",
    "print(' '.join(tokens))\n",
    "print(set(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How To Define Functions in Python 3](https://www.digitalocean.com/community/tutorials/how-to-define-functions-in-python-3)\n",
    "- [Regular Expression HOWTO](https://docs.python.org/3.6/howto/regex.html#regex-howto)\n",
    "- [Regular Expression Documentation](https://docs.python.org/3.6/library/re.html)\n",
    "- [Eliza](https://github.com/jezhiggins/eliza.py/blob/master/eliza.py)\n",
    "- [spaCy 101: Everything you need to know](https://spacy.io/usage/spacy-101#_title)\n",
    "- [Advanced NLP with spaCy](https://course.spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Module\n",
    "\n",
    "[Click here](../module_1.4/module_1.4.ipynb) to move to the next module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
